[
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Demos",
    "section": "",
    "text": "Demo\nDate\nTitle\nDemo file\n\n\n\n\n0\nJan 14\nInto the tidyverse\nHTML\n\n\n1\nJan 16\nBuilding an Expected Goals Model\nHTML\n\n\n2\nJan 21\nCalibration and Cross-Validation\nHTML\n\n\n3\nJan 23\nMultinomial Logistic Regression for Expected Points\nHTML",
    "crumbs": [
      "Demos"
    ]
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Lecture\nDate\nTitle\nLinks to slides\n\n\n\n\n1\nJan 14\nIntro and Big Ideas in Sports Analytics\nGoogle slides (requires CMU email)\n\n\n2\nJan 16\nBuilding an Expected Goals Model\nGoogle slides (requires CMU email)\n\n\n3\nJan 21\nBuilding an Expected Goals Model (Cont.)\nGoogle slides (requires CMU email)\n\n\n4\nJan 23\nBuilding an Expected Points Model\nGoogle slides (requires CMU email)",
    "crumbs": [
      "Lectures"
    ]
  },
  {
    "objectID": "demos/03-multinom-logit.html",
    "href": "demos/03-multinom-logit.html",
    "title": "Lecture 4: Multinomial Logistic Regression for Expected Points",
    "section": "",
    "text": "The goal of this demo is to introduce how to fit and evaluate a multinomial logistic regression model in the context of modeling the next scoring event in American football. For this demo, we’ll use an example dataset of NFL play-by-play data from 2013 to 2023. This is initialized with a column including the next score in the half for each play (you can find the script on Canvas, which creates the dataset using nflreadr).\nThe following code chunk reads in the relevant NFL play-by-play dataset (assuming it is in the correct directory) and performs some initial pre-processing relevant for the expected points model:\n\nlibrary(tidyverse)\nnfl_ep_model_data &lt;- read_rds(here::here(\"data/model_nfl_pbp_data.rds\"))\nnfl_ep_model_data &lt;- nfl_ep_model_data |&gt;\n  # Make the No_Score level the reference level:\n  mutate(Next_Score_Half = fct_relevel(Next_Score_Half, \"No_Score\"),\n         # log transform of yards to go and indicator for two minute warning:\n         log_ydstogo = log(ydstogo),\n         # Changing down into a factor variable: \n         down = factor(down))\n\nnfl_ep_model_data\n\n# A tibble: 422,904 × 21\n   game_id     Next_Score_Half Drive_Score_Half play_type game_seconds_remaining\n   &lt;chr&gt;       &lt;fct&gt;                      &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 2013_01_AR… Opp_Touchdown                  4 pass                        3593\n 2 2013_01_AR… Opp_Touchdown                  4 run                         3572\n 3 2013_01_AR… Opp_Touchdown                  4 pass                        3536\n 4 2013_01_AR… Opp_Touchdown                  4 no_play                     3507\n 5 2013_01_AR… Opp_Touchdown                  4 run                         3476\n 6 2013_01_AR… Opp_Touchdown                  4 no_play                     3446\n 7 2013_01_AR… Opp_Touchdown                  4 pass                        3428\n 8 2013_01_AR… Opp_Touchdown                  4 pass                        3424\n 9 2013_01_AR… Opp_Touchdown                  4 punt                        3397\n10 2013_01_AR… Touchdown                      4 run                         3386\n# ℹ 422,894 more rows\n# ℹ 16 more variables: half_seconds_remaining &lt;dbl&gt;, yardline_100 &lt;dbl&gt;,\n#   posteam &lt;chr&gt;, defteam &lt;chr&gt;, home_team &lt;chr&gt;, ydstogo &lt;dbl&gt;, season &lt;int&gt;,\n#   qtr &lt;dbl&gt;, down &lt;fct&gt;, week &lt;int&gt;, drive &lt;dbl&gt;, score_differential &lt;dbl&gt;,\n#   posteam_timeouts_remaining &lt;dbl&gt;, defteam_timeouts_remaining &lt;dbl&gt;,\n#   desc &lt;chr&gt;, log_ydstogo &lt;dbl&gt;"
  },
  {
    "objectID": "demos/03-multinom-logit.html#introduction",
    "href": "demos/03-multinom-logit.html#introduction",
    "title": "Lecture 4: Multinomial Logistic Regression for Expected Points",
    "section": "",
    "text": "The goal of this demo is to introduce how to fit and evaluate a multinomial logistic regression model in the context of modeling the next scoring event in American football. For this demo, we’ll use an example dataset of NFL play-by-play data from 2013 to 2023. This is initialized with a column including the next score in the half for each play (you can find the script on Canvas, which creates the dataset using nflreadr).\nThe following code chunk reads in the relevant NFL play-by-play dataset (assuming it is in the correct directory) and performs some initial pre-processing relevant for the expected points model:\n\nlibrary(tidyverse)\nnfl_ep_model_data &lt;- read_rds(here::here(\"data/model_nfl_pbp_data.rds\"))\nnfl_ep_model_data &lt;- nfl_ep_model_data |&gt;\n  # Make the No_Score level the reference level:\n  mutate(Next_Score_Half = fct_relevel(Next_Score_Half, \"No_Score\"),\n         # log transform of yards to go and indicator for two minute warning:\n         log_ydstogo = log(ydstogo),\n         # Changing down into a factor variable: \n         down = factor(down))\n\nnfl_ep_model_data\n\n# A tibble: 422,904 × 21\n   game_id     Next_Score_Half Drive_Score_Half play_type game_seconds_remaining\n   &lt;chr&gt;       &lt;fct&gt;                      &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1 2013_01_AR… Opp_Touchdown                  4 pass                        3593\n 2 2013_01_AR… Opp_Touchdown                  4 run                         3572\n 3 2013_01_AR… Opp_Touchdown                  4 pass                        3536\n 4 2013_01_AR… Opp_Touchdown                  4 no_play                     3507\n 5 2013_01_AR… Opp_Touchdown                  4 run                         3476\n 6 2013_01_AR… Opp_Touchdown                  4 no_play                     3446\n 7 2013_01_AR… Opp_Touchdown                  4 pass                        3428\n 8 2013_01_AR… Opp_Touchdown                  4 pass                        3424\n 9 2013_01_AR… Opp_Touchdown                  4 punt                        3397\n10 2013_01_AR… Touchdown                      4 run                         3386\n# ℹ 422,894 more rows\n# ℹ 16 more variables: half_seconds_remaining &lt;dbl&gt;, yardline_100 &lt;dbl&gt;,\n#   posteam &lt;chr&gt;, defteam &lt;chr&gt;, home_team &lt;chr&gt;, ydstogo &lt;dbl&gt;, season &lt;int&gt;,\n#   qtr &lt;dbl&gt;, down &lt;fct&gt;, week &lt;int&gt;, drive &lt;dbl&gt;, score_differential &lt;dbl&gt;,\n#   posteam_timeouts_remaining &lt;dbl&gt;, defteam_timeouts_remaining &lt;dbl&gt;,\n#   desc &lt;chr&gt;, log_ydstogo &lt;dbl&gt;"
  },
  {
    "objectID": "demos/03-multinom-logit.html#fitting-a-multinomial-logistic-regression-model",
    "href": "demos/03-multinom-logit.html#fitting-a-multinomial-logistic-regression-model",
    "title": "Lecture 4: Multinomial Logistic Regression for Expected Points",
    "section": "Fitting a multinomial logistic regression model",
    "text": "Fitting a multinomial logistic regression model\nIn order to fit a multinomial logistic regression model in R, the easiest way is to use the nnet package with the multinom function. The following code chunk fits this model to the full dataset with the Next_Score_Half variable as the response with the context variables as the predictors:\n\nlibrary(nnet)\ninit_ep_model &lt;- multinom(Next_Score_Half ~ half_seconds_remaining + \n                            yardline_100 + down + log_ydstogo + \n                            log_ydstogo * down + yardline_100 * down, \n                          data = nfl_ep_model_data, maxit = 300)\n\n# weights:  98 (78 variable)\ninitial  value 822933.185674 \niter  10 value 646448.661731\niter  20 value 628562.786640\niter  30 value 616585.530613\niter  40 value 600078.459583\niter  50 value 597249.917485\niter  60 value 568326.053579\niter  70 value 561806.646221\niter  80 value 556313.688644\niter  90 value 555528.854290\niter 100 value 555518.276158\nfinal  value 555518.120956 \nconverged\n\n\nNote the use of maxit = 300 is to provide a sufficient number of steps for model fitting. You’ll notice the printing of iteration steps here because this package is actually the simplest package used for fitting neural networks in R.\nNotice what happens when we use the summary() function on this model (it takes some time to run):\n\nsummary(init_ep_model)\n\nCall:\nmultinom(formula = Next_Score_Half ~ half_seconds_remaining + \n    yardline_100 + down + log_ydstogo + log_ydstogo * down + \n    yardline_100 * down, data = nfl_ep_model_data, maxit = 300)\n\nCoefficients:\n               (Intercept) half_seconds_remaining yardline_100      down2\nField_Goal      -0.2607051            0.004116958 -0.034840581  0.7076165\nOpp_Field_Goal  -3.0189007            0.004293194  0.001137907  0.1443799\nOpp_Safety      -8.9905777            0.004245522  0.058165564 -1.0759002\nOpp_Touchdown   -2.8386923            0.004508825  0.001923287  0.0914707\nSafety          -4.1544215            0.004459871 -0.019844872 -0.8222278\nTouchdown        2.1128586            0.004298760 -0.036028100 -1.0277706\n                    down3      down4 log_ydstogo down2:log_ydstogo\nField_Goal      0.8926587  1.2557561   0.3805016     -0.2836294318\nOpp_Field_Goal  0.2142137  0.3490739   0.1395655     -0.0468113016\nOpp_Safety     -0.4023122  0.8780304  -0.6304597      0.7793959964\nOpp_Touchdown   0.1912419  0.3611854   0.1176799     -0.0008362159\nSafety         -0.7113207 -0.5755544  -0.3702644      0.4053892714\nTouchdown      -1.4807308 -2.8058121  -0.5186325      0.3515486857\n               down3:log_ydstogo down4:log_ydstogo yardline_100:down2\nField_Goal          -0.276146057        -0.1963895      -0.0014910525\nOpp_Field_Goal      -0.053572298        -0.1586719       0.0015471520\nOpp_Safety           0.753610712         0.7305257      -0.0066148761\nOpp_Touchdown       -0.008979889        -0.1477311       0.0008414841\nSafety               0.331151454         0.3681949      -0.0006310041\nTouchdown            0.307635794         0.2487536       0.0002840841\n               yardline_100:down3 yardline_100:down4\nField_Goal           -0.006491710       -0.022099391\nOpp_Field_Goal        0.003617006        0.007415363\nOpp_Safety           -0.012361132       -0.028266233\nOpp_Touchdown         0.002475162        0.006787692\nSafety                0.001022146       -0.001257291\nTouchdown             0.005196036        0.020872169\n\nStd. Errors:\n                (Intercept) half_seconds_remaining yardline_100        down2\nField_Goal     9.973877e-03           2.056382e-05 0.0003265603 9.222868e-03\nOpp_Field_Goal 1.301499e-02           2.164403e-05 0.0003570708 3.525795e-03\nOpp_Safety     3.608208e-05           6.056503e-05 0.0009598729 1.676993e-05\nOpp_Touchdown  1.249336e-02           2.114300e-05 0.0003484688 4.994918e-03\nSafety         2.031640e-04           4.981585e-05 0.0010522061 1.154787e-04\nTouchdown      9.156701e-03           2.042802e-05 0.0003097515 1.026284e-02\n                      down3        down4  log_ydstogo down2:log_ydstogo\nField_Goal     9.165213e-03 4.368126e-03 0.0076911593      0.0084179580\nOpp_Field_Goal 3.705261e-03 2.529465e-03 0.0070133975      0.0086822558\nOpp_Safety     1.585732e-05 1.948469e-05 0.0002905214      0.0001058366\nOpp_Touchdown  4.972851e-03 3.315516e-03 0.0077647303      0.0088275451\nSafety         7.438725e-05 5.127217e-05 0.0007459947      0.0003197118\nTouchdown      1.038676e-02 2.457417e-03 0.0071809004      0.0078822666\n               down3:log_ydstogo down4:log_ydstogo yardline_100:down2\nField_Goal          0.0085571300      0.0099954257       0.0004123602\nOpp_Field_Goal      0.0086850330      0.0095492903       0.0003746628\nOpp_Safety          0.0001036511      0.0000755188       0.0009717132\nOpp_Touchdown       0.0090569632      0.0103499484       0.0003861318\nSafety              0.0002244464      0.0001505608       0.0011547005\nTouchdown           0.0080808936      0.0105894677       0.0003887755\n               yardline_100:down3 yardline_100:down4\nField_Goal           0.0004477493       0.0005864862\nOpp_Field_Goal       0.0003898272       0.0004537012\nOpp_Safety           0.0011035945       0.0013637166\nOpp_Touchdown        0.0004097842       0.0004720251\nSafety               0.0013038376       0.0015717666\nTouchdown            0.0004227704       0.0004995965\n\nResidual Deviance: 1111036 \nAIC: 1111192 \n\n\nYou see the usual type of output (coefficients, standard errors, deviance, AIC), but we see coefficient estimates for each next score outcome (except for the reference level No_Score).\nAlternatively, it can be helpful to visualize the implied relationships between the various features and the outcome probabilities using visualization techniques. In order to do this, we first need to get the fitted probabilities for each scoring event. For a nnet multinomial logistic regression model, we can use the predict() function with type = \"probs\" as an input to return the matrix of probabilities for each event:\n\nnext_score_probs &lt;- predict(init_ep_model, \n                            newdata = nfl_ep_model_data, type = \"probs\") |&gt;\n  as_tibble()\nnext_score_probs\n\n# A tibble: 422,904 × 7\n   No_Score Field_Goal Opp_Field_Goal Opp_Safety Opp_Touchdown  Safety Touchdown\n      &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 0.000970      0.204         0.157    0.00476          0.279 0.00427     0.350\n 2 0.000864      0.236         0.126    0.00217          0.222 0.00422     0.408\n 3 0.000912      0.234         0.119    0.00167          0.204 0.00445     0.437\n 4 0.000876      0.269         0.0957   0.000879         0.164 0.00398     0.465\n 5 0.00131       0.324         0.139    0.00133          0.236 0.00328     0.295\n 6 0.00132       0.274         0.131    0.00188          0.227 0.00460     0.361\n 7 0.00158       0.261         0.151    0.00282          0.262 0.00463     0.317\n 8 0.00164       0.238         0.184    0.00363          0.320 0.00471     0.247\n 9 0.00173       0.265         0.168    0.00226          0.285 0.00669     0.271\n10 0.00255       0.199         0.170    0.00655          0.291 0.00411     0.327\n# ℹ 422,894 more rows\n\n\nThe following code chunk joins these probabilities to the original dataset, and creates a visual that displays a smooth regression (we’ll cover this later) of the model’s probabilities as a function of certain inputs. Note: these visuals do not represent the exact relationship, but rather just a summary of the relationships. This code was used to generate the figures in my paper.\n\n# Create the facetted version for each events probability based on down:\nnfl_ep_model_data |&gt; \n  # Join the probs:\n  bind_cols(next_score_probs) |&gt;\n  # Only grab a subset of columns\n  dplyr::select(yardline_100, down, No_Score:Touchdown) |&gt;\n  pivot_longer(No_Score:Touchdown,\n               # Name of the column for the outcomes\n               names_to = \"next_score_type\",\n               # Name of the column for the predicted probabilities\n               values_to = \"pred_prob\") |&gt;\n  # Create a score value column to use for the color legend\n  mutate(event_value = case_when(\n                         next_score_type == \"No_Score\" ~ 0,\n                         next_score_type == \"Touchdown\" ~ 7,\n                         next_score_type == \"Field_Goal\" ~ 3,\n                         next_score_type == \"Safety\" ~ 2,\n                         next_score_type == \"Opp_Field_Goal\" ~ -3,\n                         next_score_type == \"Opp_Safety\" ~ -2,\n                         TRUE ~ -7),\n         # Label for down\n         down_label = case_when(\n                        down == 1 ~ \"1st down\",\n                        down == 2 ~ \"2nd down\",\n                        down == 3 ~ \"3rd down\",\n                        TRUE ~ \"4th down\")) |&gt;\n  ggplot(aes(x = yardline_100, y = pred_prob, color = event_value,\n             group = next_score_type)) + \n  geom_smooth(se = FALSE) + \n    ylim(0,1) + \n    facet_wrap(~down_label, ncol = 4) + \n  theme_bw() +\n  labs(x = \"Yards from opponent's end zone\", y = \"Predicted probability\") +\n  scale_color_gradient2(low = \"darkorange4\", mid = \"gray\",\n                        high = \"darkslateblue\", \n                        breaks = c(-7, -3, -2, 0, 2, 3, 7),\n                        labels=c(\" -Touchdown (-7) \", \" -Field Goal (-3) \",\n                                 \" -Safety (-2) \", \" No Score (0) \",\n                                 \" Safety (2) \", \" Field Goal (3) \", \n                                 \" Touchdown (7) \"),\n                        guide = guide_legend(title = NULL, ncol = 7,\n                                             reverse = TRUE,\n                                             override.aes = list(size = 5))) +\n  theme(legend.background = element_rect(fill = \"white\"),\n        axis.title = element_text(size = 18),\n        axis.text.y = element_text(size = 16),\n        axis.text.x = element_text(size = 10),\n        legend.position = \"bottom\",\n        strip.background = element_blank(),\n        strip.text = element_text(size = 18),\n        legend.text = element_text(size = 12))\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nWe can also create a figure that summarizes the relationships between different features with the actual variable of interest: expected points. The first step is to compute the expected points, which we can do using a simple line of code that weights each probability with a point value for the outcome:\n\nnext_score_probs &lt;- next_score_probs |&gt;\n  mutate(ep = Touchdown * 7 + Field_Goal * 3 + Safety * 2 +\n           Opp_Touchdown * -7 + Opp_Field_Goal * -3 + Opp_Safety * -2)\n\nAnd then use a similar approach as before, including historical models, to display the implied relationships:\n\n# Expected points relationships, for the historical models:\n\n# First the Carter model:\ncarter_data &lt;- tibble(\"yardline_100\" = c(95, 85, 75, 65, 55, 45, 35, 25, 15, 5),\n                      \"ep\" = c(-1.245, -.637, .236, .923, 1.538, 2.392, \n                               3.167, 3.681, 4.572, 6.041)) |&gt;\n  mutate(model = \"Carter\")\n\n# and Hidden Game of Football model:\nhgf_data &lt;- tibble(\"yardline_100\" = c(100, 75, 50, 25, 0),\n                   \"ep\" = c(-2, 0, 2, 4, 6)) |&gt;\n  mutate(model = \"Hidden Game of Football\")\n\n# Display our model's results by down and then compare to the historical\n# models from Carter and the Hidden Game of Football:\nnfl_ep_model_data |&gt; \n  bind_cols(next_score_probs) |&gt;\n  # Only grab a subset of columns\n  dplyr::select(yardline_100, down, ep) |&gt;\n  ggplot(aes(x = yardline_100, y = ep,\n             color = as.factor(down))) + \n  geom_smooth(size = 2) + \n  labs(x = \"Yards from opponent's end zone\",\n       y = \"Expected points value\",\n       color = \"Model\") +\n  theme_bw() + \n  scale_y_continuous(limits = c(-4, 6),breaks = seq(-4, 6, 2)) + \n  geom_line(data = bind_rows(carter_data, hgf_data),\n            aes(x = yardline_100, y = ep, color = model),\n            size = 2, linetype = \"dashed\") + \n  geom_point(data = bind_rows(carter_data, hgf_data),\n             aes(x = yardline_100, y = ep, color = model),\n             size = 5, alpha = 0.5) + \n  scale_x_continuous(breaks = seq(from = 5, to = 95, by = 10)) +\n  scale_color_manual(values = c(\"#0000FF\",\n                                \"#5537AA\",\n                                \"#AA6E55\",\n                                \"#FFA500\",\n                                \"seagreen4\",\n                                \"darkred\"),\n                     labels = c(\"Multilogit - 1st down\",\n                                \"Multilogit - 2nd down\",\n                                \"Multilogit - 3rd down\",\n                                \"Multilogit - 4th down\",\n                                \"Carter\",\n                                \"Hidden Game of Football\")) +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 12),\n        legend.text = element_text(size = 8),\n        legend.title = element_text(size = 10),\n        legend.position = \"bottom\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 2329 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demos/03-multinom-logit.html#cross-validation-calibration",
    "href": "demos/03-multinom-logit.html#cross-validation-calibration",
    "title": "Lecture 4: Multinomial Logistic Regression for Expected Points",
    "section": "Cross-validation calibration",
    "text": "Cross-validation calibration\nSince our goal relies on using the model’s probability estimates, we can evaluate the model similar to the expected goals model: via out-of-sample calibration. The notable difference is that we need to assess how well the model is calibrated for each scoring event. The following code generates the leave-one-year-out cross-validation predictions for each play in the dataset (note that the model fitting steps are printed for each season fold):\n\ninit_loso_cv_preds &lt;- \n  map_dfr(unique(nfl_ep_model_data$season), \n          function(x) {\n            # Separate test and training data:\n            test_data &lt;- nfl_ep_model_data |&gt; filter(season == x)\n            train_data &lt;- nfl_ep_model_data |&gt; filter(season != x)\n            # Fit multinomial logistic regression model:\n            ep_model &lt;- \n              multinom(Next_Score_Half ~ half_seconds_remaining + yardline_100 + down + \n                         log_ydstogo + log_ydstogo * down + yardline_100 * down, \n                       data = train_data, maxit = 300)\n            # Return dataset of class probabilities:\n            predict(ep_model, newdata = test_data, type = \"probs\") |&gt;\n              as_tibble() |&gt;\n              mutate(Next_Score_Half = test_data$Next_Score_Half,\n                     season = x)\n              })\n\n# weights:  98 (78 variable)\ninitial  value 747749.055246 \niter  10 value 581440.350658\niter  20 value 566007.216610\niter  30 value 553092.654185\niter  40 value 538119.354133\niter  50 value 536424.391744\niter  60 value 520828.612116\niter  70 value 511272.480459\niter  80 value 505399.169176\niter  90 value 504210.861998\niter 100 value 504171.076545\niter 110 value 504167.970371\niter 120 value 504167.510171\nfinal  value 504167.490166 \nconverged\n# weights:  98 (78 variable)\ninitial  value 748467.096091 \niter  10 value 582511.589203\niter  20 value 567340.012597\niter  30 value 553488.233716\niter  40 value 537655.594760\niter  50 value 534824.876335\niter  60 value 525502.198924\niter  70 value 512996.713239\niter  80 value 506475.015847\niter  90 value 504965.487881\niter 100 value 504938.379836\nfinal  value 504937.826673 \nconverged\n# weights:  98 (78 variable)\ninitial  value 747873.593495 \niter  10 value 581271.333157\niter  20 value 565846.033579\niter  30 value 553897.370602\niter  40 value 536428.400172\niter  50 value 533324.447646\niter  60 value 519623.381518\niter  70 value 510091.467079\niter  80 value 505678.094433\niter  90 value 504541.138811\niter 100 value 504525.471455\nfinal  value 504525.424807 \nconverged\n# weights:  98 (78 variable)\ninitial  value 748525.473395 \niter  10 value 584430.991679\niter  20 value 569735.869879\niter  30 value 558267.533816\niter  40 value 545227.965954\niter  50 value 542097.115990\niter  60 value 534837.983927\niter  70 value 514739.234466\niter  80 value 507703.178044\niter  90 value 505555.625847\niter 100 value 505532.168333\nfinal  value 505532.108496 \nconverged\n# weights:  98 (78 variable)\ninitial  value 749165.677834 \niter  10 value 586127.439746\niter  20 value 569455.903919\niter  30 value 555673.870004\niter  40 value 537108.662513\niter  50 value 535388.738604\niter  60 value 525520.820192\niter  70 value 510458.157722\niter  80 value 505818.284259\niter  90 value 505018.434102\niter 100 value 505008.890377\nfinal  value 505008.882666 \nconverged\n# weights:  98 (78 variable)\ninitial  value 750086.093335 \niter  10 value 585080.586820\niter  20 value 570595.228537\niter  30 value 555605.385864\niter  40 value 543202.950882\niter  50 value 541039.078280\niter  60 value 527670.700071\niter  70 value 516882.317084\niter  80 value 510327.264273\niter  90 value 507410.311047\niter 100 value 507383.469803\niter 110 value 507382.051725\nfinal  value 507381.972455 \nconverged\n# weights:  98 (78 variable)\ninitial  value 749346.647478 \niter  10 value 586291.599094\niter  20 value 571463.633794\niter  30 value 555251.569408\niter  40 value 540714.413545\niter  50 value 538208.068421\niter  60 value 524922.778076\niter  70 value 511749.141809\niter  80 value 506515.736547\niter  90 value 505678.465320\niter 100 value 505662.467396\nfinal  value 505662.424972 \nconverged\n# weights:  98 (78 variable)\ninitial  value 749852.584117 \niter  10 value 585091.048570\niter  20 value 570487.939148\niter  30 value 554224.337618\niter  40 value 545207.608450\niter  50 value 542998.382583\niter  60 value 523574.947287\niter  70 value 512352.949701\niter  80 value 507690.704984\niter  90 value 506457.363312\niter 100 value 506441.642486\nfinal  value 506441.579198 \nconverged\n# weights:  98 (78 variable)\ninitial  value 745867.360132 \niter  10 value 581916.027979\niter  20 value 566655.115894\niter  30 value 554775.921190\niter  40 value 536693.143905\niter  50 value 534378.950264\niter  60 value 527881.563685\niter  70 value 513033.555210\niter  80 value 505528.391065\niter  90 value 504279.763183\niter 100 value 504257.414205\nfinal  value 504257.309441 \nconverged\n# weights:  98 (78 variable)\ninitial  value 746513.402301 \niter  10 value 585896.386604\niter  20 value 571358.916426\niter  30 value 556119.424347\niter  40 value 541842.753103\niter  50 value 539199.324648\niter  60 value 521593.725464\niter  70 value 510590.617369\niter  80 value 504547.807280\niter  90 value 503510.806223\niter 100 value 503498.307119\nfinal  value 503498.009677 \nconverged\n# weights:  98 (78 variable)\ninitial  value 745884.873323 \niter  10 value 582559.463464\niter  20 value 567597.676369\niter  30 value 553086.905300\niter  40 value 539643.354316\niter  50 value 537190.694757\niter  60 value 530614.346149\niter  70 value 516141.509588\niter  80 value 506361.234224\niter  90 value 503662.111159\niter 100 value 503639.873014\nfinal  value 503639.857722 \nconverged\n\n\nWe can then generate the calibration summary as before, with the caveat we need to do this for each outcome probability. Since the above code returns a dataset with a column for each outcome separately, we need to pivot the dataset from wide to long so that there is a row for each play-outcome combination. We can do this using the useful pivot_longer() function as shown below, with the same summary steps as before:\n\nep_cv_loso_calibration_results &lt;- init_loso_cv_preds |&gt;\n  # First specify which columns to turn into rows\n  pivot_longer(No_Score:Touchdown,\n               # Name of the column for the outcomes\n               names_to = \"next_score_type\",\n               # Name of the column for the predicted probabilities\n               values_to = \"pred_prob\") |&gt;\n  # And then the same steps as before but now with grouping by score outcome:\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) |&gt;\n  group_by(next_score_type, bin_pred_prob) |&gt;\n  summarize(n_plays = n(), \n            n_scoring_event = length(which(Next_Score_Half == next_score_type)),\n            bin_actual_prob = n_scoring_event / n_plays,\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_plays),\n            .groups = \"drop\") |&gt;\n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0))\n\nAnd then with this dataset we can create calibration plots for each outcome, using similar code as before:\n\nep_cv_loso_calibration_results |&gt;\n  mutate(next_score_type = fct_relevel(next_score_type,\n                                       \"Opp_Safety\", \"Opp_Field_Goal\", \n                                       \"Opp_Touchdown\", \"No_Score\", \"Safety\", \n                                       \"Field_Goal\", \"Touchdown\"),\n  next_score_type = fct_recode(next_score_type, \n                               \"-Field Goal (-3)\" = \"Opp_Field_Goal\",\n                               \"-Safety (-2)\" = \"Opp_Safety\", \n                               \"-Touchdown (-7)\" = \"Opp_Touchdown\",\n                               \"Field Goal (3)\" = \"Field_Goal\", \n                               \"No Score (0)\" = \"No_Score\",\n                               \"Touchdown (7)\" = \"Touchdown\", \n                               \"Safety (2)\" = \"Safety\")) |&gt;\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point(size = 0.5) +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) + \n  coord_equal() +   \n  scale_x_continuous(limits = c(0, 1)) + \n  scale_y_continuous(limits = c(0, 1)) + \n  labs(x = \"Estimated next score probability\", \n       y = \"Observed next score probability\") + \n  theme_bw() + \n  theme(strip.background = element_blank(), \n        axis.text.x = element_text(angle = 90)) +\n  facet_wrap(~ next_score_type, ncol = 4)\n\n\n\n\n\n\n\n\nWhat stands out for you when inspecting the different calibration plots?"
  },
  {
    "objectID": "demos/03-multinom-logit.html#recap",
    "href": "demos/03-multinom-logit.html#recap",
    "title": "Lecture 4: Multinomial Logistic Regression for Expected Points",
    "section": "Recap",
    "text": "Recap\n\nIntroduced fitting and interpreting a multinomial logistic regression model\nWalked through steps for performing cross-validation calibration for multiple outcomes"
  },
  {
    "objectID": "demos/01-logit-expected-goals.html",
    "href": "demos/01-logit-expected-goals.html",
    "title": "Lecture 2: Building an Expected Goals Model",
    "section": "",
    "text": "The goal of this demo is to introduce the basic steps of building an expected goals model with logistic regression in R. In this demo, we’ll use a dataset of NHL shot attempts during the 2023-2024 NHL season (accessed via hockeyR) to build an expected goals model for hockey. The code used to create this dataset is available on Canvas. The following code chunk reads in the data (notice the use of the here package for reading in the data) and displays a subset of the data:\n\nlibrary(tidyverse)\n\n# This assumes the dataset is saved within the data folder of the directory this\n# file is loaded in, you can change this depending on where you save the data!\nmodel_nhl_shot_data &lt;- read_csv(here::here(\"data/model_nhl_shot_data.csv\"))\nhead(model_nhl_shot_data)\n\n# A tibble: 6 × 11\n    game_id period shooting_player shooting_team goalie_name goalie_team x_fixed\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1    2.02e9      1 Bryan Rust      Pittsburgh P… Petr Mrazek Chicago Bl…     -51\n2    2.02e9      1 Kevin Korchins… Chicago Blac… Tristan Ja… Pittsburgh…     -55\n3    2.02e9      1 Noel Acciari    Pittsburgh P… Petr Mrazek Chicago Bl…      75\n4    2.02e9      1 Wyatt Kaiser    Chicago Blac… Tristan Ja… Pittsburgh…     -39\n5    2.02e9      1 Alex Vlasic     Chicago Blac… Tristan Ja… Pittsburgh…     -36\n6    2.02e9      1 Marcus Petters… Pittsburgh P… Petr Mrazek Chicago Bl…      32\n# ℹ 4 more variables: y_fixed &lt;dbl&gt;, shot_distance &lt;dbl&gt;, shot_angle &lt;dbl&gt;,\n#   is_goal &lt;dbl&gt;\n\n\nNOTE: For hockey fans, for simplicity we are only considering even-strength shot attempts with a goalie in net that were either goals or saved by the goalie (i.e., not considering shots blocked by non-goalie defenders or shots that missed wide of the net).\nBefore diving into any modeling, we should always start with exploratory data analysis (EDA), and visualize the data! The following figure displays the scatterplot of the shot attempts using the provided x (x_fixed) and y (y_fixed) coordinates, colored by the shot outcome (is_goal):\n\nmodel_nhl_shot_data |&gt;\n  # Make a scatterplot of the shot attempts\n  ggplot(aes(x = x_fixed, y = y_fixed, color = as.factor(is_goal))) +\n  # Make sure to modify the alpha!\n  geom_point(alpha = .25) +\n  labs(x = \"x coordinate\", y = \"y coordinate\", \n       color = \"Shot outcome\") +\n  # Use the ggthemes package for colorblind friendly palette\n  ggthemes::scale_color_colorblind(labels = c(\"Save\", \"Goal\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFrom this figure, we can see a higher density of shots that were goals (indicated by orange color) closer to the two empty spaces along the endpoints of the horizontal line where y = 0 corresponding to the locations of the nets. We also see that the goals tend to be more directly in front of these locations relative to the sides. The figure provides us with some evidence that both the shot distance and angle are likely useful in predicting the probability of a goal."
  },
  {
    "objectID": "demos/01-logit-expected-goals.html#introduction",
    "href": "demos/01-logit-expected-goals.html#introduction",
    "title": "Lecture 2: Building an Expected Goals Model",
    "section": "",
    "text": "The goal of this demo is to introduce the basic steps of building an expected goals model with logistic regression in R. In this demo, we’ll use a dataset of NHL shot attempts during the 2023-2024 NHL season (accessed via hockeyR) to build an expected goals model for hockey. The code used to create this dataset is available on Canvas. The following code chunk reads in the data (notice the use of the here package for reading in the data) and displays a subset of the data:\n\nlibrary(tidyverse)\n\n# This assumes the dataset is saved within the data folder of the directory this\n# file is loaded in, you can change this depending on where you save the data!\nmodel_nhl_shot_data &lt;- read_csv(here::here(\"data/model_nhl_shot_data.csv\"))\nhead(model_nhl_shot_data)\n\n# A tibble: 6 × 11\n    game_id period shooting_player shooting_team goalie_name goalie_team x_fixed\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1    2.02e9      1 Bryan Rust      Pittsburgh P… Petr Mrazek Chicago Bl…     -51\n2    2.02e9      1 Kevin Korchins… Chicago Blac… Tristan Ja… Pittsburgh…     -55\n3    2.02e9      1 Noel Acciari    Pittsburgh P… Petr Mrazek Chicago Bl…      75\n4    2.02e9      1 Wyatt Kaiser    Chicago Blac… Tristan Ja… Pittsburgh…     -39\n5    2.02e9      1 Alex Vlasic     Chicago Blac… Tristan Ja… Pittsburgh…     -36\n6    2.02e9      1 Marcus Petters… Pittsburgh P… Petr Mrazek Chicago Bl…      32\n# ℹ 4 more variables: y_fixed &lt;dbl&gt;, shot_distance &lt;dbl&gt;, shot_angle &lt;dbl&gt;,\n#   is_goal &lt;dbl&gt;\n\n\nNOTE: For hockey fans, for simplicity we are only considering even-strength shot attempts with a goalie in net that were either goals or saved by the goalie (i.e., not considering shots blocked by non-goalie defenders or shots that missed wide of the net).\nBefore diving into any modeling, we should always start with exploratory data analysis (EDA), and visualize the data! The following figure displays the scatterplot of the shot attempts using the provided x (x_fixed) and y (y_fixed) coordinates, colored by the shot outcome (is_goal):\n\nmodel_nhl_shot_data |&gt;\n  # Make a scatterplot of the shot attempts\n  ggplot(aes(x = x_fixed, y = y_fixed, color = as.factor(is_goal))) +\n  # Make sure to modify the alpha!\n  geom_point(alpha = .25) +\n  labs(x = \"x coordinate\", y = \"y coordinate\", \n       color = \"Shot outcome\") +\n  # Use the ggthemes package for colorblind friendly palette\n  ggthemes::scale_color_colorblind(labels = c(\"Save\", \"Goal\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nFrom this figure, we can see a higher density of shots that were goals (indicated by orange color) closer to the two empty spaces along the endpoints of the horizontal line where y = 0 corresponding to the locations of the nets. We also see that the goals tend to be more directly in front of these locations relative to the sides. The figure provides us with some evidence that both the shot distance and angle are likely useful in predicting the probability of a goal."
  },
  {
    "objectID": "demos/01-logit-expected-goals.html#simple-logistic-regression-model-of-distance",
    "href": "demos/01-logit-expected-goals.html#simple-logistic-regression-model-of-distance",
    "title": "Lecture 2: Building an Expected Goals Model",
    "section": "Simple logistic regression model of distance",
    "text": "Simple logistic regression model of distance\nWe’ll start with a simple logistic regression model to estimate the goal probability of a shot as just a function of the shot distance (in feet). More specifically, we model the log-odds of a goal with a linear model:\n\\[\n\\log \\Big[ \\frac{Pr(S = 1 |\\ distance)}{Pr(S = 0 |\\ distance)} \\Big] = \\beta_0 + \\beta_1 \\cdot distance\n\\]\nBefore we fit this model, we’ll continue with some basic EDA by visualizing the relationship between the shot outcome (is_goal) and the distance (shot_distance). The following figure displays a stacked histogram of the shot distance by outcome, indicating that the majority of goals occur at much shorter distances. While the majority of shot attempts do not result in a goal, we can imagine that the probability of goal increases as the shot distance decreases.\n\nmodel_nhl_shot_data |&gt;\n  ggplot(aes(x = shot_distance, fill = as.factor(is_goal))) +\n  geom_histogram() + \n  labs(x = \"Shot distance (in feet)\",\n       y = \"Count\", fill = \"Shot outcome\") +\n  ggthemes::scale_fill_colorblind(labels = c(\"Save\", \"Goal\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNext, we’re going to create an empirical logit plot to assess the linearity assumption of the logistic regression model, i.e., to check if the log(odds) is a linear function of distance. To do this, we need to bin the continuous variable of distance and then compute the log(odds) (or equivalently log(goals / saves)) within each bin. The following code chunk uses the cut_number() function to divide the shot_distance variable into bins with approximately equal number of observations. NOTE: Due to the potential for the observed proportion to be 0, in order to avoid log(0) we set a floor of 0.0001 using the pmax() function.\n\nshot_distance_summary &lt;- model_nhl_shot_data |&gt;\n  mutate(dist_bin = cut_number(shot_distance, 10)) |&gt;\n  group_by(dist_bin) |&gt;\n  summarize(goal_p = mean(is_goal),\n            n_shots = n(),\n            dist_midpoint = median(shot_distance),\n            .groups = \"drop\") |&gt;\n  mutate(goal_p = pmax(goal_p, 0.0001),\n         emp_logit = log(goal_p / (1 - goal_p)))\nshot_distance_summary\n\n# A tibble: 10 × 5\n   dist_bin     goal_p n_shots dist_midpoint emp_logit\n   &lt;fct&gt;         &lt;dbl&gt;   &lt;int&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 [1,9.1]     0.134      9822           7.2     -1.86\n 2 (9.1,13.9]  0.107      9848          11.2     -2.12\n 3 (13.9,20.4] 0.0931     9633          17.1     -2.28\n 4 (20.4,27.2] 0.0778     9898          24       -2.47\n 5 (27.2,33.2] 0.0599     9745          30.4     -2.75\n 6 (33.2,39.1] 0.0374     9789          36.2     -3.25\n 7 (39.1,45.9] 0.0232     9640          42.2     -3.74\n 8 (45.9,54.5] 0.0162     9800          50       -4.10\n 9 (54.5,62.6] 0.0156     9908          58.5     -4.14\n10 (62.6,189]  0.00450    9551          69.4     -5.40\n\n\nNext, we can make a simple plot with the empirical logit values along the y-axis against the shot distance (via the midpoints of the bins) on the x-axis. We can use geom_smooth() to fit a flexible trend to help us determine if the relationship is linear or not.\n\nshot_distance_summary |&gt;\n  ggplot(aes(x = dist_midpoint, y = emp_logit)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Shot distance (midpoint of bins)\",\n       y = \"Empirical logits\") +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis figure looks reasonable in terms of indicating a linear relationship between the log(odds) of a goal and the shot distance. We see that the log(odds) decreases as the shot distance increases, i.e., the probability of a goal decreases as the shot distance increases.\nWith the assumption of a linear relationship checked off, we’ll now fit the logistic regression model using the glm() function in R. This is the general use function for fitting any generalized linear model (GLM) in R, where you just need to specify the distribution with the family argument. Although in this case we are technically modeling a variable (is_goal) that follows the Bernoulli distribution, we set family = \"binomial\" to fit the logistic regression model (since it’s a Binomial distribution with \\(n = 1\\)).\n\ninit_logit &lt;- glm(is_goal ~ shot_distance, data = model_nhl_shot_data,\n                  family = \"binomial\")\n\nWe can then view the summary of the model including coefficient estimates, deviance, etc. via the summary() function:\n\nsummary(init_logit)\n\n\nCall:\nglm(formula = is_goal ~ shot_distance, family = \"binomial\", data = model_nhl_shot_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6258  -0.4144  -0.2786  -0.1774   4.1872  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.4826561  0.0246069  -60.25   &lt;2e-16 ***\nshot_distance -0.0483940  0.0009567  -50.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 42736  on 97633  degrees of freedom\nResidual deviance: 39264  on 97632  degrees of freedom\nAIC: 39268\n\nNumber of Fisher Scoring iterations: 6\n\n\nUsing this model, we can create a figure displaying the predicted probability of a goal as a function of shot distance. We view this figure below, with the observed goals and saves marked as points at one and zero respectively. As expected, the probability of a goal is highest for the shortest shots. And we can clearly see that the vast majority of goals are within seventy-five feet, while saved shots span the entire range.\n\nmodel_nhl_shot_data |&gt;\n  mutate(xg = init_logit$fitted.values)  |&gt;\n  ggplot(aes(x = shot_distance)) +\n  geom_jitter(aes(y = is_goal,\n                  color = as.factor(is_goal)), \n              width = 0, height = .01,\n             size = .5, alpha = 0.15) +\n  geom_line(aes(y = xg), color = \"blue\") +\n  ggthemes::scale_color_colorblind(guide = \"none\") +\n  labs(x = \"Shot distance (in feet)\",\n       y = \"Predicted probability of goal (aka expected goals)\") +\n  theme_bw()"
  },
  {
    "objectID": "demos/01-logit-expected-goals.html#likelihood-based-approaches-for-model-evaluation-and-comparison",
    "href": "demos/01-logit-expected-goals.html#likelihood-based-approaches-for-model-evaluation-and-comparison",
    "title": "Lecture 2: Building an Expected Goals Model",
    "section": "Likelihood-based approaches for model evaluation and comparison",
    "text": "Likelihood-based approaches for model evaluation and comparison\nWe begin to assess this fit using traditional approaches based on likelihood criterion. Using the deviance, there are two tests we can consider:\n\nGoodness-of-Fit test: \\(H_0\\): fitted model vs \\(H_A\\): saturated model\nDrop-in-Deviance test: \\(H_0\\): reduced model vs \\(H_A\\): larger model (where reduced model is nested within larger model, e.g., reduced model contains subset of larger model predictors)\n\nWe’ll start with the goodness-of-fit test, which is testing the null hypothesis that the current model under consideration is a good fit. If we have evidence to suggest the null hypothesis is rejected, then we may be observing a lack-of-fit. We can perform this test easily in R using the pchisq() function to compution the appropriate p-value based on the assumption that under the null hypothesis, the residual deviance \\(\\sim \\chi^2\\) distribution with \\(n - p\\) degrees of freedom:\n\n1 - pchisq(init_logit$deviance, init_logit$df.residual)\n\n[1] 1\n\n\nBased on this result, we have insufficient evidence to suggest a lack-of-fit. If the p-value was below an error rate threshold (e.g., 0.05) then we would reject the null, but we would not know what is driving the lack-of-fit. It may be that we’re missing important covariates and interactions, suffering from outliers, or other concerns with the distribution fit. Just because we fail to reject the null hypothesis in this example does NOT mean our model is optimal! There are other ways we will be able to improve this model, and the use of the \\(\\chi^2\\) distribution is just an approximation (not guaranteed to be correct…).\nWe can use drop-in-deviance test to compare the fitted model against the null model (intercept-only), using the same function. Under the null, the difference in the reduced model deviance and larger model deviance follows a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the degrees of freedom between the two models. In other words, the degrees of freedom for this test is simply the number of parameters in the larger model that are not in the reduced model. The following code chunk shows how to perform this test using the pchisq function:\n\n1 - pchisq(init_logit$null.deviance - init_logit$deviance, \n           init_logit$df.null - init_logit$df.residual)\n\n[1] 0\n\n\nBased on this p-value, we have evidence in favor of the model with the shot distance variable.\nAnother way we can perform this test is to initially fit an intercept-only model (the null model) and then use the anova() function with test = \"Chisq\" specified to achieve the same result:\n\nnull_logit &lt;- glm(is_goal ~ 1, data = model_nhl_shot_data, family = \"binomial\")\nanova(null_logit, init_logit, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: is_goal ~ 1\nModel 2: is_goal ~ shot_distance\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1     97633      42736                          \n2     97632      39264  1   3471.9 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can use this same test to compare the fit of a model with another variable, such as shot_angle:\n\ndist_angle_logit &lt;- glm(is_goal ~ shot_distance + shot_angle, \n                        data = model_nhl_shot_data, family = \"binomial\")\nanova(init_logit, dist_angle_logit, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: is_goal ~ shot_distance\nModel 2: is_goal ~ shot_distance + shot_angle\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1     97632      39264                          \n2     97631      38620  1   644.38 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBased on this test, we have sufficient evidence to suggest the larger model with both distance and angle is a better fit than the model with only shot distance.\nThe drop-in-deviance test is only appropriate for evaluating models that are nested within another. It is not an approach we can take for general comparison when considering non-nested models (e.g., two models with distinct, non-overlapping features). However, we can use information criteria measures when comparing non-nested models which consider the model’s likelihood penalized by the number of parameters in some way (i.e., penalty for model complexity). The two common information criterions are:\n\nAkaike Information Criterion (AIC) = \\(-2 \\times\\) log(Lik) \\(+ 2p\\), i.e., the more variables we use the higher the AIC\nBayesian Information Criterion (BIC) = \\(-2 \\times\\) log(Lik) \\(+ p \\log(n)\\), i.e., log of number of observations places greater penalty on each additional variable\n\nIn these forms, for both AIC and BIC, lower values are better.\nBy default, the AIC is reported in the summary() output for GLMs:\n\nsummary(dist_angle_logit)\n\n\nCall:\nglm(formula = is_goal ~ shot_distance + shot_angle, family = \"binomial\", \n    data = model_nhl_shot_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8111  -0.3907  -0.2648  -0.1798   4.1795  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.8914673  0.0326450  -27.31   &lt;2e-16 ***\nshot_distance -0.0513284  0.0009580  -53.58   &lt;2e-16 ***\nshot_angle    -0.0146870  0.0006025  -24.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 42736  on 97633  degrees of freedom\nResidual deviance: 38620  on 97631  degrees of freedom\nAIC: 38626\n\nNumber of Fisher Scoring iterations: 6\n\n\nYou can also use the AIC() and BIC() functions to return the respective values for a given model:\n\nAIC(dist_angle_logit)\n\n[1] 38625.64\n\n\nThe above output matches the previous summary display. And below we can see how the model with both features has a better BIC than the model with only distance:\n\nBIC(init_logit)\n\n[1] 39287\n\nBIC(dist_angle_logit)\n\n[1] 38654.11"
  },
  {
    "objectID": "demos/01-logit-expected-goals.html#recap",
    "href": "demos/01-logit-expected-goals.html#recap",
    "title": "Lecture 2: Building an Expected Goals Model",
    "section": "Recap",
    "text": "Recap\n\nCovered basics of fitting logistic regression in R using glm()\nEvaluated and compared models using likelihood-based approaches"
  },
  {
    "objectID": "demos/00-into-tidyverse.html",
    "href": "demos/00-into-tidyverse.html",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/00-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "href": "demos/00-into-tidyverse.html#what-is-exploratory-data-analysis-eda",
    "title": "Demo 01: Into the tidyverse",
    "section": "",
    "text": "(Broadly speaking) EDA = questions about data + wrangling + visualization\nR for Data Science: “EDA is a state of mind”, an iterative cycle:\n\ngenerate questions\nanswer via transformations and visualizations\n\nExample of questions?\n\nWhat type of variation do the variables display?\nWhat type of relationships exist between variables?\n\nGoal: develop understanding and become familiar with your data\n\nEDA is NOT a replacement for statistical inference and learning\nEDA is an important and necessary step to build intuition\n\nWe tackle the challenges of EDA with a data science workflow. An example of this according to Hadley Wickham in R for Data Science:\n\n\n\n\n\nAspects of data wrangling:\n\nimport: reading in data (e.g., read_csv())\ntidy: rows = observations, columns = variables (i.e. tabular data)\ntransform: filter observations, create new variables, summarize, etc."
  },
  {
    "objectID": "demos/00-into-tidyverse.html#working-with-penguins",
    "href": "demos/00-into-tidyverse.html#working-with-penguins",
    "title": "Demo 01: Into the tidyverse",
    "section": "Working with penguins",
    "text": "Working with penguins\nIn R, there are many libraries or packages/groups of programs that are not permanently stored in R, so we have to load them when we want to use them. You can load an R package by typing library(package_name). (Sometimes we need to download/install the package first, as described in HW0.)\nThroughout this demo we will use the palmerpenguins dataset. To access the data, you will need to install the palmerpenguins package:\n\ninstall.packages(\"palmerpenguins\")\n\nImport the penguins dataset by loading the palmerpenguins package using the library function and then access the data with the data() function:\n\nlibrary(palmerpenguins) \ndata(penguins)\n\nView some basic info about the penguins dataset:\n\n# displays same info as c(nrow(penguins), ncol(penguins))\ndim(penguins) \n\n[1] 344   8\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\ntbl (pronounced tibble) is the tidyverse way of storing tabular data, like a spreadsheet or data.frame\nI assure you that you’ll run into errors as you code in R; in fact, my attitude as a coder is that something is wrong if I never get any errors while working on a project. When you run into an error, your first reaction may be to panic and post a question to Piazza. However, checking help documentation in R can be a great way to figure out what’s going wrong. (For good or bad, I end up having to read help documentation almost every day of my life - because, well, I regularly make mistakes in R.)\nLook at the help documentation for penguins by typing help(penguins) in the Console. What are the names of the variables in this dataset? How many observations are in this dataset?\n\nhelp(penguins)\n\nYou should always look at your data before doing anything: view the first 6 (by default) rows with head()\n\nhead(penguins) # Try just typing penguins into your console, what happens?\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIs our penguins dataset tidy?\n\nEach row = a single penguin\nEach column = different measurement about the penguins (can print out column names directly with colnames(penguins) or names(penguins))\n\nWe’ll now explore differences among the penguins using the tidyverse."
  },
  {
    "objectID": "demos/00-into-tidyverse.html#let-the-data-wrangling-begin",
    "href": "demos/00-into-tidyverse.html#let-the-data-wrangling-begin",
    "title": "Demo 01: Into the tidyverse",
    "section": "Let the data wrangling begin…",
    "text": "Let the data wrangling begin…\nFirst, load the tidyverse for exploring the data - and do NOT worry about the warning messages that will pop-up! Warning messages will tell you when other packages that are loaded may have functions replaced with the most recent package you’ve loaded. In general though, you should just be concerned when an error message pops up (errors are different than warnings!).\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe’ll start by summarizing continuous (e.g., bill_length_mm, flipper_length_mm) and categorical (e.g., species, island) variables in different ways.\nWe can compute summary statistics for continuous variables with the summary() function:\n\nsummary(penguins$bill_length_mm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  32.10   39.23   44.45   43.92   48.50   59.60       2 \n\n\nCompute counts of categorical variables with table() function:\n\ntable(\"island\" = penguins$island) # be careful it ignores NA values!\n\nisland\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n\nHow do we remove the penguins with missing bill_length_mm values? Within the tidyverse, dplyr is a package with functions for data wrangling (because it’s within the tidyverse that means you do NOT have to load it separately with library(dplyr) after using library(tidyverse)!). It’s considered a “grammar of data manipulation”: dplyr functions are verbs, datasets are nouns.\nWe can filter() our dataset to choose observations meeting conditions:\n\nclean_penguins &lt;- filter(penguins, !is.na(bill_length_mm))\n# Use help(is.na) to see what it returns. And then observe \n# that the ! operator means to negate what comes after it.\n# This means !TRUE == FALSE (i.e., opposite of TRUE is equal to FALSE).\nnrow(penguins) - nrow(clean_penguins) # Difference in rows\n\n[1] 2\n\n\nIf we want to only consider a subset of columns in our data, we can select() variables of interest:\n\nsel_penguins &lt;- select(clean_penguins, species, island, bill_length_mm, flipper_length_mm)\nhead(sel_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species island    bill_length_mm flipper_length_mm\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;             &lt;int&gt;\n1 Adelie  Torgersen           39.1               181\n2 Adelie  Torgersen           39.5               186\n3 Adelie  Torgersen           40.3               195\n\n\nWe can arrange() our dataset to sort observations by variables:\n\nbill_penguins &lt;- arrange(sel_penguins, desc(bill_length_mm)) # use desc() for descending order\nhead(bill_penguins, n = 3)\n\n# A tibble: 3 × 4\n  species   island bill_length_mm flipper_length_mm\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;\n1 Gentoo    Biscoe           59.6               230\n2 Chinstrap Dream            58                 181\n3 Gentoo    Biscoe           55.9               228\n\n\nWe can summarize() our dataset to one row based on functions of variables:\n\nsummarize(bill_penguins, max(bill_length_mm), median(flipper_length_mm))\n\n# A tibble: 1 × 2\n  `max(bill_length_mm)` `median(flipper_length_mm)`\n                  &lt;dbl&gt;                       &lt;dbl&gt;\n1                  59.6                         197\n\n\nWe can mutate() our dataset to create new variables:\n\nnew_penguins &lt;- mutate(bill_penguins, \n                       bill_flipper_ratio = bill_length_mm / flipper_length_mm,\n                       flipper_bill_ratio = flipper_length_mm / bill_length_mm)\nhead(new_penguins, n = 1)\n\n# A tibble: 1 × 6\n  species island bill_length_mm flipper_length_mm bill_flipper_ratio\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;             &lt;int&gt;              &lt;dbl&gt;\n1 Gentoo  Biscoe           59.6               230              0.259\n# ℹ 1 more variable: flipper_bill_ratio &lt;dbl&gt;\n\n\nHow do we perform several of these actions?\n\nhead(arrange(select(mutate(filter(penguins, !is.na(flipper_length_mm)), bill_flipper_ratio = bill_length_mm / flipper_length_mm), species, island, bill_flipper_ratio), desc(bill_flipper_ratio)), n = 1)\n\n# A tibble: 1 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n\n\nThat’s awfully annoying to do, and also difficult to read…"
  },
  {
    "objectID": "demos/00-into-tidyverse.html#enter-the-pipeline",
    "href": "demos/00-into-tidyverse.html#enter-the-pipeline",
    "title": "Demo 01: Into the tidyverse",
    "section": "Enter the pipeline",
    "text": "Enter the pipeline\nThe |&gt; (pipe) operator is used in the to chain commands together. Note: you can also use the tidyverse pipe %&gt;% (from magrittr), but |&gt; is the built-in pipe that is native to new versions of R without loading the tidyverse.\n|&gt; directs the data analyis pipeline: output of one function pipes into input of the next function\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  head(n = 5)\n\n# A tibble: 5 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.270\n4 Chinstrap Dream               0.270\n5 Chinstrap Dream               0.268"
  },
  {
    "objectID": "demos/00-into-tidyverse.html#more-pipeline-actions",
    "href": "demos/00-into-tidyverse.html#more-pipeline-actions",
    "title": "Demo 01: Into the tidyverse",
    "section": "More pipeline actions!",
    "text": "More pipeline actions!\nInstead of head(), we can slice() our dataset to choose the observations based on the position\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  mutate(bill_flipper_ratio = bill_length_mm / flipper_length_mm) |&gt;\n  select(species, island, bill_flipper_ratio) |&gt;\n  arrange(desc(bill_flipper_ratio)) |&gt;\n  slice(c(1, 2, 10, 100))\n\n# A tibble: 4 × 3\n  species   island bill_flipper_ratio\n  &lt;fct&gt;     &lt;fct&gt;               &lt;dbl&gt;\n1 Chinstrap Dream               0.320\n2 Chinstrap Dream               0.275\n3 Chinstrap Dream               0.264\n4 Gentoo    Biscoe              0.227"
  },
  {
    "objectID": "demos/00-into-tidyverse.html#grouped-operations",
    "href": "demos/00-into-tidyverse.html#grouped-operations",
    "title": "Demo 01: Into the tidyverse",
    "section": "Grouped operations",
    "text": "Grouped operations\nWe group_by() to split our dataset into groups based on a variable’s values\n\npenguins |&gt;\n  filter(!is.na(flipper_length_mm)) |&gt;\n  group_by(island) |&gt;\n  summarize(n_penguins = n(), #counts number of rows in group\n            ave_flipper_length = mean(flipper_length_mm), \n            sum_bill_depth = sum(bill_depth_mm),\n            .groups = \"drop\") |&gt; # all levels of grouping dropping\n  arrange(desc(n_penguins)) |&gt;\n  slice(1:5)\n\n# A tibble: 3 × 4\n  island    n_penguins ave_flipper_length sum_bill_depth\n  &lt;fct&gt;          &lt;int&gt;              &lt;dbl&gt;          &lt;dbl&gt;\n1 Biscoe           167               210.          2651.\n2 Dream            124               193.          2275.\n3 Torgersen         51               191.           940.\n\n\n\ngroup_by() is only useful in a pipeline (e.g. with summarize()), and pay attention to its behavior\nspecify the .groups field to decide if observations remain grouped or not after summarizing (you can also use ungroup() for this as well)"
  },
  {
    "objectID": "demos/00-into-tidyverse.html#putting-it-all-together",
    "href": "demos/00-into-tidyverse.html#putting-it-all-together",
    "title": "Demo 01: Into the tidyverse",
    "section": "Putting it all together…",
    "text": "Putting it all together…\nAs your own exercise, create a tidy dataset where each row == an island with the following variables:\n\nnumber of penguins,\nnumber of unique species on the island (see help(unique)),\naverage body_mass_g,\nvariance (see help(var)) of bill_depth_mm\n\nPrior to making those variables, make sure to filter missings and also only consider female penguins. Then arrange the islands in order of the average body_mass_g:\n\n# INSERT YOUR CODE HERE"
  },
  {
    "objectID": "demos/02-calibration-cv.html",
    "href": "demos/02-calibration-cv.html",
    "title": "Lecture 3: Calibration and Cross-Validation",
    "section": "",
    "text": "The goal of this demo is to walk through the process of evaluating logistic regression predicted probability estimates with calibration, as well as a demonstration of implementing cross-validation. In this demo, we’ll again use a dataset of NHL shot attempts during the 2023-2024 NHL season (accessed via hockeyR) to evaluate an expected goals model for hockey. The following code chunk reads in the data and displays a subset of the data:\n\nlibrary(tidyverse)\n\nmodel_nhl_shot_data &lt;- read_csv(here::here(\"data/model_nhl_shot_data.csv\"))\nhead(model_nhl_shot_data)\n\n# A tibble: 6 × 11\n    game_id period shooting_player shooting_team goalie_name goalie_team x_fixed\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1    2.02e9      1 Bryan Rust      Pittsburgh P… Petr Mrazek Chicago Bl…     -51\n2    2.02e9      1 Kevin Korchins… Chicago Blac… Tristan Ja… Pittsburgh…     -55\n3    2.02e9      1 Noel Acciari    Pittsburgh P… Petr Mrazek Chicago Bl…      75\n4    2.02e9      1 Wyatt Kaiser    Chicago Blac… Tristan Ja… Pittsburgh…     -39\n5    2.02e9      1 Alex Vlasic     Chicago Blac… Tristan Ja… Pittsburgh…     -36\n6    2.02e9      1 Marcus Petters… Pittsburgh P… Petr Mrazek Chicago Bl…      32\n# ℹ 4 more variables: y_fixed &lt;dbl&gt;, shot_distance &lt;dbl&gt;, shot_angle &lt;dbl&gt;,\n#   is_goal &lt;dbl&gt;"
  },
  {
    "objectID": "demos/02-calibration-cv.html#introduction",
    "href": "demos/02-calibration-cv.html#introduction",
    "title": "Lecture 3: Calibration and Cross-Validation",
    "section": "",
    "text": "The goal of this demo is to walk through the process of evaluating logistic regression predicted probability estimates with calibration, as well as a demonstration of implementing cross-validation. In this demo, we’ll again use a dataset of NHL shot attempts during the 2023-2024 NHL season (accessed via hockeyR) to evaluate an expected goals model for hockey. The following code chunk reads in the data and displays a subset of the data:\n\nlibrary(tidyverse)\n\nmodel_nhl_shot_data &lt;- read_csv(here::here(\"data/model_nhl_shot_data.csv\"))\nhead(model_nhl_shot_data)\n\n# A tibble: 6 × 11\n    game_id period shooting_player shooting_team goalie_name goalie_team x_fixed\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;\n1    2.02e9      1 Bryan Rust      Pittsburgh P… Petr Mrazek Chicago Bl…     -51\n2    2.02e9      1 Kevin Korchins… Chicago Blac… Tristan Ja… Pittsburgh…     -55\n3    2.02e9      1 Noel Acciari    Pittsburgh P… Petr Mrazek Chicago Bl…      75\n4    2.02e9      1 Wyatt Kaiser    Chicago Blac… Tristan Ja… Pittsburgh…     -39\n5    2.02e9      1 Alex Vlasic     Chicago Blac… Tristan Ja… Pittsburgh…     -36\n6    2.02e9      1 Marcus Petters… Pittsburgh P… Petr Mrazek Chicago Bl…      32\n# ℹ 4 more variables: y_fixed &lt;dbl&gt;, shot_distance &lt;dbl&gt;, shot_angle &lt;dbl&gt;,\n#   is_goal &lt;dbl&gt;"
  },
  {
    "objectID": "demos/02-calibration-cv.html#creating-a-calibration-plot",
    "href": "demos/02-calibration-cv.html#creating-a-calibration-plot",
    "title": "Lecture 3: Calibration and Cross-Validation",
    "section": "Creating a calibration plot",
    "text": "Creating a calibration plot\nWe’ll start with the simple logistic regression model that is just a function of shot distance (shot_distance):\n\ninit_logit &lt;- glm(is_goal ~ shot_distance, data = model_nhl_shot_data,\n                  family = \"binomial\")\n\nUsing this model, we can get the predicted probabilities for each shot using the predict() function but with type = \"response\" as an input (otherwise the logit values are returned):\n\n# Add a column to the dataset with the predicted probabilities:\nmodel_nhl_shot_data &lt;- model_nhl_shot_data |&gt;\n  mutate(pred_prob = predict(init_logit, newdata = model_nhl_shot_data,\n                             type = \"response\"))\n\nWe can now construct the calibration plot by binning the predicted probabilities followed by computing the proportion estimates and standard errors for each of the bins. There are a number of different ways to construct the bins, the following code represents just one way of doing this process using the round() function. For simplicity, we construct bins using increments of 0.05. There are several steps here so make sure you read through the code comments!\nThe following code chunk first creates a dataset with the relevant values for the calibration plot:\n\ninit_calibration_data &lt;- model_nhl_shot_data |&gt;\n  # First bin the pred probs in increments of 0.05\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) |&gt;\n  # Group by bin_pred_prob:\n  group_by(bin_pred_prob) |&gt;\n  # Calculate the calibration results:\n  summarize(n_shots = n(),\n            # Observed proportion\n            bin_actual_prob = mean(is_goal),\n            # Compute the standard error based on the proportion and # shots\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_shots),\n            .groups = \"drop\") |&gt;\n  # Cap the intervals to be within 0 and 1\n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0))\ninit_calibration_data\n\n# A tibble: 5 × 6\n  bin_pred_prob n_shots bin_actual_prob   bin_se bin_upper bin_lower\n          &lt;dbl&gt;   &lt;int&gt;           &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1          0      30281          0.0121 0.000629    0.0134    0.0109\n2          0.05   36884          0.0496 0.00113     0.0518    0.0473\n3          0.1    19214          0.0984 0.00215     0.103     0.0941\n4          0.15   11232          0.131  0.00319     0.138     0.125 \n5          0.2       23          0.652  0.0993      0.851     0.454 \n\n\nWe can see fairly large differences in the number of shots across the bins, which leads much wider intervals. Using this dataset, we can now create a calibration plot where we include the y = x line as reference:\n\ninit_calibration_data |&gt;\n  # Display pred probs along x and actual on y\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  # Display as points \n  geom_point() +\n  # Add error bars based on standard errors:\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) + \n  #geom_smooth(method = \"loess\", se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, \n              color = \"black\", linetype = \"dashed\") +\n  coord_equal() + \n  scale_x_continuous(limits = c(0,1)) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(x = \"Estimated goal probability\",\n       y = \"Observed goal frequency\") + \n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nYou may also see the number of observations in each bin mapped to the size of the points, but this is effectively already accounted for via the standard error intervals:\n\ninit_calibration_data |&gt;\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  # Display as points with number of shots mapped to size\n  geom_point(aes(size = n_shots)) +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) + \n  geom_abline(slope = 1, intercept = 0, \n              color = \"black\", linetype = \"dashed\") +\n  coord_equal() + \n  scale_x_continuous(limits = c(0,1)) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(size = \"Number of shot attempts\",\n       x = \"Estimated goal probability\",\n       y = \"Observed goal frequency\") + \n  theme_bw() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "demos/02-calibration-cv.html#cross-validation",
    "href": "demos/02-calibration-cv.html#cross-validation",
    "title": "Lecture 3: Calibration and Cross-Validation",
    "section": "Cross-validation",
    "text": "Cross-validation\nAs discussed in lecture, an expected goals model will likely be used to generate values for new shot attempts. This means we will need to take an existing model and generate predictions on new data. The alternative idea is to constantly refit the model after every single shot (or some determined batch) but that’s going to be computationally burdensome and tedious to manage. Instead, we should rely on evaluating the performance of our model based on out-of-sample performance. The most common way to do that is with \\(K\\)-fold cross-validation.\nBecause of the interesting structure of sports data, we can NOT randomly assign shots to folds. Instead, we must preserve the group structure and assign groups of observations together into the folds. For instance, in this example we will randomly assign games to folds thus ensuring that shots within the same game stay together. The following code chunk demonstrates how to first set-up a table containing only the game IDs, with 5-fold assignments and displays how many games are in each fold:\n\nset.seed(1979)\ngame_fold_table &lt;- tibble(game_id = unique(model_nhl_shot_data$game_id)) |&gt;\n  mutate(game_fold = sample(rep(1:5, length.out = n()), n()))\n\n# See how many games are in each fold:\ntable(game_fold_table$game_fold)\n\n\n  1   2   3   4   5 \n280 280 280 280 280 \n\n\nNext, we need to join the fold ids to the modeling data, so that each shot is assigned to a fold based on the game assignment:\n\nmodel_nhl_shot_data &lt;- model_nhl_shot_data |&gt; \n  left_join(game_fold_table, by = \"game_id\")\n\n# See how many shots are in each fold:\ntable(model_nhl_shot_data$game_fold)\n\n\n    1     2     3     4     5 \n19521 19754 19432 19434 19493 \n\n\nAnd finally, we can generate the holdout predictions for every shot attempt by iterating through each game fold. The following code chunk represents one way of generating the cross-validation predictions, but effectively it is just looping through the test folds, fitting the model on the training data, and storing the predictions for the test data:\n\nlogit_cv_preds &lt;- \n  map_dfr(unique(model_nhl_shot_data$game_fold), \n          function(test_fold) {\n            \n            # Separate test and training data:\n            test_data &lt;- model_nhl_shot_data |&gt;\n              filter(game_fold == test_fold)\n            train_data &lt;- model_nhl_shot_data |&gt;\n              filter(game_fold != test_fold)\n            \n            # Train model:\n            logit_model &lt;- glm(is_goal ~ shot_distance, \n                               data = train_data, family = \"binomial\")\n            \n            # Return tibble of holdout results:\n            tibble(test_pred_probs = predict(logit_model, newdata = test_data,\n                                             type = \"response\"),\n                   test_actual = test_data$is_goal,\n                   game_fold = test_fold) \n          })\n\nYou’ll notice that the logit_cv_preds dataset has the same number of rows as the original model dataset. This is because every observation receives a holdout prediction in cross-validation.\nWith the cross-validation predictions, we can now create a hold-out calibration plot using the same steps as before:\n\nlogit_cv_preds |&gt;\n  mutate(bin_pred_prob = round(test_pred_probs / 0.05) * .05) |&gt;\n  # Group by bin_pred_prob:\n  group_by(bin_pred_prob) |&gt;\n  # Calculate the calibration results:\n  summarize(n_shots = n(),\n            bin_actual_prob = mean(test_actual),\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_shots),\n            .groups = \"drop\") |&gt;\n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0)) |&gt;\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) + \n  geom_abline(slope = 1, intercept = 0, \n              color = \"black\", linetype = \"dashed\") +\n  coord_equal() + \n  scale_x_continuous(limits = c(0,1)) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(x = \"Estimated goal probability\",\n       y = \"Observed goal frequency\") + \n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nNOTE: As discussed in lecture, this is slightly different than the usual cross-validation procedure of generating an estimate for a loss (or any objective function) across \\(K\\)-folds (e.g., \\(K = 5\\) estimates of RMSE), then averaging over the \\(K\\)-fold values to generate an estimate of the test loss with standard errors. The reason for this, is because the calibration plot itself is our desired quantity (one could hypothetically compute \\(K\\) estimates for each bin instead).\nWe can use cross-validation to compare and evaluate which set of features are more appropriate. For instance, the following code check generates the cross-validation predictions when using shot distance AND angle:\n\nangle_cv_preds &lt;- \n  map_dfr(unique(model_nhl_shot_data$game_fold), \n          function(test_fold) {\n            \n            # Separate test and training data:\n            test_data &lt;- model_nhl_shot_data |&gt;\n              filter(game_fold == test_fold)\n            train_data &lt;- model_nhl_shot_data |&gt;\n              filter(game_fold != test_fold)\n            \n            # Train model:\n            logit_model &lt;- glm(is_goal ~ shot_distance + shot_angle, \n                               data = train_data, family = \"binomial\")\n            \n            # Return tibble of holdout results:\n            tibble(test_pred_probs = predict(logit_model, newdata = test_data,\n                                             type = \"response\"),\n                   test_actual = test_data$is_goal,\n                   game_fold = test_fold) \n          })\n\nWe can then stack the two datasets of predictions together, with a new column denoting what variables are in included, and then generate side-by-side calibration plots for comparison:\n\nlogit_cv_preds |&gt;\n  mutate(features = \"shot_distance\") |&gt;\n  bind_rows(mutate(angle_cv_preds, features = \"shot_distance + shot_angle\")) |&gt;\n  mutate(bin_pred_prob = round(test_pred_probs / 0.05) * .05) |&gt;\n  # Group by features AND bin_pred_prob:\n  group_by(features, bin_pred_prob) |&gt;\n  # Calculate the calibration results:\n  summarize(n_shots = n(),\n            bin_actual_prob = mean(test_actual),\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_shots),\n            .groups = \"drop\") |&gt;\n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0)) |&gt;\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) + \n  geom_abline(slope = 1, intercept = 0, \n              color = \"black\", linetype = \"dashed\") +\n  # Facet by the features:\n  facet_wrap(~features, ncol = 2) +\n  coord_equal() + \n  scale_x_continuous(limits = c(0,1)) + \n  scale_y_continuous(limits = c(0,1)) + \n  labs(x = \"Estimated goal probability\",\n       y = \"Observed goal frequency\") + \n  theme_bw() +\n  theme(legend.position = \"bottom\",\n        strip.background = element_blank())\n\n\n\n\n\n\n\n\nBetween these two model options, which do you think is better?"
  },
  {
    "objectID": "demos/02-calibration-cv.html#recap",
    "href": "demos/02-calibration-cv.html#recap",
    "title": "Lecture 3: Calibration and Cross-Validation",
    "section": "Recap",
    "text": "Recap\n\nIntroduced using calibration for evaluating logistic regression probability estimates\nWalked through steps for performing cross-validation while accounting for unique data structure"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Special Topics: Sports Analytics 36-460/660",
    "section": "",
    "text": "This is the companion website for Special Topics: Sports Analytics 36-460/660. While all of the assignments will be posted on Canvas, this website provides an alternative way to access lecture materials and additional demo files (see the see side-bar).\nLectures are on Tuesdays and Thursday from 12:30 - 1:50 PM, located in SH 105.\nOffice hours schedule:\n\n\n\n\n\n\n\n\nName\nOffice hours\nLocation\n\n\n\n\nProf Yurko\nTuesdays @ 3:30 PM, Wednesdays @ 10:45 AM\nBaker Hall 132D\n\n\nQuang Nguyen\nThursdays @ 11 AM\nZoom\n\n\nMeg Ellingwood\nMondays @ 10:30 Am\nZoom\n\n\n\nThere are no required textbooks for this course, but the following are useful free resources that I will sometimes refer to as recommended reading:\n\nBeyond Multiple Linear Regression\nBayes Rules! An Introduction to Applied Bayesian Modeling\nAnalyzing Baseball Data with R (3e)\n\nAnd the following are some interesting sports analytics websites (more are listed in course syllabus):\n\nNeil Paine’s Substack\nExploring Baseball Data with R by Jim Albert\nPlot the Ball\nThe F5",
    "crumbs": [
      "Home"
    ]
  }
]