---
title: "Lecture 11: Introduction to Bayes with a Binomial model"
format: html
---

## Introduction

The purpose of this demo is to begin our understanding the framework used to build Bayesian models. We are starting with a simple model of Caitlin Clark's field goal percentage (FG%), and will expand on this in the next demo.

## Discrete Prior Model Example

Before diving into the full Beta-Binomial model in the next demo, we'll consider an initial starting point based on a discrete prior model for Caitlin Clark's probability of making a field goal as denoted by $\pi$. We'll start with the following discrete prior model $f(\pi)$:

| $\pi$    | 0.2  | 0.4  | 0.6  | Total |
|:--------:|:----:|:----:|:----:|:-----:|
| $f(\pi)$ | 0.10 | 0.65 | 0.25 | 1     |

On Feb 15th 2024, Caitlin Clark broke the NCAA women's basketball scoring record by making 16 of 31 field goal attempts. Based on our prior, there are three different possibilities for the Binomial model of her field goal success: (1) Binomial($n$ = 31, $\pi$ = 0.2), (2) Binomial($n$ = 31, $\pi$ = 0.4), or (3) Binomial($n$ = 31, $\pi$ = 0.6). The following code chunk displays the conditional probability mass functions (__PMFs__) $f(y|\pi)$ for each of these prior possibilities:

```{r}
#| warning: false
#| message: false
#| fig-height: 5
#| fig-width: 8
library(tidyverse)
# First create a table with values from 0 to 31 to indicate the number of 
# made field goals:
field_goal_data <- tibble(made_fg = 0:31,
                          n_fg = 31)

# Now, loop over a vector of values for the prior pi:
prior_pi <- c(0.2, 0.4, 0.6)

# Create a stacked dataset containing the Binomial probability of observing
# the number of made_fg given pi and n_fg for each of the three values of pi:
field_goal_probs <- map_dfr(prior_pi,
                            function(pi) {
                              
                              field_goal_data |>
                                mutate(fg_pi = pi,
                                       binom_pmf = dbinom(made_fg, n_fg, fg_pi))
                              
                            }) |>
  # Add an indicator denoting the observed outcome:
  mutate(is_observed = ifelse(made_fg == 16, "yes", "no"))

# And now create a plot displaying the probabilities for these three prior 
# probabilities:
field_goal_probs |>
  # Make a new label using fg_pi:
  mutate(binom_label = paste0("Binomial(31, ", fg_pi, ")")) |>
  ggplot(aes(x = made_fg, y = binom_pmf, color = is_observed)) +
  geom_bar(aes(fill = is_observed), stat = "identity", width = 0.1) +
  geom_point() +
  scale_color_manual(values = c("gray", "black")) +
  scale_fill_manual(values = c("gray", "black")) +
  facet_wrap(~binom_label, ncol = 3) +
  labs(x = "y", y = expression(paste("f(y|", pi, ")"))) +
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 14))
```


While the Binomial model provides us with the probabilities of outcomes we could have observed, we can see (in black) the actual observed outcome where Caitlin Clark made $Y = 16$ FGs. From this figure, we can actually observe the likelihood function of her making $Y = 16$ FGs for each of the possible $\pi$ values by grabbing the parts corresponding to $Y = 16$:

```{r}
field_goal_probs |>
  # only use the rows for the observed outcome
  filter(is_observed == "yes") |>
  ggplot(aes(x = fg_pi, y = binom_pmf)) +
  geom_bar(stat = "identity", width = 0.001,
           color = "black", fill = "black") +
  geom_point(color = "black", size = 4) +
  scale_x_continuous(breaks = prior_pi) +
  labs(x = expression(pi), y = expression(paste("L(", pi, "|y = 16)"))) +
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 14))
```

Likewise, we can report the likelihood values describing the __compatibility of the observed data__ $Y = 16$ with the different choices of $\pi$ from the values used to make this graph:

```{r}
field_goal_probs |>
  # only use the rows for the observed outcome
  filter(is_observed == "yes")
```

In order to compute the posterior probabilities for the different choices of $\pi$, we will first compute the __normalizing constant__ which is the __total probability__ $Y = 16$ by summing across the different choices $\pi$ with their respective prior probabilities $f(\pi)$. The following code chunk sets up this table by first initializing a prior distribution table:

```{r}
prior_probs <- tibble(fg_pi = prior_pi,
                      prior = c(0.1, 0.65, 0.25))

# Make the visualization of it:
prior_probs |>
  # only use the rows for the observed outcome
  ggplot(aes(x = fg_pi, y = prior)) +
  geom_bar(stat = "identity", width = 0.001,
           color = "black", fill = "black") +
  geom_point(color = "black", size = 4) +
  scale_x_continuous(breaks = prior_pi) +
  labs(x = expression(pi), y = expression(paste("f(", pi, ")"))) +
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 14))
```

We can then join it over to the rows corresponding to the observed data to compute the normalizing constant:

```{r}
obs_fg_data <- field_goal_probs |>
  filter(is_observed == "yes") |>
  left_join(prior_probs, by = "fg_pi")

normalizing_constant <- sum(obs_fg_data$binom_pmf * obs_fg_data$prior)
```

With this, we can now compute the posterior probabilities for the three different choices for $\pi$:

```{r}
obs_fg_data <- obs_fg_data |>
  mutate(posterior = (binom_pmf * prior) / normalizing_constant)

obs_fg_data |>
  ggplot(aes(x = fg_pi, y = posterior)) +
  geom_bar(stat = "identity", width = 0.001,
           color = "black", fill = "black") +
  geom_point(color = "black", size = 4) +
  scale_x_continuous(breaks = prior_pi) +
  labs(x = expression(pi), y = expression(paste("f(", pi, "|y = 16)"))) +
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 14))
```


In comparison to the prior and likelihood, we can see that the posterior distribution for $\pi$ is very similar to the prior distribution. The choice of $\pi = 0.4$ is still the most likely, but the probability is slightly lower than what it was under the prior because of the observed data (Caitlin Clark making 16 of 31 FG attempts). 

As discussed in lecture, in practice we do not need to compute the normalizing constant. The posterior is proportional to the product of the likelihood and prior, while the constant does not depend on a choice for $\pi$ (since it integrates across all possible values). To illustrate this, the following figure displays the unnormalized version of the posterior, yielding the same display as the explicitly computed posterior:

```{r}
obs_fg_data |>
  mutate(unnorm_posterior = posterior * normalizing_constant) |>
  ggplot(aes(x = fg_pi, y = unnorm_posterior)) +
  geom_bar(stat = "identity", width = 0.001,
           color = "black", fill = "black") +
  geom_point(color = "black", size = 4) +
  scale_x_continuous(breaks = prior_pi) +
  labs(x = expression(pi), 
       y = expression(paste("Unnormalized f(", pi, "|y = 16)"))) +
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 14))
```

This property of proportionality will become useful later on when we are interested in building more complex models.
