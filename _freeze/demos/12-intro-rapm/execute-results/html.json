{
  "hash": "92b3e6c3637abdb6feda1adc76dda557",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 13: Introduction to Regularized Adjusted Plus-Minus (RAPM)\"\nformat: html\n---\n\n\n\n\n## Introduction\n\nThe purpose of this demo is to walk through the basics of building a __regularized adjusted plus-minus (RAPM) model__ to estimate the impact of basketball players when they are on the court, while adjusting for the quality of their teammates and opponents. We will use a dataset that is already in the wide design matrix form with indicator columns for every player that was observed during the regular season. You can find the script (`init_nba_rapm_data.R`) for initializing this dataset on Canvas using the [`hoopR` package](https://hoopr.sportsdataverse.org/).\n\nThe following code chunk reads in the wide data (assuming it is in the correct directory):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n# Load the data\nnba_rapm_data <- read_csv(here::here(\"data/nba_2324_season_rapm_data.csv.gz\"))\nnba_rapm_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31,885 × 579\n   game_id    stint_id n_pos home_points away_points minutes  margin `203484`\n   <chr>         <dbl> <dbl>       <dbl>       <dbl>   <dbl>   <dbl>    <dbl>\n 1 0022300061        1    27          20          16   5.88    14.8         1\n 2 0022300061        2    14           9           2   3.29    50           1\n 3 0022300061        3     8           2           2   2.16     0           0\n 4 0022300061        4    11          11           4   3.2     63.6         0\n 5 0022300061        6     2           0           2   0.400 -100           1\n 6 0022300061        7     7           3           6   1.55   -42.9         1\n 7 0022300061        8     2           2           0   0.25   100           1\n 8 0022300061        9    12           7           8   2.43    -8.33        1\n 9 0022300061       10     9           6           6   1.76     0           1\n10 0022300061       11    34          15          19   7.39   -11.8         1\n# ℹ 31,875 more rows\n# ℹ 571 more variables: `203932` <dbl>, `203999` <dbl>, `1627750` <dbl>,\n#   `1629008` <dbl>, `202704` <dbl>, `1630192` <dbl>, `1631128` <dbl>,\n#   `1631212` <dbl>, `1629618` <dbl>, `1630296` <dbl>, `1631221` <dbl>,\n#   `101108` <dbl>, `201939` <dbl>, `202691` <dbl>, `203952` <dbl>,\n#   `1626172` <dbl>, `1630228` <dbl>, `203967` <dbl>, `1627780` <dbl>,\n#   `1630541` <dbl>, `202709` <dbl>, `1628380` <dbl>, `1629640` <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\nIn this dataset, we have 31885 unique shifts/stints with 572 players represented by the indicator variables (+1 if on court for home team, -1 if on court for away team, and 0 if not on court). Additional context is captured by the following variables:\n\n| Variable | Description |\n|----|-------------|\n| `game_id` |\tUnique game ID |\n| `stint_id` |\tUnique identifier within a game for a stint for particular combination of home and away lineup (in appearance of order, where 1 is the first stint in the game) |\n| `n_pos` |\tNumber of possessions (combined for both home and away) during the observed stint |\n| `home_points` |\tNumber of points scored by the home team during the stint |\n| `away_points` |\tNumber of points scored by the away team during the stint |\n| `minutes` |\tLength of the stint in terms of minutes played |\n| `margin` | Common response for RAPM models defined as: (`home_points` - `away_points`) / `n_pos` * 100 |\n\n## Adjusted Plus-Minus (APM)\n\nWe'll first consider the classic [Rosenbaum (2004)](https://www.82games.com/comm30.htm) __adjusted plus-minus (APM)__ model, which is **weighted** least-squares where:\n\n+ Response variable is the score differential with respect to home team, i.e., `home_points - away_points`\n\n+ Weights are the number of posessions during the shift/stint, i.e., `n_pos`\n\nThe following code chunk fits this initial model (note this will take a bit to run::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First compute the score differential response:\nnba_rapm_data <- nba_rapm_data |>\n  mutate(score_diff = home_points - away_points)\n\n# Now for ease, create a dataset that only has the response and player columns:\nnba_apm_model_data <- nba_rapm_data |>\n  dplyr::select(-c(game_id, stint_id, n_pos, home_points, away_points, minutes,\n                   margin))\n\n# Fit the model (notice we do not include an intercept term)\nrosenbaum_model <- lm(score_diff ~ 0 + ., data = nba_apm_model_data,\n                      weights = nba_rapm_data$n_pos)\n```\n:::\n\n\n\n\n\nWe're not going to view the summary of this model since it is a bit of a mess. Instead, we'll take advantage of the [`broom` package](https://broom.tidymodels.org/index.html) to view the coefficients:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nrosenbaum_coef <- tidy(rosenbaum_model)\nrosenbaum_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 572 × 5\n   term      estimate std.error statistic p.value\n   <chr>        <dbl>     <dbl>     <dbl>   <dbl>\n 1 `203484`     1.58       1.74     0.911   0.362\n 2 `203932`     1.26       1.74     0.725   0.469\n 3 `203999`     1.92       1.75     1.10    0.273\n 4 `1627750`    1.58       1.74     0.909   0.363\n 5 `1629008`    1.27       1.74     0.732   0.464\n 6 `202704`     1.08       1.74     0.622   0.534\n 7 `1630192`    0.392      1.77     0.222   0.825\n 8 `1631128`    1.18       1.74     0.682   0.495\n 9 `1631212`    0.851      1.74     0.489   0.625\n10 `1629618`    1.00       1.86     0.537   0.591\n# ℹ 562 more rows\n```\n\n\n:::\n:::\n\n\n\n\nObviously, in this current form we have no idea, we have no idea which player is which. Fortunately for you, there is a table on Canvas with the names of the players to join using these IDs in the `term` column: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnba_player_table <- read_csv(here::here(\"data/nba_2324_player_table.csv\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 572 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): player_name\ndbl (1): player_id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nnba_player_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 572 × 2\n   player_id player_name             \n       <dbl> <chr>                   \n 1   1630173 Precious Achiuwa        \n 2   1628389 Bam Adebayo             \n 3   1630534 Ochai Agbaji            \n 4   1630583 Santi Aldama            \n 5   1629638 Nickeil Alexander-Walker\n 6   1628960 Grayson Allen           \n 7   1628386 Jarrett Allen           \n 8   1641851 Timmy Allen             \n 9   1630631 Jose Alvarado           \n10    203937 Kyle Anderson           \n# ℹ 562 more rows\n```\n\n\n:::\n:::\n\n\n\n\nYou'll notice that this matches the number of rows as the `rosenbaum_coef` table. But we first need to modify the `term` column by removing the back-tick symbols and then converting the IDs to numeric values before joining:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrosenbaum_coef <- rosenbaum_coef |>\n  # First convert the term column to numeric:\n  mutate(term = as.numeric(str_remove_all(term, \"`\"))) |>\n  # Now join the player names:\n  left_join(nba_player_table, by = c(\"term\" = \"player_id\"))\nrosenbaum_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 572 × 6\n      term estimate std.error statistic p.value player_name             \n     <dbl>    <dbl>     <dbl>     <dbl>   <dbl> <chr>                   \n 1  203484    1.58       1.74     0.911   0.362 Kentavious Caldwell-Pope\n 2  203932    1.26       1.74     0.725   0.469 Aaron Gordon            \n 3  203999    1.92       1.75     1.10    0.273 Nikola Jokić            \n 4 1627750    1.58       1.74     0.909   0.363 Jamal Murray            \n 5 1629008    1.27       1.74     0.732   0.464 Michael Porter Jr.      \n 6  202704    1.08       1.74     0.622   0.534 Reggie Jackson          \n 7 1630192    0.392      1.77     0.222   0.825 Zeke Nnaji              \n 8 1631128    1.18       1.74     0.682   0.495 Christian Braun         \n 9 1631212    0.851      1.74     0.489   0.625 Peyton Watson           \n10 1629618    1.00       1.86     0.537   0.591 Jalen Pickett           \n# ℹ 562 more rows\n```\n\n\n:::\n:::\n\n\n\n\nWho are the top players based on this approach?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrosenbaum_coef |>\n  slice_max(estimate, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 6\n      term estimate std.error statistic p.value player_name     \n     <dbl>    <dbl>     <dbl>     <dbl>   <dbl> <chr>           \n 1 1641806     6.07      4.31     1.41   0.159  Markquis Nowell \n 2 1630259     5.18      2.27     2.28   0.0226 Jordan Ford     \n 3 1631157     4.04      1.88     2.15   0.0313 Ryan Rollins    \n 4 1630608     3.65      4.07     0.896  0.370  Malcolm Cazalon \n 5 1631321     3.13      1.83     1.71   0.0868 Sidy Cissoko    \n 6 1641766     3.09      1.96     1.57   0.116  Adama Sanogo    \n 7 1641774     3.08      1.82     1.69   0.0912 Tristan Vukcevic\n 8 1631220     3.07      1.99     1.55   0.122  Dereon Seabron  \n 9 1630207     3.07      2.02     1.52   0.129  Nate Hinton     \n10 1631217     3.01      1.92     1.57   0.117  Moussa Diabaté  \n```\n\n\n:::\n:::\n\n\n\n\nAnd the worst players?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrosenbaum_coef |>\n  slice_min(estimate, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 6\n      term estimate std.error statistic p.value player_name         \n     <dbl>    <dbl>     <dbl>     <dbl>   <dbl> <chr>               \n 1 1631199    -6.80      3.35    -2.03   0.0425 Ron Harper Jr.      \n 2 1630606    -4.20      7.65    -0.549  0.583  Javonte Smart       \n 3 1631214    -2.51      2.38    -1.05   0.292  Alondes Williams    \n 4 1631250    -2.48      2.27    -1.09   0.276  Pete Nance          \n 5 1627885    -2.37      3.10    -0.764  0.445  Shaquille Harrison  \n 6 1641847    -2.31      2.65    -0.871  0.384  Andrew Funk         \n 7 1631298    -1.78      1.98    -0.895  0.371  Jack White          \n 8 1631112    -1.68      2.04    -0.824  0.410  Kendall Brown       \n 9  202738    -1.65      2.33    -0.708  0.479  Isaiah Thomas       \n10 1631257    -1.63      2.12    -0.767  0.443  Jermaine Samuels Jr.\n```\n\n\n:::\n:::\n\n\n\n\nThese look like pretty extreme values, with the most extreme values observed by players that have limited playing time (upon searching their stats online). Before we think about how to address these issues, let's look at what happens if we make a slight tweak to our model by using the `margin` variable as the response instead:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now for ease, create a dataset that only has the response and player columns:\nnba_margin_apm_model_data <- nba_rapm_data |>\n  dplyr::select(-c(game_id, stint_id, n_pos, home_points, away_points, minutes,\n                   score_diff))\n\n# Fit the model (notice we do not include an intercept term)\nrosenbaum_margin_model <- lm(margin ~ 0 + ., data = nba_margin_apm_model_data)\n\n# Get the coefficients and join player names:\nrosenbaum_margin_coef <- tidy(rosenbaum_margin_model) |>\n  # First convert the term column to numeric:\n  mutate(term = as.numeric(str_remove_all(term, \"`\"))) |>\n  # Now join the player names:\n  left_join(nba_player_table, by = c(\"term\" = \"player_id\"))\n\n# View top 10:\nrosenbaum_margin_coef |>\n  slice_max(estimate, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 6\n      term estimate std.error statistic p.value player_name    \n     <dbl>    <dbl>     <dbl>     <dbl>   <dbl> <chr>          \n 1 1628977     52.7      43.1     1.22    0.221 Hamidou Diallo \n 2 1630608     34.7      56.3     0.617   0.537 Malcolm Cazalon\n 3 1631131     29.0      28.5     1.02    0.308 Oscar Tshiebwe \n 4 1631220     28.7      26.6     1.08    0.280 Dereon Seabron \n 5 1631157     28.4      26.2     1.08    0.278 Ryan Rollins   \n 6 1629717     28.1      25.5     1.10    0.272 Armoni Brooks  \n 7 1630227     27.2      28.7     0.949   0.343 Daishen Nix    \n 8 1629010     24.2      26.7     0.907   0.365 Jerome Robinson\n 9 1630207     23.9      27.2     0.881   0.378 Nate Hinton    \n10 1631205     22.8      25.5     0.891   0.373 Buddy Boeheim  \n```\n\n\n:::\n:::\n\n\n\n\nNotice the difference in magnitude now for the coefficient estimates compared to the score differential model. This is because the response is on the scale of points per 100 possessions.\n\nBefore we dive into fixing the issues covered in lecture, let's take a look at the distribution of the coefficients for the players:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrosenbaum_margin_coef |>\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  labs(x = \"APM estimate\", y = \"Count\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](12-intro-rapm_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n__What do you notice about this distribution?__\n\n## Regularized Adjusted Plus-Minus (RAPM)\n\nIn order to address the common issues facing APM models, we can fit a RAPM model using ridge regression. The go-to approach for fitting ridge (as well as lasso and elastic-net models) is with the [`glmnet` package](https://glmnet.stanford.edu/articles/glmnet.html). The following code chunk demonstrates how we can easily fit a ridge regression model with the RAPM design matrix. In order to tune the penalty $\\lambda$, we will use the built-in cross-validation code with the `cv.glmnet()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we need to grab the X and convert to a matrix:\nplayer_matrix <- nba_margin_apm_model_data |>\n  dplyr::select(-margin) |>\n  as.matrix()\n\n# Next we load the package (assuming it's installed)\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Matrix' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit ridge (alpha = 0) w/ 10 fold CV, no intercept and no standardization\nfit_ridge_cv <- cv.glmnet(player_matrix, nba_margin_apm_model_data$margin, \n                          alpha = 0, intercept = FALSE, standardize = FALSE)\n\n# View the penalty selection:\nplot(fit_ridge_cv)\n```\n\n::: {.cell-output-display}\n![](12-intro-rapm_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can easily plot the path of the ridge regression shrinkage, to see how the coefficients are pulled towards 0 as the penalty increases. The following code chunk shows this full path:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit_ridge_cv$glmnet.fit, xvar = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](12-intro-rapm_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nUsing the `broom` package again, we can again make a tidy table of the coefficients for each player:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_ridge_coef <- tidy(fit_ridge_cv$glmnet.fit)\ntidy_ridge_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 57,200 × 5\n   term    step estimate lambda dev.ratio\n   <chr>  <dbl>    <dbl>  <dbl>     <dbl>\n 1 203484     1 2.39e-37   351.  2.01e-39\n 2 203484     2 5.15e- 2   320.  4.25e- 4\n 3 203484     3 5.56e- 2   291.  4.62e- 4\n 4 203484     4 6.09e- 2   265.  5.05e- 4\n 5 203484     5 6.67e- 2   242.  5.52e- 4\n 6 203484     6 7.30e- 2   220.  6.03e- 4\n 7 203484     7 7.98e- 2   201.  6.59e- 4\n 8 203484     8 8.73e- 2   183.  7.20e- 4\n 9 203484     9 9.55e- 2   167.  7.85e- 4\n10 203484    10 1.04e- 1   152.  8.57e- 4\n# ℹ 57,190 more rows\n```\n\n\n:::\n:::\n\n\n\n\nIf you look closely, this returns 100 rows for each player in the data - because it is returning the coefficient for each player at each value of the `lambda` penalty. We can filter to the values for the optimal choice of `lambda` based on the cross-validation results, and then join our player names as before:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter to the min lambda CV and join the player names:\nrapm_ridge_coef <- tidy_ridge_coef |>\n  filter(lambda == fit_ridge_cv$lambda.min) |>\n  # Convert term to numeric:\n  mutate(term = as.numeric(term)) |>\n  # Now join the player names:\n  left_join(nba_player_table, by = c(\"term\" = \"player_id\"))\n\n# View top 10:\nrapm_ridge_coef |>\n  slice_max(estimate, n = 10) |>\n  dplyr::select(term, player_name, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n      term player_name        estimate\n     <dbl> <chr>                 <dbl>\n 1  203999 Nikola Jokić           4.68\n 2 1629029 Luka Dončić            4.34\n 3 1628973 Jalen Brunson          4.11\n 4  203954 Joel Embiid            3.63\n 5 1626164 Devin Booker           3.42\n 6 1626157 Karl-Anthony Towns     3.37\n 7 1628374 Lauri Markkanen        3.21\n 8 1630198 Isaiah Joe             2.95\n 9 1630581 Josh Giddey            2.89\n10 1628401 Derrick White          2.85\n```\n\n\n:::\n:::\n\n\n\n\nConsidering Jokić won the MVP last season, this list definitely passes the eye test (it's honestly amazing how well this works for basketball data). For context, let's view the bottom 10:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrapm_ridge_coef |>\n  slice_min(estimate, n = 10) |>\n  dplyr::select(term, player_name, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 3\n      term player_name    estimate\n     <dbl> <chr>             <dbl>\n 1 1630171 Isaac Okoro       -2.94\n 2 1629651 Nic Claxton       -2.90\n 3 1630164 James Wiseman     -2.86\n 4 1630537 Chris Duarte      -2.77\n 5 1626224 Cedi Osman        -2.70\n 6  202397 Ish Smith         -2.67\n 7 1629018 Gary Trent Jr.    -2.46\n 8 1628381 John Collins      -2.38\n 9 1631367 Jacob Gilyard     -2.37\n10 1630201 Malachi Flynn     -2.35\n```\n\n\n:::\n:::\n\n\n\n\nAnd finally, let's view the RAPM coefficient distribution (for comparison against the APM coefficients):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrapm_ridge_coef |>\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  labs(x = \"RAPM estimate\", y = \"Count\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](12-intro-rapm_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "12-intro-rapm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}