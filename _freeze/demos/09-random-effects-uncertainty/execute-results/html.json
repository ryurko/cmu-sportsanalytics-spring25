{
  "hash": "22607cd69a1edb5bfa7354589fd69505",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 10: Uncertainty about random effects\"\nformat: html\n---\n\n\n\n\n## Introduction\n\nThe purpose of this demo is to walk through how to access and generate different estimates of uncertainty about the random effects from multilevel models. This will be our last demo with the NFL passing data, as we'll consider the random effects in the context of modeling completion probability. As a reminder, you can find the dataset and code to create the data (`init_nfl_passing_data.R`) on Canvas in the demos/week3 folder.  \n\nThe following code chunk reads in the relevant dataset (assuming it is in the correct directory) of passing plays:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nnfl_passing_data <- read_csv(here::here(\"data/nfl_passing_data.csv\"))\nnfl_passing_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 35,987 × 17\n   game_id         play_id drive posteam defteam posteam_type passer_name_id    \n   <chr>             <dbl> <dbl> <chr>   <chr>   <chr>        <chr>             \n 1 2023_01_ARI_WAS      77     1 WAS     ARI     home         S.Howell_00-00370…\n 2 2023_01_ARI_WAS     124     1 WAS     ARI     home         S.Howell_00-00370…\n 3 2023_01_ARI_WAS     147     1 WAS     ARI     home         S.Howell_00-00370…\n 4 2023_01_ARI_WAS     172     1 WAS     ARI     home         S.Howell_00-00370…\n 5 2023_01_ARI_WAS     197     1 WAS     ARI     home         S.Howell_00-00370…\n 6 2023_01_ARI_WAS     220     1 WAS     ARI     home         S.Howell_00-00370…\n 7 2023_01_ARI_WAS     332     2 ARI     WAS     away         J.Dobbs_00-0033949\n 8 2023_01_ARI_WAS     357     2 ARI     WAS     away         J.Dobbs_00-0033949\n 9 2023_01_ARI_WAS     380     2 ARI     WAS     away         J.Dobbs_00-0033949\n10 2023_01_ARI_WAS     526     3 WAS     ARI     home         S.Howell_00-00370…\n# ℹ 35,977 more rows\n# ℹ 10 more variables: receiver_name_id <chr>, complete_pass <dbl>,\n#   pass_location <chr>, air_yards <dbl>, qb_hit <dbl>, epa <dbl>,\n#   yardline_100 <dbl>, down <dbl>, ydstogo <dbl>, is_home <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nIf you notice, there is an error in this dataset where there are receivers marked as `NA_NA`. We need to remove these receivers before continuing on:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnfl_passing_data <- nfl_passing_data |>\n  filter(receiver_name_id != \"NA_NA\")\n```\n:::\n\n\n\n\n\n## Random effects for passers, receivers, and defenses\n\nFor this demo, we'll consider the crossed effects model from earlier in the week which involved random intercepts for passers, receivers, and defenses:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'lme4' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Matrix' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n```{.r .cell-code}\nfull_pass_glmm <- glmer(complete_pass ~ air_yards + (1 | passer_name_id) +\n                          (1 | receiver_name_id) + (1 | defteam),\n                        family = binomial, data = nfl_passing_data)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(full_pass_glmm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \ncomplete_pass ~ air_yards + (1 | passer_name_id) + (1 | receiver_name_id) +  \n    (1 | defteam)\n   Data: nfl_passing_data\n\n     AIC      BIC   logLik deviance df.resid \n 40076.5  40118.7 -20033.2  40066.5    34491 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2047 -0.9825  0.5218  0.6349  3.9676 \n\nRandom effects:\n Groups           Name        Variance Std.Dev.\n receiver_name_id (Intercept) 0.01917  0.13846 \n passer_name_id   (Intercept) 0.02349  0.15325 \n defteam          (Intercept) 0.00393  0.06269 \nNumber of obs: 34496, groups:  \nreceiver_name_id, 634; passer_name_id, 136; defteam, 32\n\nFixed effects:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.250184   0.029507   42.37   <2e-16 ***\nair_yards   -0.065060   0.001288  -50.51   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nair_yards -0.360\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nModel is nearly unidentifiable: very large eigenvalue\n - Rescale variables?\n```\n\n\n:::\n:::\n\n\n\n\nWe can access the full table of random effects using the [`broom.mixed` package](https://github.com/bbolker/broom.mixed) again. The following code chunk extracts the three groups of random effects, as designated by the `group` column:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom.mixed)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'broom.mixed' was built under R version 4.2.3\n```\n\n\n:::\n\n```{.r .cell-code}\ngroup_raneff <- tidy(full_pass_glmm, effects = \"ran_vals\")\n# View the dataset\ngroup_raneff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 802 × 6\n   effect   group            level                 term       estimate std.error\n   <chr>    <chr>            <chr>                 <chr>         <dbl>     <dbl>\n 1 ran_vals receiver_name_id A.Abdullah_00-0032104 (Intercep…  8.01e-2     0.125\n 2 ran_vals receiver_name_id A.Armah_00-0033296    (Intercep…  1.07e-2     0.138\n 3 ran_vals receiver_name_id A.Bachman_00-0035602  (Intercep…  1.87e-2     0.138\n 4 ran_vals receiver_name_id A.Barner_00-0039793   (Intercep…  2.23e-2     0.131\n 5 ran_vals receiver_name_id A.Beck_00-0034959     (Intercep…  7.95e-4     0.135\n 6 ran_vals receiver_name_id A.Brown_00-0035676    (Intercep…  7.43e-2     0.105\n 7 ran_vals receiver_name_id A.Cooper_00-0031544   (Intercep… -5.12e-2     0.103\n 8 ran_vals receiver_name_id A.Dillon_00-0036265   (Intercep…  5.47e-3     0.133\n 9 ran_vals receiver_name_id A.Dulin_00-0035021    (Intercep… -3.37e-2     0.136\n10 ran_vals receiver_name_id A.Ekeler_00-0033699   (Intercep… -7.71e-2     0.119\n# ℹ 792 more rows\n```\n\n\n:::\n:::\n\n\n\n\nThe first type of uncertainty we'll consider is based on the theoretical model from lecture, which are effectively conditional standard deviation estimates (under the assumptions that the fixed effects and random effect variances are correct). In the `group_raneff` table above, this corresponds to the `std.error` column. We can quickly use this dataset to make a panel of plots for each random effect, displaying the estimates +/- one standard error to quickly see how they vary across the groups:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroup_raneff |>\n  ggplot(aes(x = level, y = estimate)) +\n  geom_point(alpha = 0.5) +\n  geom_errorbar(aes(ymin = estimate - std.error, ymax = estimate + std.error),\n                alpha = 0.5) +\n  # Add line at 0 on top:\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(~group, ncol = 3, scales = \"free_x\") +\n  theme_bw() +\n  # Remove the x axis labels and ticks for each group level \n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](09-random-effects-uncertainty_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nFrom this, you quickly observe the different levels of variation across the three groups: `defteam`, `passer_name_id`, and `receiver_name_id`, with `receiver_name_id` displaying the greatest variation. This is consistent with the variance estimate in the summary output of the model. If we wanted to rely on this model and its assumptions, we could use the standard errors to get a sense of which levels of the random effects are noticeably different from 0 (either above or below).\n\n## Bootstrapping random effects\n\nWe're now going to proceed to quantify the uncertainty of the random effect estimates using __bootstrapping (i.e., resampling)__. This procedure requires careful recognition of the sources of variability in player and team performances. We can NOT simply resample pass attempts to construct a new dataset. If we think about the course of a football season, __the schedule is fixed__. In other words, any resampling procedure we consider must preserve the schedule that each player and team plays in each resampled dataset - with the same number of games and opponents. If we naively resampled at the observation level without thinking about this structure, we would observe __impossible__ datasets that do not reflect the target source of variability that we're interested in. \n\nInstead, we need to resample the plays that take place within a particular game/match-up between two teams. You can think of this as attempting to replay a game over again and observing different performances and outcomes. But given the nature of football plays, we should not just resample plays within games either! Instead, we want our simulated datasets to capture the wide range of context that different plays can take place in. The simplest way to capture this structure, as well as any dependency between plays, is by resampling collections of plays at the drive level. A single iteration of resampling the dataset involves: going through each game and resampling the drives for each team (to ensure that each team has the same number of drives as observed). There are other ways to resample, this is just one approach that tries to account for the game structure.\n\nIn order to implement this, we first need to set-up a dataset of the games and drives for each team. One efficient way to do this, is to construct a __nested__ dataset at the game-team-drive level. A nested dataset will contain \"one row\" for game-team-drive combination, but within that \"row\" will be all plays corresponding to that game-team-drive. The following code chunk uses the `nest()` function to collapse the `nfl_passing_data` into a dataset at this combination level. (NOTE: this may not account for all possible drives in case there are drives without pass attempts - however I'm just being lazy in the set-up of this to only rely on the `nfl_passing_data` in this demo.) This will make the process of resampling plays much easier:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngame_team_drives <- nfl_passing_data |>\n  # First group by the three variables of interest:\n  group_by(game_id, posteam, drive) |>\n  # Now nest the data\n  nest() |>\n  ungroup()\ngame_team_drives\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,585 × 4\n   game_id         drive posteam data             \n   <chr>           <dbl> <chr>   <list>           \n 1 2023_01_ARI_WAS     1 WAS     <tibble [6 × 14]>\n 2 2023_01_ARI_WAS     2 ARI     <tibble [3 × 14]>\n 3 2023_01_ARI_WAS     3 WAS     <tibble [3 × 14]>\n 4 2023_01_ARI_WAS     4 ARI     <tibble [4 × 14]>\n 5 2023_01_ARI_WAS     5 WAS     <tibble [1 × 14]>\n 6 2023_01_ARI_WAS     6 ARI     <tibble [2 × 14]>\n 7 2023_01_ARI_WAS     7 WAS     <tibble [3 × 14]>\n 8 2023_01_ARI_WAS     9 WAS     <tibble [2 × 14]>\n 9 2023_01_ARI_WAS    10 ARI     <tibble [3 × 14]>\n10 2023_01_ARI_WAS    11 WAS     <tibble [3 × 14]>\n# ℹ 10,575 more rows\n```\n\n\n:::\n:::\n\n\n\n\nUsing this dataset, we are now ready to bootstrap team drives within games to generate distributions of the passer, receiver, and defense effects. The following code chunk performs this process with $B = 100$ bootstrapped iterations. __NOTE: this will take some time to run and I hid the repeated observed warning messages that pop-up when fitting this model!__\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of bootstrap iterations:\nN_BOOT <- 100\n\nbootstrap_random_effects <-\n  map_dfr(1:N_BOOT,\n          function(boot_i) {\n            \n            # First resample drives by teams within games\n            boot_game_team_drives <- game_team_drives |>\n              group_by(game_id, posteam) |>\n              # Sample with replacement\n              sample_n(n(), replace = TRUE) |>\n              # ungroup and unnest\n              ungroup() |>\n              unnest(cols = data)\n            \n            # Now fit the model\n            boot_glmm <- glmer(complete_pass ~ air_yards + (1 | passer_name_id) +\n                          (1 | receiver_name_id) + (1 | defteam), \n                               data = boot_game_team_drives, \n                          family = binomial)\n            \n            # Store the random effects:\n            boot_raneff <- tidy(boot_glmm, effects = \"ran_vals\")\n\n            # Add a column for the bootstrap iteration and return\n            boot_raneff |>\n              mutate(boot = boot_i)\n          })\n```\n:::\n\n\n\n\nWe now have a dataset with 100 estimates for each of the different random effects (potentially fewer than 100 for some passers and receivers that might have been dropped in certain iterations due to their drives not being selected). We can proceed to inspect the distribution of the random effects beyond just computing standard errors. The following code chunk first sets up a dataset with the medians of the bootstrapped values which we'll then use to select the top passers and receivers, as well as provide the ordering for the factors in relevant graphics displaying the distributions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_summary <- bootstrap_random_effects |>\n  group_by(group, level) |>\n  summarize(med_intercept = median(estimate, na.rm = TRUE),\n            n_sims = n(),\n            .groups = \"drop\")\n\n# Defenses in order:\ndef_table <- bootstrap_summary |>\n  filter(group == \"defteam\") |>\n  mutate(level = fct_reorder(level, med_intercept))\n\n# Table of the top 10 passers\ntop_passers <- bootstrap_summary |>\n  filter(group == \"passer_name_id\") |>\n  slice_max(order_by = med_intercept, n = 10) |>\n  mutate(level = fct_reorder(level, med_intercept))\n\n# Top 10 receivers\ntop_receivers <- bootstrap_summary |>\n  filter(group == \"receiver_name_id\") |>\n  slice_max(order_by = med_intercept, n = 10) |>\n  mutate(level = fct_reorder(level, med_intercept))\n```\n:::\n\n\n\n\nNext, we'll create displays of the top passers, receivers, and all of the defenses using ridge plots (which can be constructed with [`ggridges`](https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html) - this is a great way to visualize distributions across many levels of a categorical variable). First, we create the plot for the top passers:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggridges)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggridges' was built under R version 4.2.3\n```\n\n\n:::\n\n```{.r .cell-code}\nbootstrap_random_effects |>\n  filter(group == \"passer_name_id\",\n         level %in% top_passers$level) |>\n  # Reorder them by the median order\n  mutate(level = factor(level, levels = levels(top_passers$level))) |>\n  ggplot(aes(x = estimate, y = level)) +\n  # Display the ridges with the median line\n  geom_density_ridges(quantile_lines = TRUE,\n                      quantiles = 0.5,\n                      rel_min_height = 0.01) +\n  labs(x = \"Passer random effect\", y = \"Passer name\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.0261\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-random-effects-uncertainty_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\nAnd then for top receivers:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_random_effects |>\n  filter(group == \"receiver_name_id\",\n         level %in% top_receivers$level) |>\n  # Reorder them by the median order\n  mutate(level = factor(level, levels = levels(top_receivers$level))) |>\n  ggplot(aes(x = estimate, y = level)) +\n  # Display the ridges with the median line\n  geom_density_ridges(quantile_lines = TRUE,\n                      quantiles = 0.5,\n                      rel_min_height = 0.01) +\n  labs(x = \"Receiver random effect\", y = \"Receiver name\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.041\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-random-effects-uncertainty_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nAnd finally for the defenses:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstrap_random_effects |>\n  filter(group == \"defteam\") |>\n  # Reorder them by the median order\n  mutate(level = factor(level, levels = levels(def_table$level))) |>\n  ggplot(aes(x = estimate, y = level)) +\n  # Display the ridges with the median line\n  geom_density_ridges(quantile_lines = TRUE,\n                      quantiles = 0.5,\n                      rel_min_height = 0.01) +\n  labs(x = \"Defense random effect\", y = \"Team\") +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPicking joint bandwidth of 0.0139\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-random-effects-uncertainty_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}