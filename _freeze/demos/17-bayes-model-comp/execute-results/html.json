{
  "hash": "8edcb5648039426110898b668605132a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 19: Bayesian model comparison\"\nformat: html\n---\n\n\n\n\n## Introduction\n\nThe purpose of this demo is introduce approaches for comparing Bayesian regression models. Similar to the last demo, we will use the soccer match data from HW1. As a reminder, this dataset in the homeworks/hw1 folder on Canvas (`soccer_match_goals.csv`) and contains the goals scored across men's soccer matches in the five biggest European leagues during the 2023-2024 season. This dataset contains the following columns:\n\n1. `goals`: number of goals scored by `team` in a match\n\n2. `xG`: accumulated number of expected goals by `team` in a match\n\n3. `off_team`: the team name affiliated with the __offense scoring__ the number of `goals` and total `xG`\n\n4. `def_team`: the team name affiliated with the __defense allowing__ the number of `goals` and total `xG`\n\n5. `league`: string denoting the country: ENG (Premier League), ESP (La Liga), FRA (Ligue 1), GER (Fußball-Bundesliga), and ITA (Serie A)\n\n6. `match_id`: unique identifier for each match, such that each match has two rows in the dataset, one for the home team and one for the away team\n\n7. `is_home`: binary indicator denoting whether or not the team was the home team in the match.\n\nNote that each row in this dataset corresponds to information about a team's offensive performance for a single match. This means a team is repeatedly observed for each match they played during the 2023-2024 season. I constructed this dataset using the [worldfootballR](https://jaseziv.github.io/worldfootballR/) package, and the script `init_soccer_match_goals.R` on Canvas is the code I used to make the dataset.\n\nThe code chunk below loads in the data, and then for ease we'll only consider matches in the Premier League (i.e., `league == ENG`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmatch_goals_scored <- read_csv(here::here(\"data/soccer_match_goals.csv\"))\n\n# Create the ENG only table:\neng_match_goals_scored <- match_goals_scored |>\n  filter(league == \"ENG\")\neng_match_goals_scored\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 760 × 7\n   goals    xG off_team       def_team        league match_id is_home\n   <dbl> <dbl> <chr>          <chr>           <chr>     <dbl>   <dbl>\n 1     0   0.3 Burnley        Manchester City ENG           1       1\n 2     2   0.8 Arsenal        Nott'ham Forest ENG           2       1\n 3     0   2.7 Everton        Fulham          ENG           3       1\n 4     0   0.5 Sheffield Utd  Crystal Palace  ENG           4       1\n 5     4   4   Brighton       Luton Town      ENG           5       1\n 6     1   1.3 Bournemouth    West Ham        ENG           6       1\n 7     5   3.3 Newcastle Utd  Aston Villa     ENG           7       1\n 8     2   2.2 Brentford      Tottenham       ENG           8       1\n 9     1   1.4 Chelsea        Liverpool       ENG           9       1\n10     1   2.2 Manchester Utd Wolves          ENG          10       1\n# ℹ 750 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n## Fitting models in `brms`\n\nAs in the previous demo, we'll continue to model the same goals scored response variable using a Bayesian hierarchical Poisson regression model. But rather than using  the [`rstanarm` package](https://mc-stan.org/rstanarm/), we'll now switch to the popular [`brms` package](https://paulbuerkner.com/brms/) that was demonstrated to you by [Quang Nguyen in his guest lecture](https://ryurko.github.io/cmu-sportsanalytics-spring25/lectures/17-multilevel-heterogeneity.html#/what-is-brms). The `brms` package is effectively the same as `rstanarm`, but provides greater flexibility in the types of models it can fit ([see here for a nice summary by one of the Stan developers](https://discourse.mc-stan.org/t/rstan-cmdstanr-brms-and-rstanarm/18865/2)) - along with more helper functions that were developed to work with it. \n\nThe following code chunk shows you how to fit the same Poisson model in the previous demo, along with viewing the summary, and extracting the posterior samples for analysis:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'brms' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Rcpp' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'brms'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    ar\n```\n\n\n:::\n\n```{.r .cell-code}\nteam_goals_model <- brm(goals ~ is_home + (1 | off_team) + (1 | def_team),\n                        family = poisson,\n                        data = eng_match_goals_scored, seed = 2013)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCompiling Stan program...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000101 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.949 seconds (Warm-up)\nChain 1:                0.646 seconds (Sampling)\nChain 1:                1.595 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.969 seconds (Warm-up)\nChain 2:                0.654 seconds (Sampling)\nChain 2:                1.623 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.943 seconds (Warm-up)\nChain 3:                0.65 seconds (Sampling)\nChain 3:                1.593 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4.7e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.924 seconds (Warm-up)\nChain 4:                0.703 seconds (Sampling)\nChain 4:                1.627 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(team_goals_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: poisson \n  Links: mu = log \nFormula: goals ~ is_home + (1 | off_team) + (1 | def_team) \n   Data: eng_match_goals_scored (Number of observations: 760) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~def_team (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.25      0.06     0.16     0.38 1.00     1185     2303\n\n~off_team (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.27      0.06     0.17     0.41 1.00     1399     1914\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.33      0.09     0.15     0.51 1.00     1274     1845\nis_home       0.20      0.05     0.09     0.30 1.00     6173     3305\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(team_goals_model, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: poisson \n  Links: mu = log \nFormula: goals ~ is_home + (1 | off_team) + (1 | def_team) \n   Data: eng_match_goals_scored (Number of observations: 760) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~def_team (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI   Rhat Bulk_ESS Tail_ESS\nsd(Intercept)   0.2493    0.0564   0.1583   0.3765 1.0005     1185     2303\n\n~off_team (Number of levels: 20) \n              Estimate Est.Error l-95% CI u-95% CI   Rhat Bulk_ESS Tail_ESS\nsd(Intercept)   0.2659    0.0611   0.1680   0.4095 1.0039     1399     1914\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI   Rhat Bulk_ESS Tail_ESS\nIntercept   0.3305    0.0942   0.1475   0.5140 1.0030     1274     1845\nis_home     0.1967    0.0537   0.0923   0.3040 1.0006     6173     3305\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n\n```{.r .cell-code}\nposterior_sample <- as_tibble(team_goals_model)\nposterior_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 47\n   b_Intercept b_is_home sd_def_team__Intercept sd_off_team__Intercept Intercept\n         <dbl>     <dbl>                  <dbl>                  <dbl>     <dbl>\n 1       0.244    0.207                   0.311                  0.305     0.347\n 2       0.330    0.217                   0.251                  0.322     0.438\n 3       0.250    0.229                   0.222                  0.284     0.365\n 4       0.342    0.144                   0.248                  0.267     0.414\n 5       0.232    0.244                   0.252                  0.369     0.355\n 6       0.409    0.116                   0.270                  0.305     0.467\n 7       0.151    0.348                   0.224                  0.303     0.325\n 8       0.298    0.104                   0.312                  0.277     0.350\n 9       0.298    0.0907                  0.263                  0.268     0.344\n10       0.199    0.260                   0.274                  0.295     0.329\n# ℹ 3,990 more rows\n# ℹ 42 more variables: `r_def_team[Arsenal,Intercept]` <dbl>,\n#   `r_def_team[Aston.Villa,Intercept]` <dbl>,\n#   `r_def_team[Bournemouth,Intercept]` <dbl>,\n#   `r_def_team[Brentford,Intercept]` <dbl>,\n#   `r_def_team[Brighton,Intercept]` <dbl>,\n#   `r_def_team[Burnley,Intercept]` <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\nYou'll notice that there are two `Intercept` terms displayed in the output: `b_Intercept` and `Intercept`. You can ignore the `Intercept` column because that refers to [the intercept on the mean-centered scale](https://discourse.mc-stan.org/t/what-is-the-difference-between-intercept-and-b-intercept/20423/7) that `brms` automatically performs beforehand (this is done because it is usually more stable in the model fitting process). So from an inference perspective, you can ignore `Intercept`. The fixed effects are all represented by the terms with `b_` in the prefix (e.g., `b_Intercept`, `b_is_home`).\n\n## Model comparison and selection\n\nLet's imagine we're interested in comparing two different models: \n\n1. the model from above `goals ~ is_home + (1 | off_team) + (1 | def_team)`, and\n\n2. the model with `def_team` random effects removed `goals ~ is_home + (1 | off_team)`.\n\nWe could assess the training data performance of the models, but we know that will likely be biased towards more complex models. Instead, we should consider implementing some type of out-of-sample comparison via train/test splits or $K$-fold cross-validation to compare the models. Bayesian regression models are no different than frequentist regression models in the sense that we should evaluate how the models generalize to new data. However, the downside is that the Bayesian models tend to take longer to fit which makes implementing cross-validation time consuming (depending on the model you're fitting).\n\nTo demonstrate, I'm only going to consider a random 80/20 split in this demo - **but you really should use cross-validation instead!** The code below initializes the training and test datasets we'll use to compare the two models:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1979)\neng_match_goals_scored <- eng_match_goals_scored |>\n  mutate(fold = sample(c(\"train\", \"test\"), n(), replace = TRUE, \n                       prob = c(.8, .2)))\ntrain_data <- eng_match_goals_scored |>\n  filter(fold == \"train\")\ntest_data <- eng_match_goals_scored |>\n  filter(fold == \"test\")\n```\n:::\n\n\n\n\nNext, we'll fit the two models on the training data. For ease, we'll speed things up with [`cmdstanr`](https://mc-stan.org/cmdstanr/) which effectively makes the model fitting faster since it does not directly call any C++ code from R. Additionally, the latest improvements in Stan will be available from R immediately after updating CmdStan via the package installation step: `cmdstanr::install_cmdstan()`. This will also work better with external processes, such as rendering a Stan model in Quarto or when knitting a RMarkdown file (you may notice an error message in my original `brms` fit above that does not use `cmdstanr` - but this \"fake\" error is only an artifact of the rendering step).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the model with defense\nmodel_w_def <- brm(goals ~ is_home + (1 | off_team) + (1 | def_team),\n                        family = poisson, backend = \"cmdstanr\",\n                        data = train_data, seed = 2013)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.2 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.2 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.2 seconds.\nTotal execution time: 4.8 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_wo_def <- brm(goals ~ is_home + (1 | off_team),\n                        family = poisson, backend = \"cmdstanr\",\n                        data = train_data, seed = 2013)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nStart sampling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.7 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.7 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.7 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 3.1 seconds.\n```\n\n\n:::\n:::\n\n\n\n\n\n### RMSE Comparison\n\nFor our initial comparison, we'll evaluate each model's predictions using **root mean squared error (RMSE)**. If you remember from the previous lecture/demo, when we generate predictions with Bayesian regression models we will have a prediction for every observation **with every posterior sample**. For instance, the following code chunk generates the test data predictions using the posterior distribution for the conditional expectation (i.e., `posterior_epred` from last time) with `model_w_def`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred_w_def_samples <- posterior_epred(model_w_def, newdata = test_data)\nas_tibble(test_pred_w_def_samples)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 158\n      V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12   V13\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 0.604  1.50  1.76 1.21  0.918  2.48  1.11 0.904  1.68  2.31 1.05   2.20 1.77 \n 2 1.12   2.62  1.35 1.29  0.890  2.17  1.90 1.62   1.80  2.58 1.44   2.40 1.43 \n 3 1.44   1.80  1.28 1.77  0.900  1.97  1.39 1.02   1.72  2.17 1.00   1.97 1.32 \n 4 0.938  1.51  1.74 0.886 1.00   2.50  1.79 1.23   1.78  2.37 1.76   1.98 1.96 \n 5 1.00   1.82  1.26 1.66  1.03   1.65  1.32 1.20   1.57  2.16 0.976  2.14 1.17 \n 6 0.975  1.89  2.28 1.05  0.911  2.97  2.05 1.26   1.80  2.62 1.58   2.43 2.10 \n 7 1.14   1.83  1.18 1.20  0.815  3.00  1.56 1.31   1.92  2.48 0.972  3.04 0.973\n 8 0.967  2.02  1.59 2.11  1.16   1.77  1.66 1.48   1.94  1.77 1.54   1.99 1.87 \n 9 1.16   1.77  1.56 1.98  1.20   1.87  1.24 1.98   1.71  2.73 1.37   2.30 1.76 \n10 1.27   2.08  1.64 1.52  0.766  2.34  1.84 0.849  1.71  2.60 1.23   2.26 1.29 \n# ℹ 3,990 more rows\n# ℹ 145 more variables: V14 <dbl>, V15 <dbl>, V16 <dbl>, V17 <dbl>, V18 <dbl>,\n#   V19 <dbl>, V20 <dbl>, V21 <dbl>, V22 <dbl>, V23 <dbl>, V24 <dbl>,\n#   V25 <dbl>, V26 <dbl>, V27 <dbl>, V28 <dbl>, V29 <dbl>, V30 <dbl>,\n#   V31 <dbl>, V32 <dbl>, V33 <dbl>, V34 <dbl>, V35 <dbl>, V36 <dbl>,\n#   V37 <dbl>, V38 <dbl>, V39 <dbl>, V40 <dbl>, V41 <dbl>, V42 <dbl>,\n#   V43 <dbl>, V44 <dbl>, V45 <dbl>, V46 <dbl>, V47 <dbl>, V48 <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\nYou can see that this creates a dataset with a row for each posterior sample, along with a column for each observation (158 in this case). We can compute the traditional point estimate version of RMSE, by first taking the posterior mean of each observation's prediction (i.e., column means) and then computing the RMSE across all observations:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(mean((test_data$goals - colMeans(test_pred_w_def_samples))^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.190338\n```\n\n\n:::\n:::\n\n\n\n\nWe can repeat to compare the RMSE with defense removed:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred_wo_def_samples <- posterior_epred(model_wo_def, newdata = test_data)\nsqrt(mean((test_data$goals - colMeans(test_pred_wo_def_samples))^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.238943\n```\n\n\n:::\n:::\n\n\n\n\nFrom this we can see that the model with defense included displays better performance (this should not be surprising). \n\nHowever, this is just a single number summary. Instead of collapsing every observation's prediction into a one number, we could take also advantage of the posterior samples. While you could manually compute the test error for each of the posterior sample predictions, `brms` has a convenient `predictive_error()` function that will return this for you. For convenience, `brms` has a function `predictive_error()` that will return the matrix of test errors (i.e., observed - predicted) for each posterior sample (the rows) and observations (the columns). For instance, the following code chunk generates the test data prediction errors using the posterior distribution for the conditional expectation (i.e., `posterior_epred` from last time) with `model_w_def`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_error_w_def_samples <- predictive_error(model_w_def, newdata = test_data,\n                                             method = \"posterior_epred\")\nas_tibble(test_error_w_def_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 158\n       V1      V2    V3     V4    V5     V6      V7     V8    V9   V10    V11\n    <dbl>   <dbl> <dbl>  <dbl> <dbl>  <dbl>   <dbl>  <dbl> <dbl> <dbl>  <dbl>\n 1 -0.604  0.497  -1.76 -1.21  1.08  -1.48   0.885  1.10    1.32 0.691 -1.05 \n 2 -1.12  -0.617  -1.35 -1.29  1.11  -1.17   0.0957 0.379   1.20 0.419 -1.44 \n 3 -1.44   0.203  -1.28 -1.77  1.10  -0.968  0.613  0.977   1.28 0.828 -1.00 \n 4 -0.938  0.491  -1.74 -0.886 0.997 -1.50   0.205  0.766   1.22 0.629 -1.76 \n 5 -1.00   0.176  -1.26 -1.66  0.973 -0.651  0.677  0.802   1.43 0.840 -0.976\n 6 -0.975  0.112  -2.28 -1.05  1.09  -1.97  -0.0481 0.737   1.20 0.375 -1.58 \n 7 -1.14   0.175  -1.18 -1.20  1.19  -2.00   0.439  0.687   1.08 0.524 -0.972\n 8 -0.967 -0.0228 -1.59 -2.11  0.843 -0.772  0.337  0.524   1.06 1.23  -1.54 \n 9 -1.16   0.225  -1.56 -1.98  0.795 -0.870  0.759  0.0178  1.29 0.268 -1.37 \n10 -1.27  -0.0776 -1.64 -1.52  1.23  -1.34   0.164  1.15    1.29 0.397 -1.23 \n# ℹ 3,990 more rows\n# ℹ 147 more variables: V12 <dbl>, V13 <dbl>, V14 <dbl>, V15 <dbl>, V16 <dbl>,\n#   V17 <dbl>, V18 <dbl>, V19 <dbl>, V20 <dbl>, V21 <dbl>, V22 <dbl>,\n#   V23 <dbl>, V24 <dbl>, V25 <dbl>, V26 <dbl>, V27 <dbl>, V28 <dbl>,\n#   V29 <dbl>, V30 <dbl>, V31 <dbl>, V32 <dbl>, V33 <dbl>, V34 <dbl>,\n#   V35 <dbl>, V36 <dbl>, V37 <dbl>, V38 <dbl>, V39 <dbl>, V40 <dbl>,\n#   V41 <dbl>, V42 <dbl>, V43 <dbl>, V44 <dbl>, V45 <dbl>, V46 <dbl>, …\n```\n\n\n:::\n:::\n\n\n\nThere are two different ways we can take advantage of these samples:\n\n1. **Compute RMSE for each posterior sample (i.e., for each row) and then observe posterior distribution for RMSE.** We'll refer to this as $RMSE_{basic}$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_basic_w_def <- sqrt(rowMeans(test_error_w_def_samples^2))\nhist(rmse_basic_w_def)\n```\n\n::: {.cell-output-display}\n![](17-bayes-model-comp_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n2. **Compute RMSE for each observation (i.e., for each column) and then observe distribution of RMSE values across observations.** We'll refer to this as $RMSE_{alt}$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_alt_w_def <- sqrt(colMeans(test_error_w_def_samples^2))\nhist(rmse_alt_w_def)\n```\n\n::: {.cell-output-display}\n![](17-bayes-model-comp_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nThis histogram is very different than the previous one, because we now observe an RMSE value **for each observation**, averaging over the posterior samples. One of the reasons we would want to do this, is that we can then take the difference in the $RMSE_{alt, i}$ values for each observation $i$ between two models. In other words, for each observation we can compute $RMSE_{alt, i}^{Model1} - RMSE_{alt, i}^{Model2}$. The following code first computes the $RMSE_{alt}$ values for each observation with `model_wo_def` and then computes the difference between the `model_w_def` and `model_wo_def` values across the observations:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_error_wo_def_samples <- predictive_error(model_wo_def, newdata = test_data,\n                                              method = \"posterior_epred\")\nrmse_alt_wo_def <- sqrt(colMeans(test_error_wo_def_samples^2))\n# Display the histogram of the differences\nhist(rmse_alt_w_def - rmse_alt_wo_def)\n```\n\n::: {.cell-output-display}\n![](17-bayes-model-comp_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nUsing this distribution of differences, we can also compute mean difference with standard errors to provide us with some notion of uncertainty about the difference between the performance of the two models:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_rmse_alt_diff <- mean(rmse_alt_w_def - rmse_alt_wo_def)\nse_rmse_alt_diff <- sd(rmse_alt_w_def - rmse_alt_wo_def) / sqrt(nrow(test_data))\n# Display interval:\n(c(sum(mean_rmse_alt_diff) - 2 * se_rmse_alt_diff, # mean - 2 * SE\n   mean_rmse_alt_diff, # mean\n   sum(mean_rmse_alt_diff) + 2 * se_rmse_alt_diff)) # mean + 2 * SE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0522383287 -0.0004198031  0.0513987225\n```\n\n\n:::\n:::\n\n\n\n\nFrom this, we can see that the model with defense is on average slightly better than the model without defense (since lower RMSE is better). However, this difference is within two standard errors so we don't have enough evidence to claim the model with defense is better than the one without.\n\nOf course, in the steps above we generated the predictions using the posterior distributions of the models' conditional expectations (via `posterior_epred`). We could instead repeat the above steps using the posterior predictive distributions (via `posterior_predict`) to generate the RMSE values. I'll leave that for you as an exercise to see how accounting for outcome-level uncertainty affects the results.\n\n### ELPD Comparison\n\nAs an alternative to RMSE, we can also consider **likelihood-based** measures of performance. Simply put, the higher the likelihood of the test data given the model's parameter estimate then the better the fit of the model to the data. This leads us to computing the **expected log pointwise predictive density (ELPD)** of a model which is the sum of the posterior mean log-likelihood values across observations. For instance, the following computes the posterior mean of the log-likelihood values for each test data observation using `model_w_def`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllt_w_defense <- colMeans(log_lik(model_w_def, newdata = test_data,\n                                  # In case there are new level in the test data\n                                  allow_new_levels = TRUE))\nhist(llt_w_defense)\n```\n\n::: {.cell-output-display}\n![](17-bayes-model-comp_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n The `log_lik` function conveniently computes the log-likelihood for us and returns a matrix in a similar way to the `brms` predict functions, with one row per posterior sample and one column per observation. The above histogram displays the distribution of posterior mean log-likelihood values for each observation. The ELPD for this model is simply the sum of these values:\n \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(llt_w_defense)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -242.3975\n```\n\n\n:::\n:::\n\n\n\n \nWe can repeat for the model without defense and take the difference in their ELPD values:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllt_wo_defense <- colMeans(log_lik(model_wo_def, newdata = test_data,\n                                   allow_new_levels = TRUE))\nsum(llt_w_defense) - sum(llt_wo_defense)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.246794\n```\n\n\n:::\n:::\n\n\n\n\nWe observe that the ELPD for the model with defense is better (because it's higher), but is this noticeably higher? In order to make such claim, we can compute the standard error of this ELPD difference since it is the sum of log-likelihood differences between the two models. The standard error for a sum is different than the standard error for a mean, it is computed as standard deviation of the quantity multiplied by the square root of the length. The following code chunk computes the standard error for this particular ELPD difference:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(llt_w_defense - llt_wo_defense) * sqrt(nrow(test_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.916453\n```\n\n\n:::\n:::\n\n\n\n\nThis standard error tells us that the ELPD difference is about 1.5 standard errors away from zero ($\\approx$ 4.246794 / 2.916453). While we have some evidence that the model with defense is better than the model without defense, it is not overwhelming evidence.\n\nOne of the benefits of using the log-likelihood based criterion like ELPD is that it is based on the evaluating the posterior predictive distributions at the observed test data response values. The `posterior_epred` version of predictions that we used in the RMSE demonstration above completely ignores the outcome-level distribution that is accounted for with the posterior predictive distribution via `posterior_predict`. However, `posterior_predict` relies on sampling values from the posterior predictive distribution. This means that the likelihood-based measure can provide us with a stable way of measuring performance for more extreme response values in the tails, without depending on sampling unstable values from the posterior predictive distribution. This leads to the ELPD being a very common approach for comparing two different models. While it is ideal to compute the ELPD differences with cross-validation rather than a single train/test split, you can [read here for information about a leave-one-out shortcut approach](https://mc-stan.org/loo/index.html) for ELPD differences that is commonly used in Bayesian model selection problems. \n\n\n\n",
    "supporting": [
      "17-bayes-model-comp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}