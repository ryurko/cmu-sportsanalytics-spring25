{
  "hash": "2bab386021c73e072b3a6140bbafdaa6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 14: Methods for Approximating the Posterior\"\nformat: html\n---\n\n\n\n\n## Introduction\n\nThe purpose of this demo is to walk through different approaches for approximating the posterior distribution. We'll consider the Beta-Binomial Bayesian model for Caitlin Clark's field goal percentage (FG%) from previous lectures. As we already know, the Beta-Binomial is an example of a conjugate prior model such that the posterior distribution also follows a Beta with a simple calculation. However, we'll imagine that we did not know this fact and will need to use procedures for approximating the posterior distribution. This will prepare us for scenarios where the posterior is actually too difficult to compute, but you'll see how well these types of approximation can perform.\n\nWe will not actually load any data for this demo, instead we'll just use the values from the Week 6 Beta-Binomial demo (`beta_binomial.qmd`). For our prior distribution, we'll use the same Empirical Bayes prior based on the WNBA FG% statistics:\n\n$$\n\\pi \\sim \\text{Beta}(\\alpha = 45.9,\\  \\beta = 68.7)\n$$\n\nWe'll consider the observed data from the first 5 games of Clark's rookie season, where she made $y = 33$ of $n = 85$ FG attempts. As a reminder, the likelihood function is Binomial:\n\n$$\nY | \\pi \\sim \\text{Binomial}(n = 85, \\pi)\n$$\n\nYou can keep in mind during this demo that the true posterior distribution for a Beta-Binomial conjugate model is simply:\n\n$$\n\\pi | (Y = y) \\sim \\text{Beta}(\\alpha + y,\\ \\beta + n - y)\n$$\n\nThis means that the posterior distribution for $\\pi$ after observing the $y = 33$ for $n = 85$ performance is:\n\n$$\n\\pi | (Y = y) \\sim \\text{Beta}(45.9 + 33,\\ 68.7 + 85 - 33)\n$$\n\n## Grid Approximation\n\nWe'll first consider __grid approximation__, which proceeds in the following manner:\n\n### Step 1: define a grid of possible parameter values\n\nThe code chunk below sets up an initial grid of possible values for $\\pi$. There are technically an infinite number of values for $\\pi \\in (0, 1)$, so it is ideal to produce a grid containing as many values as possible - but there is a computational trade-off at play. A larger grid will take longer to work with, but lead to a better approximation of the posterior. For demonstration purposes, we'll consider a grid of 101 values:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n# Step 1: Define a grid of 101 pi values\ngrid_data <- tibble(pi = seq(from = 0, to = 1, length = 101))\ngrid_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 101 × 1\n      pi\n   <dbl>\n 1  0   \n 2  0.01\n 3  0.02\n 4  0.03\n 5  0.04\n 6  0.05\n 7  0.06\n 8  0.07\n 9  0.08\n10  0.09\n# ℹ 91 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n### Step 2: evaluate prior and likelihood at each parameter value in grid\n\nNext, the code chunk below adds columns to the `grid_data` table that are the values for the prior and likelihood for each $\\pi$ in the grid above:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 2: Evaluate the prior and likelihood at each pi\ngrid_data <- grid_data |>\n  mutate(prior = dbeta(pi, 45.9, 68.7), likelihood = dbinom(33, 85, pi))\ngrid_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 101 × 3\n      pi    prior likelihood\n   <dbl>    <dbl>      <dbl>\n 1  0    0          0       \n 2  0.01 5.38e-57   2.39e-43\n 3  0.02 8.88e-44   1.21e-33\n 4  0.03 3.58e-36   4.59e-28\n 5  0.04 7.22e-31   3.55e-24\n 6  0.05 7.98e-27   3.25e-21\n 7  0.06 1.40e-23   7.69e-19\n 8  0.07 6.88e-21   7.14e-17\n 9  0.08 1.33e-18   3.34e-15\n10  0.09 1.26e-16   9.22e-14\n# ℹ 91 more rows\n```\n\n\n:::\n:::\n\n\n\n\n### Step 3: approximate the posterior\n\nWe're now ready to approximate the posterior using the product of the prior and likelihood. This involves first computing the unnormalized version (i.e., the numerator in the posterior formula), and then approximating the posterior by dividing the unnormalized version by the sum of values across the grid. This results in a discretized version of the posterior, with values that sum to 1 due to the normalization. The code chunk below performs these steps and then displays a visual of the approximate posterior distribution:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 3: Approximate the posterior\ngrid_data <- grid_data |> \n  mutate(unnormalized = likelihood * prior, \n         posterior = unnormalized / sum(unnormalized))\n\n# And now display the approx posterior\ngrid_data |>\n  ggplot(aes(x = pi, y = posterior)) + \n  geom_point() + \n  geom_segment(aes(x = pi, xend = pi, \n                   y = 0, yend = posterior)) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n### Step 4: sample from the discretized posterior\n\nAnd finally, we can sample from our discretized posterior PDF to generate a posterior sample:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(2024)\n\n# Step 4: sample from the discretized posterior\nposterior_sample <- grid_data |>\n  # Use the sample_n function to resample with replacement the parameter grid \n  # data weighted by the posterior probability weights\n  sample_n(size = 10000, replace = TRUE, weight = posterior)\n```\n:::\n\n\n\n\nThe following displays a histogram of the sampled parameter values on the density scale:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_sample |>\n  ggplot(aes(x = pi)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  scale_x_continuous(limits = c(0, 1)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nAnd for reference, we draw the true posterior distribution density on top in red:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_sample |>\n  ggplot(aes(x = pi)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dbeta, args = list(78.9, 120.7),\n                color = \"red\") + \n  scale_x_continuous(limits = c(0, 1)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can see that this does a pretty good job at capturing the true posterior distribution, with some variation around it since we're dealing with a sample. Similar to your work in HW4, we can compute various summaries of the posterior via our sample:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute various summaries of posterior sample:\nposterior_sample |> \n  summarize(posterior_mean = mean(pi), \n            posterior_median = median(pi),\n            # Convenient function for mode:\n            posterior_mode = bayesrules::sample_mode(pi),\n            # 95% credible interval:\n            lower_95 = quantile(pi, 0.025),\n            upper_95 = quantile(pi, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  posterior_mean posterior_median posterior_mode lower_95 upper_95\n           <dbl>            <dbl>          <dbl>    <dbl>    <dbl>\n1          0.395              0.4          0.400     0.33     0.46\n```\n\n\n:::\n:::\n\n\n\n\nWe can see these values are relatively close to the true values of the known posterior distribution:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_posterior_alpha <- 78.9\ntrue_posterior_beta <- 120.7\n\n# True posterior mean:\nprint(paste0(\"Posterior mean: \", true_posterior_alpha / (true_posterior_alpha + true_posterior_beta)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Posterior mean: 0.395290581162325\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# True posterior median:\nprint(paste0(\"Posterior median: \", qbeta(0.5, true_posterior_alpha, true_posterior_beta)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Posterior median: 0.394940169853597\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# True posterior mode:\nprint(paste0(\"Posterior mode: \", \n      (true_posterior_alpha - 1) / (true_posterior_alpha + true_posterior_beta - 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Posterior mode: 0.394230769230769\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% credible interval:\nprint(paste0(\"95% credible interval: \", \n             paste0(qbeta(c(0.025, 0.975), true_posterior_alpha, true_posterior_beta),\n                    collapse = \", \")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"95% credible interval: 0.328704819452707, 0.463864243216083\"\n```\n\n\n:::\n:::\n\n\n\n\n## Markov Chain Monte Carlo\n\nWe'll now consider the widely used __Markov chain Monte Carlo (MCMC)__ approach for approximating the posterior. Similar to grid approximation, we can use MCMC to approximate the posterior without directly sampling from it. However, unlike grid approximation, MCMC samples are subsequent values that depend on the previously observed values - hence the _chain_.\n\nTo provide you with a walk-through of how MCMC works, we'll implement the __Metropolis-Hastings algorithm__. \n\nTo start, we need a function that will produce a single iteration of the MCMC sequence of $\\pi$ values. This involves proposing a new value followed by the decision to accept it or not. we'll call to produce a sequence of $\\pi$ values. The code chunk below initializes a function for a single step that uses the Uniform proposal model. Notice that since $\\pi$ is bounded, we'll need to use a floor and ceiling to ensure the value is possible.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_unif_iteration <- function(w, current){\n  \n  # STEP 1: Propose the next chain location\n  proposal <- runif(1, min = current - w, max = current + w)\n  # Use the right floor and cap for the value:\n  proposal <- pmax(pmin(proposal, 1), 0)\n  \n  # STEP 2: Decide whether or not to go there (prior x likelihood)\n  proposal_plaus <- dbeta(proposal, 45.9, 68.7) * dbinom(33, 85, proposal)\n  current_plaus  <- dbeta(current, 45.9, 68.7) * dbinom(33, 85, current)\n  alpha <- min(1, proposal_plaus / current_plaus)\n  next_stop <- sample(c(proposal, current), \n                      size = 1, prob = c(alpha, 1 - alpha))\n  \n  return(data.frame(proposal, alpha, next_stop))\n}\n```\n:::\n\n\n\n\nTo demonstrate, we'll run one iteration with the function using a window of 0.1 and starting with a value of 0.5:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\none_unif_iteration(0.1, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   proposal alpha next_stop\n1 0.4531017     1 0.4531017\n```\n\n\n:::\n:::\n\n\n\n\nYou can see that the proposal is 0.4531017, the acceptance probability is 1 and then we move to the proposed value based on the sample output at the end.\n\nAnd now we can write a function to \"tour\" the posterior distribution via the Metropolis-Hastings algorithm:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmh_tour <- function(N, w, start_pi) {\n  # 1. Start the chain at the initial given value\n  current <- start_pi\n\n  # 2. Initialize the simulation\n  pi <- rep(0, N)\n\n  # 3. Simulate N Markov chain stops\n  for (i in 1:N) {    \n    # Simulate one iteration\n    sim <- one_unif_iteration(w = w, current = current)\n    \n    # Record next location\n    pi[i] <- sim$next_stop\n    \n    # Reset the current location\n    current <- sim$next_stop\n  }\n  \n  # 4. Return the chain locations\n  return(data.frame(iteration = c(1:N), pi))\n}\n```\n:::\n\n\n\n\n\nWe call this function below to generate a chain of 5000 values,  using `w = .1` and `start_pi = 0.5`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1979)\nmh_sim_1 <- mh_tour(N = 5000, w = 0.1, start_pi = 0.5)\n```\n:::\n\n\n\n\nWe'll first display the __trace plot__ for this simulation to display how the chain traverses the __sample space__ of the plausible posterior $\\pi$ values:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmh_sim_1 |>\n  ggplot(aes(x = iteration, y = pi)) + \n  geom_line() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\nNotice that this plot shows how the Markov chain moves across each step, with local dependence observed, but an overall random looking walk. If we ignore the longitudinal structure of these values, we can visualize the distribution of the MCMC samples to see how well it approximates the posterior:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmh_sim_1 |>\n  ggplot(aes(x = pi)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dbeta, args = list(78.9, 120.7),\n                color = \"red\") + \n  scale_x_continuous(limits = c(0, 1)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\nAs you can see, this approximates the posterior distribution extremely well! Even better than the grid approximation from early - which technically had more samples! Just like the grid approximation posterior, we can compute various summaries (like the mean, mode, percentiles for credible intervals, etc).\n\nJust for the sake of demonstration, the following code chunk repeats the this MCMC sampling from above but uses __independence sampling__ with a Beta distribution for the proposal distribution. We can again see how well this approximates the posterior distribution:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_beta_iteration <- function(a, b, current){\n  \n  # STEP 1: Propose the next chain location\n  proposal <- rbeta(1, a, b)\n  \n  \n  # STEP 2: Decide whether or not to go there (prior x likelihood)\n  proposal_plaus <- dbeta(proposal, 45.9, 68.7) * dbinom(33, 85, proposal)\n  proposal_q     <- dbeta(proposal, a, b)\n  current_plaus  <- dbeta(current, 45.9, 68.7) * dbinom(33, 85, current)\n  current_q      <- dbeta(current, a, b)\n  alpha <- min(1, proposal_plaus / current_plaus * current_q / proposal_q)\n  next_stop <- sample(c(proposal, current), \n                      size = 1, prob = c(alpha, 1 - alpha))\n  \n  return(data.frame(proposal, alpha, next_stop))\n}\n\nmh_beta_tour <- function(N, a, b, start_pi) {\n  # 1. Start the chain at the initial given value\n  current <- start_pi\n\n  # 2. Initialize the simulation\n  pi <- rep(0, N)\n\n  # 3. Simulate N Markov chain stops\n  for (i in 1:N) {    \n    # Simulate one iteration\n    sim <- one_beta_iteration(a, b, current)\n    \n    # Record next location\n    pi[i] <- sim$next_stop\n    \n    # Reset the current location\n    current <- sim$next_stop\n  }\n  \n  # 4. Return the chain locations\n  return(data.frame(iteration = c(1:N), pi))\n}\n\n\nset.seed(2013)\nmh_sim_2 <- mh_beta_tour(N = 5000, a = 1, b = 1, start_pi = 0.5)\n\n# Create the plots:\nmh_sim_2 |>\n  ggplot(aes(x = iteration, y = pi)) + \n  geom_line() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmh_sim_2 |>\n  ggplot(aes(x = pi)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  stat_function(fun = dbeta, args = list(78.9, 120.7),\n                color = \"red\") + \n  scale_x_continuous(limits = c(0, 1)) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](13-posterior-approx_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "13-posterior-approx_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}