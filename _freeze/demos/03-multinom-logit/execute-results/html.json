{
  "hash": "1bbe563c7adfa6755acd0facce856b8c",
  "result": {
    "markdown": "---\ntitle: 'Lecture 4: Multinomial Logistic Regression for Expected Points'\nformat: html\n---\n\n\n## Introduction\n\nThe goal of this demo is to introduce how to fit and evaluate a __multinomial logistic regression__ model in the context of modeling the next scoring event in American football. For this demo, we'll use an example dataset of NFL play-by-play data from 2013 to 2023. This is initialized with a column including the next score in the half for each play (you can find the script on Canvas, which creates the dataset using [`nflreadr`](https://nflreadr.nflverse.com/)). \n\nThe following code chunk reads in the relevant NFL play-by-play dataset (assuming it is in the correct directory) and performs some initial pre-processing relevant for the expected points model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nnfl_ep_model_data <- read_rds(here::here(\"data/model_nfl_pbp_data.rds\"))\nnfl_ep_model_data <- nfl_ep_model_data |>\n  # Make the No_Score level the reference level:\n  mutate(Next_Score_Half = fct_relevel(Next_Score_Half, \"No_Score\"),\n         # log transform of yards to go and indicator for two minute warning:\n         log_ydstogo = log(ydstogo),\n         # Changing down into a factor variable: \n         down = factor(down))\n\nnfl_ep_model_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 422,904 × 21\n   game_id     Next_Score_Half Drive_Score_Half play_type game_seconds_remaining\n   <chr>       <fct>                      <dbl> <chr>                      <dbl>\n 1 2013_01_AR… Opp_Touchdown                  4 pass                        3593\n 2 2013_01_AR… Opp_Touchdown                  4 run                         3572\n 3 2013_01_AR… Opp_Touchdown                  4 pass                        3536\n 4 2013_01_AR… Opp_Touchdown                  4 no_play                     3507\n 5 2013_01_AR… Opp_Touchdown                  4 run                         3476\n 6 2013_01_AR… Opp_Touchdown                  4 no_play                     3446\n 7 2013_01_AR… Opp_Touchdown                  4 pass                        3428\n 8 2013_01_AR… Opp_Touchdown                  4 pass                        3424\n 9 2013_01_AR… Opp_Touchdown                  4 punt                        3397\n10 2013_01_AR… Touchdown                      4 run                         3386\n# ℹ 422,894 more rows\n# ℹ 16 more variables: half_seconds_remaining <dbl>, yardline_100 <dbl>,\n#   posteam <chr>, defteam <chr>, home_team <chr>, ydstogo <dbl>, season <int>,\n#   qtr <dbl>, down <fct>, week <int>, drive <dbl>, score_differential <dbl>,\n#   posteam_timeouts_remaining <dbl>, defteam_timeouts_remaining <dbl>,\n#   desc <chr>, log_ydstogo <dbl>\n```\n:::\n:::\n\n\n\n## Fitting a multinomial logistic regression model\n\nIn order to fit a multinomial logistic regression model in `R`, the easiest way is to use the `nnet` package with the `multinom` function. The following code chunk fits this model to the full dataset with the `Next_Score_Half` variable as the response with the context variables as the predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\ninit_ep_model <- multinom(Next_Score_Half ~ half_seconds_remaining + \n                            yardline_100 + down + log_ydstogo + \n                            log_ydstogo * down + yardline_100 * down, \n                          data = nfl_ep_model_data, maxit = 300)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  98 (78 variable)\ninitial  value 822933.185674 \niter  10 value 646448.661731\niter  20 value 628562.786640\niter  30 value 616585.530613\niter  40 value 600078.459583\niter  50 value 597249.917485\niter  60 value 568326.053579\niter  70 value 561806.646221\niter  80 value 556313.688644\niter  90 value 555528.854290\niter 100 value 555518.276158\nfinal  value 555518.120956 \nconverged\n```\n:::\n:::\n\n\nNote the use of `maxit = 300` is to provide a sufficient number of steps for model fitting. You'll notice the printing of iteration steps here because this package is actually the simplest package used for fitting neural networks in `R`. \n\nNotice what happens when we use the `summary()` function on this model (it takes some time to run):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(init_ep_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = Next_Score_Half ~ half_seconds_remaining + \n    yardline_100 + down + log_ydstogo + log_ydstogo * down + \n    yardline_100 * down, data = nfl_ep_model_data, maxit = 300)\n\nCoefficients:\n               (Intercept) half_seconds_remaining yardline_100      down2\nField_Goal      -0.2607051            0.004116958 -0.034840581  0.7076165\nOpp_Field_Goal  -3.0189007            0.004293194  0.001137907  0.1443799\nOpp_Safety      -8.9905777            0.004245522  0.058165564 -1.0759002\nOpp_Touchdown   -2.8386923            0.004508825  0.001923287  0.0914707\nSafety          -4.1544215            0.004459871 -0.019844872 -0.8222278\nTouchdown        2.1128586            0.004298760 -0.036028100 -1.0277706\n                    down3      down4 log_ydstogo down2:log_ydstogo\nField_Goal      0.8926587  1.2557561   0.3805016     -0.2836294318\nOpp_Field_Goal  0.2142137  0.3490739   0.1395655     -0.0468113016\nOpp_Safety     -0.4023122  0.8780304  -0.6304597      0.7793959964\nOpp_Touchdown   0.1912419  0.3611854   0.1176799     -0.0008362159\nSafety         -0.7113207 -0.5755544  -0.3702644      0.4053892714\nTouchdown      -1.4807308 -2.8058121  -0.5186325      0.3515486857\n               down3:log_ydstogo down4:log_ydstogo yardline_100:down2\nField_Goal          -0.276146057        -0.1963895      -0.0014910525\nOpp_Field_Goal      -0.053572298        -0.1586719       0.0015471520\nOpp_Safety           0.753610712         0.7305257      -0.0066148761\nOpp_Touchdown       -0.008979889        -0.1477311       0.0008414841\nSafety               0.331151454         0.3681949      -0.0006310041\nTouchdown            0.307635794         0.2487536       0.0002840841\n               yardline_100:down3 yardline_100:down4\nField_Goal           -0.006491710       -0.022099391\nOpp_Field_Goal        0.003617006        0.007415363\nOpp_Safety           -0.012361132       -0.028266233\nOpp_Touchdown         0.002475162        0.006787692\nSafety                0.001022146       -0.001257291\nTouchdown             0.005196036        0.020872169\n\nStd. Errors:\n                (Intercept) half_seconds_remaining yardline_100        down2\nField_Goal     9.973877e-03           2.056382e-05 0.0003265603 9.222868e-03\nOpp_Field_Goal 1.301499e-02           2.164403e-05 0.0003570708 3.525795e-03\nOpp_Safety     3.608208e-05           6.056503e-05 0.0009598729 1.676993e-05\nOpp_Touchdown  1.249336e-02           2.114300e-05 0.0003484688 4.994918e-03\nSafety         2.031640e-04           4.981585e-05 0.0010522061 1.154787e-04\nTouchdown      9.156701e-03           2.042802e-05 0.0003097515 1.026284e-02\n                      down3        down4  log_ydstogo down2:log_ydstogo\nField_Goal     9.165213e-03 4.368126e-03 0.0076911593      0.0084179580\nOpp_Field_Goal 3.705261e-03 2.529465e-03 0.0070133975      0.0086822558\nOpp_Safety     1.585732e-05 1.948469e-05 0.0002905214      0.0001058366\nOpp_Touchdown  4.972851e-03 3.315516e-03 0.0077647303      0.0088275451\nSafety         7.438725e-05 5.127217e-05 0.0007459947      0.0003197118\nTouchdown      1.038676e-02 2.457417e-03 0.0071809004      0.0078822666\n               down3:log_ydstogo down4:log_ydstogo yardline_100:down2\nField_Goal          0.0085571300      0.0099954257       0.0004123602\nOpp_Field_Goal      0.0086850330      0.0095492903       0.0003746628\nOpp_Safety          0.0001036511      0.0000755188       0.0009717132\nOpp_Touchdown       0.0090569632      0.0103499484       0.0003861318\nSafety              0.0002244464      0.0001505608       0.0011547005\nTouchdown           0.0080808936      0.0105894677       0.0003887755\n               yardline_100:down3 yardline_100:down4\nField_Goal           0.0004477493       0.0005864862\nOpp_Field_Goal       0.0003898272       0.0004537012\nOpp_Safety           0.0011035945       0.0013637166\nOpp_Touchdown        0.0004097842       0.0004720251\nSafety               0.0013038376       0.0015717666\nTouchdown            0.0004227704       0.0004995965\n\nResidual Deviance: 1111036 \nAIC: 1111192 \n```\n:::\n:::\n\n\nYou see the usual type of output (coefficients, standard errors, deviance, AIC), but we see coefficient estimates for each next score outcome (except for the reference level `No_Score`).\n\nAlternatively, it can be helpful to visualize the implied relationships between the various features and the outcome probabilities using visualization techniques. In order to do this, we first need to get the fitted probabilities for each scoring event. For a `nnet` multinomial logistic regression model, we can use the `predict()` function with `type = \"probs\"` as an input to return the matrix of probabilities for each event:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnext_score_probs <- predict(init_ep_model, \n                            newdata = nfl_ep_model_data, type = \"probs\") |>\n  as_tibble()\nnext_score_probs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 422,904 × 7\n   No_Score Field_Goal Opp_Field_Goal Opp_Safety Opp_Touchdown  Safety Touchdown\n      <dbl>      <dbl>          <dbl>      <dbl>         <dbl>   <dbl>     <dbl>\n 1 0.000970      0.204         0.157    0.00476          0.279 0.00427     0.350\n 2 0.000864      0.236         0.126    0.00217          0.222 0.00422     0.408\n 3 0.000912      0.234         0.119    0.00167          0.204 0.00445     0.437\n 4 0.000876      0.269         0.0957   0.000879         0.164 0.00398     0.465\n 5 0.00131       0.324         0.139    0.00133          0.236 0.00328     0.295\n 6 0.00132       0.274         0.131    0.00188          0.227 0.00460     0.361\n 7 0.00158       0.261         0.151    0.00282          0.262 0.00463     0.317\n 8 0.00164       0.238         0.184    0.00363          0.320 0.00471     0.247\n 9 0.00173       0.265         0.168    0.00226          0.285 0.00669     0.271\n10 0.00255       0.199         0.170    0.00655          0.291 0.00411     0.327\n# ℹ 422,894 more rows\n```\n:::\n:::\n\n\nThe following code chunk joins these probabilities to the original dataset, and creates a visual that displays a smooth regression (we'll cover this later) of the model's probabilities as a function of certain inputs. _Note: these visuals do not represent the exact relationship, but rather just a summary of the relationships._ [This code](https://github.com/ryurko/nflscrapR-models/blob/master/R/nflWAR_paper_figures_code/nflWAR_ep_figures.R) was used to generate the figures in [my paper](https://arxiv.org/abs/1802.00998).\n\n\n::: {.cell fig.asp='0.25'}\n\n```{.r .cell-code}\n# Create the facetted version for each events probability based on down:\nnfl_ep_model_data |> \n  # Join the probs:\n  bind_cols(next_score_probs) |>\n  # Only grab a subset of columns\n  dplyr::select(yardline_100, down, No_Score:Touchdown) |>\n  pivot_longer(No_Score:Touchdown,\n               # Name of the column for the outcomes\n               names_to = \"next_score_type\",\n               # Name of the column for the predicted probabilities\n               values_to = \"pred_prob\") |>\n  # Create a score value column to use for the color legend\n  mutate(event_value = case_when(\n                         next_score_type == \"No_Score\" ~ 0,\n                         next_score_type == \"Touchdown\" ~ 7,\n                         next_score_type == \"Field_Goal\" ~ 3,\n                         next_score_type == \"Safety\" ~ 2,\n                         next_score_type == \"Opp_Field_Goal\" ~ -3,\n                         next_score_type == \"Opp_Safety\" ~ -2,\n                         TRUE ~ -7),\n         # Label for down\n         down_label = case_when(\n                        down == 1 ~ \"1st down\",\n                        down == 2 ~ \"2nd down\",\n                        down == 3 ~ \"3rd down\",\n                        TRUE ~ \"4th down\")) |>\n  ggplot(aes(x = yardline_100, y = pred_prob, color = event_value,\n             group = next_score_type)) + \n  geom_smooth(se = FALSE) + \n    ylim(0,1) + \n    facet_wrap(~down_label, ncol = 4) + \n  theme_bw() +\n  labs(x = \"Yards from opponent's end zone\", y = \"Predicted probability\") +\n  scale_color_gradient2(low = \"darkorange4\", mid = \"gray\",\n                        high = \"darkslateblue\", \n                        breaks = c(-7, -3, -2, 0, 2, 3, 7),\n                        labels=c(\" -Touchdown (-7) \", \" -Field Goal (-3) \",\n                                 \" -Safety (-2) \", \" No Score (0) \",\n                                 \" Safety (2) \", \" Field Goal (3) \", \n                                 \" Touchdown (7) \"),\n                        guide = guide_legend(title = NULL, ncol = 7,\n                                             reverse = TRUE,\n                                             override.aes = list(size = 5))) +\n  theme(legend.background = element_rect(fill = \"white\"),\n        axis.title = element_text(size = 18),\n        axis.text.y = element_text(size = 16),\n        axis.text.x = element_text(size = 10),\n        legend.position = \"bottom\",\n        strip.background = element_blank(),\n        strip.text = element_text(size = 18),\n        legend.text = element_text(size = 12))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output-display}\n![](03-multinom-logit_files/figure-html/unnamed-chunk-5-1.png){width=1536}\n:::\n:::\n\n\nWe can also create a figure that summarizes the relationships between different features with the actual variable of interest: __expected points__. The first step is to compute the expected points, which we can do using a simple line of code that weights each probability with a point value for the outcome:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnext_score_probs <- next_score_probs |>\n  mutate(ep = Touchdown * 7 + Field_Goal * 3 + Safety * 2 +\n           Opp_Touchdown * -7 + Opp_Field_Goal * -3 + Opp_Safety * -2)\n```\n:::\n\n\nAnd then use a similar approach as before, including historical models, to display the implied relationships:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Expected points relationships, for the historical models:\n\n# First the Carter model:\ncarter_data <- tibble(\"yardline_100\" = c(95, 85, 75, 65, 55, 45, 35, 25, 15, 5),\n                      \"ep\" = c(-1.245, -.637, .236, .923, 1.538, 2.392, \n                               3.167, 3.681, 4.572, 6.041)) |>\n  mutate(model = \"Carter\")\n\n# and Hidden Game of Football model:\nhgf_data <- tibble(\"yardline_100\" = c(100, 75, 50, 25, 0),\n                   \"ep\" = c(-2, 0, 2, 4, 6)) |>\n  mutate(model = \"Hidden Game of Football\")\n\n# Display our model's results by down and then compare to the historical\n# models from Carter and the Hidden Game of Football:\nnfl_ep_model_data |> \n  bind_cols(next_score_probs) |>\n  # Only grab a subset of columns\n  dplyr::select(yardline_100, down, ep) |>\n  ggplot(aes(x = yardline_100, y = ep,\n             color = as.factor(down))) + \n  geom_smooth(size = 2) + \n  labs(x = \"Yards from opponent's end zone\",\n       y = \"Expected points value\",\n       color = \"Model\") +\n  theme_bw() + \n  scale_y_continuous(limits = c(-4, 6),breaks = seq(-4, 6, 2)) + \n  geom_line(data = bind_rows(carter_data, hgf_data),\n            aes(x = yardline_100, y = ep, color = model),\n            size = 2, linetype = \"dashed\") + \n  geom_point(data = bind_rows(carter_data, hgf_data),\n             aes(x = yardline_100, y = ep, color = model),\n             size = 5, alpha = 0.5) + \n  scale_x_continuous(breaks = seq(from = 5, to = 95, by = 10)) +\n  scale_color_manual(values = c(\"#0000FF\",\n                                \"#5537AA\",\n                                \"#AA6E55\",\n                                \"#FFA500\",\n                                \"seagreen4\",\n                                \"darkred\"),\n                     labels = c(\"Multilogit - 1st down\",\n                                \"Multilogit - 2nd down\",\n                                \"Multilogit - 3rd down\",\n                                \"Multilogit - 4th down\",\n                                \"Carter\",\n                                \"Hidden Game of Football\")) +\n  theme(axis.title = element_text(size = 18),\n        axis.text = element_text(size = 12),\n        legend.text = element_text(size = 8),\n        legend.title = element_text(size = 10),\n        legend.position = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2329 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](03-multinom-logit_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Cross-validation calibration\n\nSince our goal relies on using the model's probability estimates, we can evaluate the model similar to the expected goals model: via out-of-sample calibration. The notable difference is that we need to assess how well the model is calibrated __for each scoring event__. The following code generates the __leave-one-year-out cross-validation__ predictions for each play in the dataset (note that the model fitting steps are printed for each season fold):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninit_loso_cv_preds <- \n  map_dfr(unique(nfl_ep_model_data$season), \n          function(x) {\n            # Separate test and training data:\n            test_data <- nfl_ep_model_data |> filter(season == x)\n            train_data <- nfl_ep_model_data |> filter(season != x)\n            # Fit multinomial logistic regression model:\n            ep_model <- \n              multinom(Next_Score_Half ~ half_seconds_remaining + yardline_100 + down + \n                         log_ydstogo + log_ydstogo * down + yardline_100 * down, \n                       data = train_data, maxit = 300)\n            # Return dataset of class probabilities:\n            predict(ep_model, newdata = test_data, type = \"probs\") |>\n              as_tibble() |>\n              mutate(Next_Score_Half = test_data$Next_Score_Half,\n                     season = x)\n              })\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  98 (78 variable)\ninitial  value 747749.055246 \niter  10 value 581440.350658\niter  20 value 566007.216610\niter  30 value 553092.654185\niter  40 value 538119.354133\niter  50 value 536424.391744\niter  60 value 520828.612116\niter  70 value 511272.480459\niter  80 value 505399.169176\niter  90 value 504210.861998\niter 100 value 504171.076545\niter 110 value 504167.970371\niter 120 value 504167.510171\nfinal  value 504167.490166 \nconverged\n# weights:  98 (78 variable)\ninitial  value 748467.096091 \niter  10 value 582511.589203\niter  20 value 567340.012597\niter  30 value 553488.233716\niter  40 value 537655.594760\niter  50 value 534824.876335\niter  60 value 525502.198924\niter  70 value 512996.713239\niter  80 value 506475.015847\niter  90 value 504965.487881\niter 100 value 504938.379836\nfinal  value 504937.826673 \nconverged\n# weights:  98 (78 variable)\ninitial  value 747873.593495 \niter  10 value 581271.333157\niter  20 value 565846.033579\niter  30 value 553897.370602\niter  40 value 536428.400172\niter  50 value 533324.447646\niter  60 value 519623.381518\niter  70 value 510091.467079\niter  80 value 505678.094433\niter  90 value 504541.138811\niter 100 value 504525.471455\nfinal  value 504525.424807 \nconverged\n# weights:  98 (78 variable)\ninitial  value 748525.473395 \niter  10 value 584430.991679\niter  20 value 569735.869879\niter  30 value 558267.533816\niter  40 value 545227.965954\niter  50 value 542097.115990\niter  60 value 534837.983927\niter  70 value 514739.234466\niter  80 value 507703.178044\niter  90 value 505555.625847\niter 100 value 505532.168333\nfinal  value 505532.108496 \nconverged\n# weights:  98 (78 variable)\ninitial  value 749165.677834 \niter  10 value 586127.439746\niter  20 value 569455.903919\niter  30 value 555673.870004\niter  40 value 537108.662513\niter  50 value 535388.738604\niter  60 value 525520.820192\niter  70 value 510458.157722\niter  80 value 505818.284259\niter  90 value 505018.434102\niter 100 value 505008.890377\nfinal  value 505008.882666 \nconverged\n# weights:  98 (78 variable)\ninitial  value 750086.093335 \niter  10 value 585080.586820\niter  20 value 570595.228537\niter  30 value 555605.385864\niter  40 value 543202.950882\niter  50 value 541039.078280\niter  60 value 527670.700071\niter  70 value 516882.317084\niter  80 value 510327.264273\niter  90 value 507410.311047\niter 100 value 507383.469803\niter 110 value 507382.051725\nfinal  value 507381.972455 \nconverged\n# weights:  98 (78 variable)\ninitial  value 749346.647478 \niter  10 value 586291.599094\niter  20 value 571463.633794\niter  30 value 555251.569408\niter  40 value 540714.413545\niter  50 value 538208.068421\niter  60 value 524922.778076\niter  70 value 511749.141809\niter  80 value 506515.736547\niter  90 value 505678.465320\niter 100 value 505662.467396\nfinal  value 505662.424972 \nconverged\n# weights:  98 (78 variable)\ninitial  value 749852.584117 \niter  10 value 585091.048570\niter  20 value 570487.939148\niter  30 value 554224.337618\niter  40 value 545207.608450\niter  50 value 542998.382583\niter  60 value 523574.947287\niter  70 value 512352.949701\niter  80 value 507690.704984\niter  90 value 506457.363312\niter 100 value 506441.642486\nfinal  value 506441.579198 \nconverged\n# weights:  98 (78 variable)\ninitial  value 745867.360132 \niter  10 value 581916.027979\niter  20 value 566655.115894\niter  30 value 554775.921190\niter  40 value 536693.143905\niter  50 value 534378.950264\niter  60 value 527881.563685\niter  70 value 513033.555210\niter  80 value 505528.391065\niter  90 value 504279.763183\niter 100 value 504257.414205\nfinal  value 504257.309441 \nconverged\n# weights:  98 (78 variable)\ninitial  value 746513.402301 \niter  10 value 585896.386604\niter  20 value 571358.916426\niter  30 value 556119.424347\niter  40 value 541842.753103\niter  50 value 539199.324648\niter  60 value 521593.725464\niter  70 value 510590.617369\niter  80 value 504547.807280\niter  90 value 503510.806223\niter 100 value 503498.307119\nfinal  value 503498.009677 \nconverged\n# weights:  98 (78 variable)\ninitial  value 745884.873323 \niter  10 value 582559.463464\niter  20 value 567597.676369\niter  30 value 553086.905300\niter  40 value 539643.354316\niter  50 value 537190.694757\niter  60 value 530614.346149\niter  70 value 516141.509588\niter  80 value 506361.234224\niter  90 value 503662.111159\niter 100 value 503639.873014\nfinal  value 503639.857722 \nconverged\n```\n:::\n:::\n\n\nWe can then generate the calibration summary as before, with the caveat we need to do this for each outcome probability. Since the above code returns a dataset with a column for each outcome separately, we need to __pivot__ the dataset from wide to long so that there is a row for each play-outcome combination. We can do this using the useful `pivot_longer()` function as shown below, with the same summary steps as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nep_cv_loso_calibration_results <- init_loso_cv_preds |>\n  # First specify which columns to turn into rows\n  pivot_longer(No_Score:Touchdown,\n               # Name of the column for the outcomes\n               names_to = \"next_score_type\",\n               # Name of the column for the predicted probabilities\n               values_to = \"pred_prob\") |>\n  # And then the same steps as before but now with grouping by score outcome:\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) |>\n  group_by(next_score_type, bin_pred_prob) |>\n  summarize(n_plays = n(), \n            n_scoring_event = length(which(Next_Score_Half == next_score_type)),\n            bin_actual_prob = n_scoring_event / n_plays,\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_plays),\n            .groups = \"drop\") |>\n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0))\n```\n:::\n\n\nAnd then with this dataset we can create calibration plots for each outcome, using similar code as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nep_cv_loso_calibration_results |>\n  mutate(next_score_type = fct_relevel(next_score_type,\n                                       \"Opp_Safety\", \"Opp_Field_Goal\", \n                                       \"Opp_Touchdown\", \"No_Score\", \"Safety\", \n                                       \"Field_Goal\", \"Touchdown\"),\n  next_score_type = fct_recode(next_score_type, \n                               \"-Field Goal (-3)\" = \"Opp_Field_Goal\",\n                               \"-Safety (-2)\" = \"Opp_Safety\", \n                               \"-Touchdown (-7)\" = \"Opp_Touchdown\",\n                               \"Field Goal (3)\" = \"Field_Goal\", \n                               \"No Score (0)\" = \"No_Score\",\n                               \"Touchdown (7)\" = \"Touchdown\", \n                               \"Safety (2)\" = \"Safety\")) |>\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point(size = 0.5) +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) + \n  coord_equal() +   \n  scale_x_continuous(limits = c(0, 1)) + \n  scale_y_continuous(limits = c(0, 1)) + \n  labs(x = \"Estimated next score probability\", \n       y = \"Observed next score probability\") + \n  theme_bw() + \n  theme(strip.background = element_blank(), \n        axis.text.x = element_text(angle = 90)) +\n  facet_wrap(~ next_score_type, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](03-multinom-logit_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n_What stands out for you when inspecting the different calibration plots?_\n\n## Recap\n\n+ Introduced fitting and interpreting a multinomial logistic regression model\n\n+ Walked through steps for performing cross-validation calibration for multiple outcomes\n\n\n\n",
    "supporting": [
      "03-multinom-logit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}