{
  "hash": "433a60695564ae5b11b4f502f2eee89e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 18: Modeling team ratings and posterior predictions\"\nformat: html\n---\n\n\n\n\n## Introduction\n\nThe purpose of this demo is introduce basic approaches for modeling team ratings based on the soccer match data from HW1. As a reminder, this dataset in the homeworks/hw1 folder on Canvas (`soccer_match_goals.csv`) and contains the goals scored across men's soccer matches in the five biggest European leagues during the 2023-2024 season. This dataset contains the following columns:\n\n1. `goals`: number of goals scored by `team` in a match\n\n2. `xG`: accumulated number of expected goals by `team` in a match\n\n3. `off_team`: the team name affiliated with the __offense scoring__ the number of `goals` and total `xG`\n\n4. `def_team`: the team name affiliated with the __defense allowing__ the number of `goals` and total `xG`\n\n5. `league`: string denoting the country: ENG (Premier League), ESP (La Liga), FRA (Ligue 1), GER (Fußball-Bundesliga), and ITA (Serie A)\n\n6. `match_id`: unique identifier for each match, such that each match has two rows in the dataset, one for the home team and one for the away team\n\n7. `is_home`: binary indicator denoting whether or not the team was the home team in the match.\n\nNote that each row in this dataset corresponds to information about a team's offensive performance for a single match. This means a team is repeatedly observed for each match they played during the 2023-2024 season. I constructed this dataset using the [worldfootballR](https://jaseziv.github.io/worldfootballR/) package, and the script `init_soccer_match_goals.R` on Canvas is the code I used to make the dataset.\n\nThe code chunk below loads in the data, and then for ease we'll only consider matches in the Premier League (i.e., `league == ENG`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmatch_goals_scored <- read_csv(here::here(\"data/soccer_match_goals.csv\"))\n\n# Create the ENG only table:\neng_match_goals_scored <- match_goals_scored |>\n  filter(league == \"ENG\")\neng_match_goals_scored\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 760 × 7\n   goals    xG off_team       def_team        league match_id is_home\n   <dbl> <dbl> <chr>          <chr>           <chr>     <dbl>   <dbl>\n 1     0   0.3 Burnley        Manchester City ENG           1       1\n 2     2   0.8 Arsenal        Nott'ham Forest ENG           2       1\n 3     0   2.7 Everton        Fulham          ENG           3       1\n 4     0   0.5 Sheffield Utd  Crystal Palace  ENG           4       1\n 5     4   4   Brighton       Luton Town      ENG           5       1\n 6     1   1.3 Bournemouth    West Ham        ENG           6       1\n 7     5   3.3 Newcastle Utd  Aston Villa     ENG           7       1\n 8     2   2.2 Brentford      Tottenham       ENG           8       1\n 9     1   1.4 Chelsea        Liverpool       ENG           9       1\n10     1   2.2 Manchester Utd Wolves          ENG          10       1\n# ℹ 750 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Poisson multilevel model\n\nAs discussed in lecture, we'll start with the traditional `lme4` multilevel model that we've discussed earlier in the semester:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nglmer_goal_model <- glmer(goals ~ 0 + is_home + (1 | off_team) + (1 | def_team),\n                               family = poisson,\n                               data = eng_match_goals_scored)\nsummary(glmer_goal_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: goals ~ 0 + is_home + (1 | off_team) + (1 | def_team)\n   Data: eng_match_goals_scored\n\n     AIC      BIC   logLik deviance df.resid \n  2386.6   2400.5  -1190.3   2380.6      757 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.69427 -0.68131 -0.09881  0.60867  3.12444 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n off_team (Intercept) 0.08959  0.2993  \n def_team (Intercept) 0.08241  0.2871  \nNumber of obs: 760, groups:  off_team, 20; def_team, 20\n\nFixed effects:\n        Estimate Std. Error z value Pr(>|z|)    \nis_home  0.25349    0.05621    4.51 6.48e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\nWe could proceed to use this model in the format we discussed earlier in the semester... however that fails to propagate the uncertainty we have about the parameters. Instead we can fit the fully Bayesian version of this model with relative ease.\n\n## Bayesian hierarchical Poisson model\n\nRather than using `lme4` to fit the frequentist version of the multilevel model, we can use the exact same type of syntax with the [`rstanarm` package](https://mc-stan.org/rstanarm/) to fit the fully Bayesian version of the model. Behind the scenes, this calls Stan and uses **pre-compiled** model code to perform the posterior sampling. If we do not have any informative prior knowledge, then we can be relatively lazy easy in terms of using the default priors that are autoscaled based on the variance in the data - resulting in weakly informative priors that apply shrinkage to parameter estimates. The code chunk below fits the Bayesian model with `stan_glmer()` which is simply the Bayesian version of the `glmer()` function from above __with the exact same syntax__:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstanarm)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'rstanarm' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Rcpp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Rcpp' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is rstanarm version 2.32.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n  options(mc.cores = parallel::detectCores())\n```\n\n\n:::\n\n```{.r .cell-code}\nteam_goals_model <- stan_glmer(goals ~ 0 + is_home + (1 | off_team) + (1 | def_team),\n                               family = poisson,\n                               data = eng_match_goals_scored, seed = 2013)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.754 seconds (Warm-up)\nChain 1:                0.96 seconds (Sampling)\nChain 1:                2.714 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.47 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.503 seconds (Warm-up)\nChain 2:                0.802 seconds (Sampling)\nChain 2:                2.305 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.786 seconds (Warm-up)\nChain 3:                0.694 seconds (Sampling)\nChain 3:                2.48 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.669 seconds (Warm-up)\nChain 4:                0.901 seconds (Sampling)\nChain 4:                2.57 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\n\nNotice that this model fits very quickly! We see the same type of output from running Stan code like before, with displays of the run time of the four chains. Notice that the default settings using 2000 iterations in total for each chain, with the first half used for burn-in. We could change the same inputs from before, e.g., `chains`, `iter`, and `warmup`, if we want to make any changes to the number of samples.\n\nWhat happens when we call the `summary()` function on this model?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(team_goals_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel Info:\n function:     stan_glmer\n family:       poisson [log]\n formula:      goals ~ 0 + is_home + (1 | off_team) + (1 | def_team)\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 760\n groups:       off_team (20), def_team (20)\n\nEstimates:\n                                          mean   sd   10%   50%   90%\nis_home                                  0.2    0.1  0.2   0.3   0.3 \nb[(Intercept) off_team:Arsenal]          0.5    0.1  0.3   0.5   0.7 \nb[(Intercept) off_team:Aston_Villa]      0.3    0.1  0.2   0.3   0.5 \nb[(Intercept) off_team:Bournemouth]      0.0    0.1 -0.1   0.0   0.2 \nb[(Intercept) off_team:Brentford]        0.1    0.1 -0.1   0.1   0.2 \nb[(Intercept) off_team:Brighton]         0.1    0.1 -0.1   0.1   0.2 \nb[(Intercept) off_team:Burnley]         -0.2    0.1 -0.4  -0.2   0.0 \nb[(Intercept) off_team:Chelsea]          0.4    0.1  0.2   0.4   0.5 \nb[(Intercept) off_team:Crystal_Palace]   0.1    0.1 -0.1   0.1   0.3 \nb[(Intercept) off_team:Everton]         -0.2    0.1 -0.4  -0.2   0.0 \nb[(Intercept) off_team:Fulham]           0.1    0.1 -0.1   0.1   0.2 \nb[(Intercept) off_team:Liverpool]        0.4    0.1  0.3   0.4   0.6 \nb[(Intercept) off_team:Luton_Town]       0.0    0.1 -0.2   0.0   0.2 \nb[(Intercept) off_team:Manchester_City]  0.5    0.1  0.4   0.5   0.7 \nb[(Intercept) off_team:Manchester_Utd]   0.1    0.1 -0.1   0.1   0.3 \nb[(Intercept) off_team:Newcastle_Utd]    0.4    0.1  0.3   0.4   0.6 \nb[(Intercept) off_team:Nott'ham_Forest]  0.0    0.1 -0.2   0.0   0.2 \nb[(Intercept) off_team:Sheffield_Utd]   -0.3    0.2 -0.5  -0.3  -0.1 \nb[(Intercept) off_team:Tottenham]        0.3    0.1  0.1   0.3   0.5 \nb[(Intercept) off_team:West_Ham]         0.1    0.1  0.0   0.1   0.3 \nb[(Intercept) off_team:Wolves]           0.0    0.1 -0.2   0.0   0.2 \nb[(Intercept) def_team:Arsenal]         -0.4    0.2 -0.6  -0.4  -0.2 \nb[(Intercept) def_team:Aston_Villa]      0.1    0.1  0.0   0.1   0.3 \nb[(Intercept) def_team:Bournemouth]      0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) def_team:Brentford]        0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) def_team:Brighton]         0.1    0.1  0.0   0.1   0.3 \nb[(Intercept) def_team:Burnley]          0.3    0.1  0.2   0.3   0.5 \nb[(Intercept) def_team:Chelsea]          0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) def_team:Crystal_Palace]   0.1    0.1 -0.1   0.1   0.3 \nb[(Intercept) def_team:Everton]          0.0    0.1 -0.2   0.0   0.1 \nb[(Intercept) def_team:Fulham]           0.1    0.1  0.0   0.1   0.3 \nb[(Intercept) def_team:Liverpool]       -0.2    0.1 -0.4  -0.2   0.0 \nb[(Intercept) def_team:Luton_Town]       0.4    0.1  0.3   0.4   0.6 \nb[(Intercept) def_team:Manchester_City] -0.3    0.2 -0.5  -0.3  -0.1 \nb[(Intercept) def_team:Manchester_Utd]   0.1    0.1 -0.1   0.1   0.3 \nb[(Intercept) def_team:Newcastle_Utd]    0.2    0.1  0.0   0.2   0.3 \nb[(Intercept) def_team:Nott'ham_Forest]  0.2    0.1  0.0   0.2   0.4 \nb[(Intercept) def_team:Sheffield_Utd]    0.6    0.1  0.4   0.6   0.8 \nb[(Intercept) def_team:Tottenham]        0.1    0.1  0.0   0.1   0.3 \nb[(Intercept) def_team:West_Ham]         0.3    0.1  0.1   0.3   0.5 \nb[(Intercept) def_team:Wolves]           0.2    0.1  0.0   0.2   0.4 \nSigma[off_team:(Intercept),(Intercept)]  0.1    0.1  0.1   0.1   0.2 \nSigma[def_team:(Intercept),(Intercept)]  0.1    0.1  0.0   0.1   0.2 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 1.6    0.1  1.5   1.6   1.7  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                                        mcse Rhat n_eff\nis_home                                 0.0  1.0  3727 \nb[(Intercept) off_team:Arsenal]         0.0  1.0  1188 \nb[(Intercept) off_team:Aston_Villa]     0.0  1.0  1353 \nb[(Intercept) off_team:Bournemouth]     0.0  1.0  1497 \nb[(Intercept) off_team:Brentford]       0.0  1.0  1909 \nb[(Intercept) off_team:Brighton]        0.0  1.0  1725 \nb[(Intercept) off_team:Burnley]         0.0  1.0  2531 \nb[(Intercept) off_team:Chelsea]         0.0  1.0  1211 \nb[(Intercept) off_team:Crystal_Palace]  0.0  1.0  1698 \nb[(Intercept) off_team:Everton]         0.0  1.0  2546 \nb[(Intercept) off_team:Fulham]          0.0  1.0  1797 \nb[(Intercept) off_team:Liverpool]       0.0  1.0  1180 \nb[(Intercept) off_team:Luton_Town]      0.0  1.0  2004 \nb[(Intercept) off_team:Manchester_City] 0.0  1.0  1101 \nb[(Intercept) off_team:Manchester_Utd]  0.0  1.0  1604 \nb[(Intercept) off_team:Newcastle_Utd]   0.0  1.0  1125 \nb[(Intercept) off_team:Nott'ham_Forest] 0.0  1.0  1938 \nb[(Intercept) off_team:Sheffield_Utd]   0.0  1.0  3405 \nb[(Intercept) off_team:Tottenham]       0.0  1.0  1223 \nb[(Intercept) off_team:West_Ham]        0.0  1.0  1466 \nb[(Intercept) off_team:Wolves]          0.0  1.0  1929 \nb[(Intercept) def_team:Arsenal]         0.0  1.0  3751 \nb[(Intercept) def_team:Aston_Villa]     0.0  1.0  1420 \nb[(Intercept) def_team:Bournemouth]     0.0  1.0  1354 \nb[(Intercept) def_team:Brentford]       0.0  1.0  1319 \nb[(Intercept) def_team:Brighton]        0.0  1.0  1561 \nb[(Intercept) def_team:Burnley]         0.0  1.0  1228 \nb[(Intercept) def_team:Chelsea]         0.0  1.0  1427 \nb[(Intercept) def_team:Crystal_Palace]  0.0  1.0  1431 \nb[(Intercept) def_team:Everton]         0.0  1.0  1761 \nb[(Intercept) def_team:Fulham]          0.0  1.0  1363 \nb[(Intercept) def_team:Liverpool]       0.0  1.0  2764 \nb[(Intercept) def_team:Luton_Town]      0.0  1.0  1119 \nb[(Intercept) def_team:Manchester_City] 0.0  1.0  3401 \nb[(Intercept) def_team:Manchester_Utd]  0.0  1.0  1509 \nb[(Intercept) def_team:Newcastle_Utd]   0.0  1.0  1550 \nb[(Intercept) def_team:Nott'ham_Forest] 0.0  1.0  1512 \nb[(Intercept) def_team:Sheffield_Utd]   0.0  1.0  1069 \nb[(Intercept) def_team:Tottenham]       0.0  1.0  1483 \nb[(Intercept) def_team:West_Ham]        0.0  1.0  1356 \nb[(Intercept) def_team:Wolves]          0.0  1.0  1330 \nSigma[off_team:(Intercept),(Intercept)] 0.0  1.0   741 \nSigma[def_team:(Intercept),(Intercept)] 0.0  1.0   714 \nmean_PPD                                0.0  1.0  3991 \nlog-posterior                           0.2  1.0   836 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\nWe can view the traditional type of summary we care about with the `print()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(team_goals_model, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstan_glmer\n family:       poisson [log]\n formula:      goals ~ 0 + is_home + (1 | off_team) + (1 | def_team)\n observations: 760\n------\n        Median MAD_SD\nis_home 0.2505 0.0581\n\nError terms:\n Groups   Name        Std.Dev.\n off_team (Intercept) 0.32554 \n def_team (Intercept) 0.31534 \nNum. levels: off_team 20, def_team 20 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n```\n\n\n:::\n:::\n\n\n\n\nYou can see that the display here is fairly similar to the output from `lme4`, with slightly different values. However, the key difference is we have access to posterior samples for all of parameters. We can grab such posterior samples in a similar to the `rstan` output:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_sample <- as.data.frame(team_goals_model) |> as_tibble()\nposterior_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 43\n   is_home b[(Intercept) off_tea…¹ b[(Intercept) off_te…² b[(Intercept) off_te…³\n     <dbl>                   <dbl>                  <dbl>                  <dbl>\n 1   0.222                   0.521                  0.412                -0.0670\n 2   0.255                   0.347                  0.281                 0.200 \n 3   0.285                   0.575                  0.321                -0.206 \n 4   0.364                   0.533                  0.240                 0.194 \n 5   0.354                   0.475                  0.345                -0.198 \n 6   0.390                   0.388                  0.277                -0.0949\n 7   0.189                   0.712                  0.471                 0.174 \n 8   0.298                   0.615                  0.493                 0.130 \n 9   0.244                   0.557                  0.238                 0.0133\n10   0.207                   0.567                  0.407                -0.0625\n# ℹ 3,990 more rows\n# ℹ abbreviated names: ¹​`b[(Intercept) off_team:Arsenal]`,\n#   ²​`b[(Intercept) off_team:Aston_Villa]`,\n#   ³​`b[(Intercept) off_team:Bournemouth]`\n# ℹ 39 more variables: `b[(Intercept) off_team:Brentford]` <dbl>,\n#   `b[(Intercept) off_team:Brighton]` <dbl>,\n#   `b[(Intercept) off_team:Burnley]` <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\nWith this, we can visualize and summarize the posterior distributions for the different quantities of interest like we did in the `bayes_rapm_stand.qmd` demo. Just for demonstration, I'm visualizing the `is_home` posterior samples below:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_sample |>\n  ggplot(aes(x = is_home)) +\n  geom_histogram() +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](16-team-ratings_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nYou'll notice that the random intercepts and variance terms for the team groups have annoying text in them, which will require careful cleaning using functions like `str_remove`. For instance, the histogram below displays the distributions of the variance terms for the offense and defense groups:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_sample |>\n  dplyr::select(contains(\"Sigma\")) |>\n  pivot_longer(cols = everything(),\n               names_to = \"group\", values_to = \"posterior_sample\") |>\n  ggplot(aes(x = posterior_sample)) +\n  geom_histogram() +\n  facet_wrap(~group, ncol = 1) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](16-team-ratings_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nYou could proceed to visualize and summarize the team posterior distributions similar to the `bayes_rapm_stan.qmd` demo.\n\n## Generating posterior predictions\n\nPreviously, when we used frequentist regression models (with or without multilevel components) to generate predictions - we were estimating the conditional expectation of the response given the predictor variables. We only had a single point estimate for the prediction, but could construct a confidence interval to quantify uncertainty about the conditional mean estimate or a prediction interval for an individual observation. For Bayesian regression models, we use the posterior samples for our parameters to generate predictions which directly provides us with full distributions for every prediction.\n\nLet's consider making a prediction for the following hypothetical example for the number of goals scored by Chelsea at home against Liverpool:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchelsea_liverpool <- tibble(off_team = \"Chelsea\", def_team = \"Liverpool\",\n                            is_home = 1)\nchelsea_liverpool\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  off_team def_team  is_home\n  <chr>    <chr>       <dbl>\n1 Chelsea  Liverpool       1\n```\n\n\n:::\n:::\n\n\n\nIn order to predictions for this game, we will need to use the posterior distribution samples for these parameters of interest. Remember we have 4,000 samples quantifying the uncertainty about these parameters:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchelsea_liverpool_samples <- posterior_sample |>\n  dplyr::select(`is_home`, `b[(Intercept) off_team:Chelsea]`,\n                `b[(Intercept) def_team:Liverpool]`) |>\n  # Clean up the column names for ease:\n  janitor::clean_names()\nchelsea_liverpool_samples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 3\n   is_home b_intercept_off_team_chelsea b_intercept_def_team_liverpool\n     <dbl>                        <dbl>                          <dbl>\n 1   0.222                        0.357                        0.122  \n 2   0.255                        0.283                       -0.218  \n 3   0.285                        0.335                       -0.161  \n 4   0.364                        0.312                       -0.344  \n 5   0.354                        0.268                        0.00368\n 6   0.390                        0.412                       -0.455  \n 7   0.189                        0.333                       -0.127  \n 8   0.298                        0.436                       -0.299  \n 9   0.244                        0.197                       -0.126  \n10   0.207                        0.488                        0.00332\n# ℹ 3,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\nRemember, we can effectively do whatever we want with our parameters' posterior distribution samples. This includes computing estimates for the usual type of predictions or estimates for the conditional expectation of the response given inputs. For the Poisson example, the linear model predictions are on the log-link scale, i.e., $\\log \\lambda | X = X\\beta$, so we could simply across the columns to generate a full distribution for the estimate of this conditional expectation and then exponentiate (or more generally transform using the inverse-link function) to get on the response scale. Then just like before with our parameters of interest, we can visualize and summarize this posterior distribution for the conditional expectation of goals scored by Chelsea at home against (as demonstrated in the code chunk below).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchelsea_liverpool_samples <- chelsea_liverpool_samples |>\n  mutate(log_pred = is_home + b_intercept_off_team_chelsea + \n           b_intercept_def_team_liverpool,\n         response_pred = exp(log_pred))\nchelsea_liverpool_samples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 5\n   is_home b_intercept_off_team_…¹ b_intercept_def_team…² log_pred response_pred\n     <dbl>                   <dbl>                  <dbl>    <dbl>         <dbl>\n 1   0.222                   0.357                0.122      0.701          2.02\n 2   0.255                   0.283               -0.218      0.320          1.38\n 3   0.285                   0.335               -0.161      0.459          1.58\n 4   0.364                   0.312               -0.344      0.332          1.39\n 5   0.354                   0.268                0.00368    0.626          1.87\n 6   0.390                   0.412               -0.455      0.348          1.42\n 7   0.189                   0.333               -0.127      0.395          1.49\n 8   0.298                   0.436               -0.299      0.435          1.55\n 9   0.244                   0.197               -0.126      0.315          1.37\n10   0.207                   0.488                0.00332    0.698          2.01\n# ℹ 3,990 more rows\n# ℹ abbreviated names: ¹​b_intercept_off_team_chelsea,\n#   ²​b_intercept_def_team_liverpool\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize the distribution for Chelsea's predicted goals at home again Liverpool\nchelsea_liverpool_samples |>\n  ggplot(aes(x = response_pred)) +\n  geom_histogram() +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](16-team-ratings_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Compute summaries of the posterior distribution:\nchelsea_liverpool_samples |>\n  summarize(posterior_mean = mean(response_pred), \n            posterior_median = median(response_pred),\n            # 80% credible interval:\n            lower_80 = quantile(response_pred, 0.1),\n            upper_80 = quantile(response_pred, 0.9))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  posterior_mean posterior_median lower_80 upper_80\n           <dbl>            <dbl>    <dbl>    <dbl>\n1           1.57             1.55     1.23     1.93\n```\n\n\n:::\n:::\n\n\n\n\nWhile the above is analogous to generating predictions with regression models in the usual sense, the quantified uncertainty only captures the __epistemic uncertainty__ - the uncertainty over the model parameters that we hope to reduce to informative features and data. But, if we want to account for the __aleatoric uncertainty__ - the uncertainty due to the randomness intrinsic to the problem that is not explained by observed variables - then the posterior distribution for the conditional expectation is insufficient! Instead, we also need to draw values from the assumed model generating the data. In this case, we are assuming the goals scored follows a Poisson distribution, i.e., goals scored $\\sim \\text{Poisson}(\\lambda)$. Since we have 4000 values for the estimated scoring rate $\\lambda$, we can then generate 4000 values for the predicted number of goals scored with effectively 4000 different distributions! If that sounds a bit wild to you, that's because it is actually an appropriate way to propagate uncertainty through your regression model: we're generating predictions that accounts for parameter-level uncertainty AND outcome-level uncertainty.\n\nThe code chunk below shows how to generate the Poisson predictions using the conditional expectation samples from before (notice the use of setting the seed due to the randomness here!), along with a histogram displaying the resulting distribution:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(326)\nchelsea_liverpool_samples <- chelsea_liverpool_samples |>\n  # Vectorized sampling of the goals scored by Chelsea:\n  mutate(goals_scored = rpois(n(), response_pred))\n\nchelsea_liverpool_samples |>\n  ggplot(aes(x = goals_scored)) +\n  geom_histogram() +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](16-team-ratings_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nThe above samples and resulting histogram are called the __posterior predictive distribution__, as it captures the generation of observations from the distribution of the outcome implied by the posterior distribution of the parameters of interest. \n\nBUT - we just talked walked through this process for only a single observation! All of those steps were used to generate the posterior distribution predictions for Chelsea's goals scored against Liverpool at home. __We need to do this process for every single observation we want to generate predictions for!__\n\nHere's the good news, `rstanarm` has easy-to-use functions to repeat this process for across all observations we want to generate predictions for with uncertainty quantification. This is separated by the two different versions:\n\n1. **Posterior distribution for conditional expectation**\n\n  + `posterior_linpred()` - generates posterior draws of the linear predictor on the link scale (i.e., the posterior distribution for the model parameters aggregated together). For the Poisson example, this is on the log scale like the `log_pred` values from above. The code chunk below shows how these are equivalent:\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_linpred(team_goals_model, newdata = chelsea_liverpool) |> as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 1\n     `1`\n   <dbl>\n 1 0.701\n 2 0.320\n 3 0.459\n 4 0.332\n 5 0.626\n 6 0.348\n 7 0.395\n 8 0.435\n 9 0.315\n10 0.698\n# ℹ 3,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Just display the first so many of the manually computed values:\nhead(chelsea_liverpool_samples$log_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7013738 0.3195892 0.4587797 0.3320028 0.6258529 0.3475069\n```\n\n\n:::\n:::\n\n\n\n\n  + `posterior_epred()` - same as `posterior_linpred()`, EXCEPT the response values are transformed by inverse-link function (equivalent to using `transform = TRUE` in `posterior_linpred()`). For the Poisson example, this is the `response_pred` column that results from taking the exponential of linear model predictions `log_pred`. If your model is Gaussian, then `posterior_linpred()` and `posterior_epred()` are equivalent. The code chunk below shows how this generates the same output from the manual version above:\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_epred(team_goals_model, newdata = chelsea_liverpool) |> as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 1\n     `1`\n   <dbl>\n 1  2.02\n 2  1.38\n 3  1.58\n 4  1.39\n 5  1.87\n 6  1.42\n 7  1.49\n 8  1.55\n 9  1.37\n10  2.01\n# ℹ 3,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(chelsea_liverpool_samples$response_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.016521 1.376562 1.582142 1.393757 1.869840 1.415534\n```\n\n\n:::\n:::\n\n\n\n  + By default, you should use `posterior_epred()` instead of `posterior_linpred()` because of the link transformation to the response scale. The following code chunk shows what the output looks like for generating the posterior distributions of the conditional expectations for each of the observations in the data where each row is a posterior sample while **each column is an observation from the dataset that we're interested in generating predictions for**:\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_epred(team_goals_model, newdata = eng_match_goals_scored) |> \n  as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 760\n     `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`  `13`\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 0.660  2.48 0.896 1.04   2.57  1.30  2.20  1.07  2.02  1.70  1.52  2.12  1.91\n 2 1.17   2.06 1.07  1.26   1.24  2.02  2.29  1.80  1.38  1.78  1.62  1.95  2.37\n 3 0.518  3.52 1.34  0.835  2.69  1.63  2.08  1.89  1.58  1.16  1.68  2.75  1.42\n 4 1.03   2.94 1.40  1.70   2.52  2.37  2.13  1.61  1.39  1.49  1.88  2.48  2.85\n 5 0.592  3.35 1.34  0.635  2.02  1.63  2.53  1.51  1.87  2.19  1.69  2.07  1.53\n 6 0.954  2.55 1.16  1.03   2.62  1.69  2.72  1.28  1.42  1.99  1.32  2.68  2.06\n 7 0.644  2.92 1.43  1.19   1.81  1.95  2.19  2.21  1.49  1.46  1.77  1.77  1.79\n 8 0.751  2.76 0.963 1.46   2.82  1.69  2.71  1.31  1.55  2.31  1.49  2.53  2.30\n 9 1.35   1.92 1.29  1.42   2.54  1.66  1.97  1.66  1.37  1.61  1.72  1.69  1.81\n10 1.11   1.94 1.14  1.40   2.25  1.37  2.33  1.38  2.01  1.37  1.48  2.44  1.60\n# ℹ 3,990 more rows\n# ℹ 747 more variables: `14` <dbl>, `15` <dbl>, `16` <dbl>, `17` <dbl>,\n#   `18` <dbl>, `19` <dbl>, `20` <dbl>, `21` <dbl>, `22` <dbl>, `23` <dbl>,\n#   `24` <dbl>, `25` <dbl>, `26` <dbl>, `27` <dbl>, `28` <dbl>, `29` <dbl>,\n#   `30` <dbl>, `31` <dbl>, `32` <dbl>, `33` <dbl>, `34` <dbl>, `35` <dbl>,\n#   `36` <dbl>, `37` <dbl>, `38` <dbl>, `39` <dbl>, `40` <dbl>, `41` <dbl>,\n#   `42` <dbl>, `43` <dbl>, `44` <dbl>, `45` <dbl>, `46` <dbl>, `47` <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\n2. **Posterior predictive distribution**\n\n  + `posterior_predict()` - generates posterior draws from the distribution of the outcome implied by the posterior distribution for the parameter. This is equivalent to how we generated the `goals_scored` values from above by sampling from the Poisson distribution for each of the 4000 samples. You can see this match in the code chunk below:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_predict(team_goals_model, newdata = chelsea_liverpool,\n                  seed = 326) |> as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 1\n     `1`\n   <int>\n 1     2\n 2     2\n 3     1\n 4     1\n 5     4\n 6     1\n 7     2\n 8     0\n 9     2\n10     2\n# ℹ 3,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(chelsea_liverpool_samples$goals_scored)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 2 1 1 4 1\n```\n\n\n:::\n:::\n\n\n\n\n  + And just like with `posterior_epred()`, if we give a dataset with more than one observation then the posterior predictive distributions will have separate columns for each observation:\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior_predict(team_goals_model, newdata = eng_match_goals_scored) |> \n  as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 760\n     `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`  `13`\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     0     4     2     0     3     1     2     2     4     0     4     1     2\n 2     2     1     2     0     1     3     2     2     4     0     2     0     3\n 3     0     6     0     3     3     0     3     1     2     5     3     1     2\n 4     0     2     0     1     2     5     0     3     1     1     1     1     3\n 5     2     7     1     1     1     3     4     0     1     4     1     1     4\n 6     1     1     1     1     1     2     4     1     1     0     1     4     3\n 7     2     4     3     2     1     2     0     2     0     0     2     3     2\n 8     1     4     1     0     5     2     1     3     0     1     2     4     1\n 9     0     4     1     1     6     3     2     3     1     4     1     2     2\n10     2     1     3     3     0     1     4     2     2     1     1     2     1\n# ℹ 3,990 more rows\n# ℹ 747 more variables: `14` <int>, `15` <int>, `16` <int>, `17` <int>,\n#   `18` <int>, `19` <int>, `20` <int>, `21` <int>, `22` <int>, `23` <int>,\n#   `24` <int>, `25` <int>, `26` <int>, `27` <int>, `28` <int>, `29` <int>,\n#   `30` <int>, `31` <int>, `32` <int>, `33` <int>, `34` <int>, `35` <int>,\n#   `36` <int>, `37` <int>, `38` <int>, `39` <int>, `40` <int>, `41` <int>,\n#   `42` <int>, `43` <int>, `44` <int>, `45` <int>, `46` <int>, `47` <int>, …\n```\n\n\n:::\n:::\n\n\n\n\n## Posterior predictive check\n\nThe posterior predictive distribution is also useful to consider for model diagnostics. If you randomly grabbed a row from the above table generated in the previous code chunk, you could observe what the resulting distribution of values looks like via a histogram or density curve. You want that distribution across all observations to be similar to the actual observed response variable distribution. Of course, we have 4000 rows in that table and thus have 4000 distributions for which we can compare against the observed outcome distribution. The inspection of the posterior predictive distributions is called [__posterior predictive check__](https://mc-stan.org/rstanarm/reference/pp_check.stanreg.html). This is commonly used diagnostic to see how the distribution of the posterior draws for the model's training data compares to the actual observed distribution of training data response values. This can be easily created with the `pp_check` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(team_goals_model) +\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](16-team-ratings_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n## Prior predictive check\n\nWe have not mentioned at all yet what the prior distributions were for the considered model. We can check to see what Stan used with the `prior_summary()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_summary(object = team_goals_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPriors for model 'team_goals_model' \n------\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 5)\n\nCovariance\n ~ decov(reg. = 1, conc. = 1, shape = 1, scale = 1)\n------\nSee help('prior_summary.stanreg') for more details\n```\n\n\n:::\n:::\n\n\n\n\nYou can read more about the default priors in the [_Prior Distributions for `rstanarm` Models_](https://mc-stan.org/rstanarm/articles/priors.html) vignette.\n\nWhile we just discussed the use of the posterior predictive check as a means for assessing the posterior distribution model fit, **we can also do the same type of procedure WITHOUT observing data!** This is known as a **prior predictive check** and it just involves the exact same steps as how we generated predictions with the posterior - except we can generate predictions only using the prior distributions for the parameters of interest. This is a great way to evaluate if your prior is appropriate. \n\nYou can implement a prior predictive check by first \"re-fitting\" the same model, except you'll tell Stan to ignore the data and only use the prior via `prior_PD = TRUE`. You still have to provide the data so Stan knows how to set-up the variables in the resulting output.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior_team_goals_model <- stan_glmer(goals ~ is_home + (1 | off_team) + (1 | def_team),\n                                     family = poisson,\n                                     data = eng_match_goals_scored, seed = 2013,\n                                     prior_PD = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.06 seconds (Warm-up)\nChain 1:                0.061 seconds (Sampling)\nChain 1:                0.121 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.063 seconds (Warm-up)\nChain 2:                0.063 seconds (Sampling)\nChain 2:                0.126 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.061 seconds (Warm-up)\nChain 3:                0.062 seconds (Sampling)\nChain 3:                0.123 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.061 seconds (Warm-up)\nChain 4:                0.062 seconds (Sampling)\nChain 4:                0.123 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nprior_sample <- as.data.frame(prior_team_goals_model) |> as_tibble()\nprior_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 × 44\n   `(Intercept)` is_home b[(Intercept) off_team:Arsenal…¹ b[(Intercept) off_te…²\n           <dbl>   <dbl>                            <dbl>                  <dbl>\n 1         0.972  -1.46                           -0.0434                0.00993\n 2         0.468   4.27                           -0.0494               -0.0662 \n 3         1.88   -7.61                            0.349                 0.227  \n 4         2.93   -0.610                          -0.179                -0.192  \n 5         1.15    0.480                           0.697                 0.632  \n 6         0.364   2.09                            0.765                 0.112  \n 7        -3.51   -2.00                           -0.403                 0.288  \n 8         3.69    0.336                           0.802                -0.381  \n 9         0.644  -1.92                            0.0860               -0.876  \n10         4.66   -4.31                            0.345                 1.86   \n# ℹ 3,990 more rows\n# ℹ abbreviated names: ¹​`b[(Intercept) off_team:Arsenal]`,\n#   ²​`b[(Intercept) off_team:Aston_Villa]`\n# ℹ 40 more variables: `b[(Intercept) off_team:Bournemouth]` <dbl>,\n#   `b[(Intercept) off_team:Brentford]` <dbl>,\n#   `b[(Intercept) off_team:Brighton]` <dbl>,\n#   `b[(Intercept) off_team:Burnley]` <dbl>, …\n```\n\n\n:::\n:::\n\n\n\nNotice how this barely took any time, because the data was completely ignored - this is just shortcut for drawing from the prior distribution! We can then re-use the same `pp_check()` function to visualize the prior predictive distributions across these 4000 prior samples:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(prior_team_goals_model) + \n  scale_x_continuous(limits = c(0, 10)) +\n  theme_light()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 11641 rows containing non-finite outside the scale range\n(`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](16-team-ratings_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\nI manually truncated the x-axis here, **but what do you think about the values generated from this prior distribution?**\n",
    "supporting": [
      "16-team-ratings_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}